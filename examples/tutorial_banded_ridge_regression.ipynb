{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tikreg tutorial: Banded ridge regression\n",
    "\n",
    "`tikreg`: https://github.com/gallantlab/tikreg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example follows Figure 5 from [Nunez-Elizalde, et al., 2019.](https://www.sciencedirect.com/science/article/pii/S1053811919302988?via%3Dihub)\n",
    "\n",
    "For a technical description of banded ridge regression, checkout this [Notebook](https://nbviewer.jupyter.org/github/gallantlab/tikreg/blob/main/examples/tutorial_banded_ridge_polar.ipynb) or launch it on [Google Colab.](https://colab.research.google.com/github/gallantlab/tikreg/blob/main/examples/tutorial_banded_ridge_polar.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Banded ridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When estimating a joint encoding model that consists of two feature spaces, banded ridge regression can be used to fit the model and assign each feature space a different regularization parameter. \n",
    "\n",
    "$$Y = X_1 \\beta_1 + X_2 \\beta_2 + \\epsilon$$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\beta_1 \\sim \\mathcal{N}\\left(0, \\lambda_1^{-2} I_p\\right)\\\\\n",
    "\\beta_2 \\sim \\mathcal{N}\\left(0, \\lambda_2^{-2} I_q\\right)\\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Estimating this model is computational expensive, requiring cross-validating two regularization parameters for every voxel ($\\lambda_1$ and $\\lambda_2$). \n",
    "\n",
    "In this tutorial, we'll perform banded ridge regression using data from a vision fMRI experiment (Nishimoto, et al., 2011). We'll be modeling voxel responses as a linear combination of motion energy and object category features. Instructions to download the fMRI data and the motion energy and object category features are provided below. Code to compute motion energy feature from arbitrary stimuli is provided in the Python package package ``pymoten`` ([github repo](https://github.com/gallantlab/pymoten/)).\n",
    "\n",
    "This tutorial follows Figure 5 from [Nunez-Elizalde, et al., 2019.](https://www.sciencedirect.com/science/article/pii/S1053811919302988?via%3Dihub)\n",
    "\n",
    "For a more technical description of banded ridge regression, checkout this [Notebook](https://nbviewer.jupyter.org/github/gallantlab/tikreg/blob/main/examples/tutorial_banded_ridge_polar.ipynb) or launch it on [Google Colab.](https://colab.research.google.com/github/gallantlab/tikreg/blob/main/examples/tutorial_banded_ridge_polar.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "In addition to `tikreg`, you'll need to install `h5py` to load the example data used in this tutorial and `matplotlib` to display the figures.\n",
    "\n",
    "Uncomment the following line install the requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tikreg==0.0.1 h5py matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data required to run this tutorial can be downloaded from this link or from the code below (~250MB): \n",
    "https://drive.google.com/open?id=1fcYosXaGsdS3u1xj8NMIC6ytEUbXMuGj.\n",
    "\n",
    "Once downloaded, unzip the file contents and write them to the desired location (``root_directory``):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of the unzipped data\n",
    "root_directory = '.' # defaults to current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## On linux (and maybe macs), you can use the following to download and unzip the data\n",
    "## (from: https://gist.github.com/iamtekeste/3cdfd0366ebfd2c0d805#gistcomment-2316906)\n",
    "\n",
    "if 0: # Change to 1 if running on e.g. Binder or Google Colab\n",
    "    import os\n",
    "    flname = 'tikreg_demodata.zip'\n",
    "    unzipped_folder = 'tiny_vision'\n",
    "    cmd = r'''wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILEID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILEID}\" -O {FILENAME} && rm -rf /tmp/cookies.txt'''\n",
    "    if not os.path.exists(flname) and not os.path.exists(unzipped_folder):\n",
    "        os.system(cmd.format(FILEID='1fcYosXaGsdS3u1xj8NMIC6ytEUbXMuGj',FILENAME=flname))\n",
    "    os.system('unzip -u tikreg_demodata.zip')\n",
    "    assert os.path.exists(unzipped_folder)\n",
    "    print('The exampled data for this notebook has been downloaded: %s'%unzipped_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start\n",
    "\n",
    "Now that we've downloaded the data, we are ready to proceed with the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "np.set_printoptions(precision=4, suppress=True)\n",
    "\n",
    "from scipy.stats import zscore\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import h5py\n",
    "from tikreg import models, utils as tikutils\n",
    "from tikreg import spatial_priors, temporal_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root_directory, 'tiny_vision')\n",
    "\n",
    "moten_features_file = os.path.join(path, 'features', 'motion_energy.hdf')\n",
    "object_categories_file = os.path.join(path, 'features', 'object_categories.hdf')\n",
    "responses_file = os.path.join(path, 'responses', 'S1.hdf')\n",
    "voxels_file = os.path.join(path, 'rois', 'S1.hdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hdf_load(hdf_file, key=None):\n",
    "    assert os.path.exists(hdf_file)\n",
    "    \n",
    "    with h5py.File(hdf_file, 'r') as hfl:\n",
    "        hdf_content = hfl.keys()\n",
    "        if key is None:\n",
    "            e = ValueError('Please specify the HDF file content to load:', hdf_content)\n",
    "            raise(e)\n",
    "        assert key in hdf_content\n",
    "        return np.asarray(hfl[key])\n",
    "\n",
    "def plot_model_2dhist_comparison(corrs1, corrs2, name1, name2, cmap='inferno',\n",
    "                                 ax=None, lims=(-0.5, 1.0), nbins=100, \n",
    "                                 title='model comparison\\n(prediction accuracy [$r$])'):\n",
    "    '''\n",
    "    '''\n",
    "    colormap = plt.cm.get_cmap(cmap)\n",
    "    \n",
    "    lo, hi = lims\n",
    "    bins = np.linspace(lo, hi, nbins)\n",
    "    h, xe, ye = np.histogram2d(corrs2,\n",
    "                               corrs1,\n",
    "                               bins=bins)\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "    im = ax.imshow(np.log(h), origin=\"lower\", extent=(lo, hi, lo, hi), cmap=colormap, interpolation='nearest')\n",
    "    ax.plot([lo, hi], [lo, hi], 'k-')\n",
    "    ax.axis([lo, hi, lo, hi])\n",
    "\n",
    "\n",
    "    ax.plot([0, 0], [lo, hi], color=\"0.1\", linestyle=\":\")\n",
    "    ax.plot([lo, hi], [0, 0], color=\"0.1\", linestyle=\":\")\n",
    "\n",
    "    ax.set_ylabel(\"%s model\" % name2, fontsize=15)\n",
    "    ax.set_xlabel(\"%s model\" % name1, fontsize=15)\n",
    "\n",
    "    cbar = plt.colorbar(im, orientation='vertical')\n",
    "    cbar.set_label('voxel density [$log_{10}$]', fontsize=15)\n",
    "    _ = ax.set_title(title, fontsize=20)\n",
    "    return ax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 6555) (270, 6555)\n"
     ]
    }
   ],
   "source": [
    "Mtrain = hdf_load(moten_features_file, 'train')\n",
    "Mtest = hdf_load(moten_features_file, 'test')\n",
    "print(Mtrain.shape, Mtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 1705) (270, 1705)\n"
     ]
    }
   ],
   "source": [
    "Otrain = hdf_load(object_categories_file, 'train')\n",
    "Otest = hdf_load(object_categories_file, 'test')\n",
    "print(Otrain.shape, Otest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3600, 2650) (270, 2650)\n"
     ]
    }
   ],
   "source": [
    "Ytrain = hdf_load(responses_file, 'train')\n",
    "Ytest = hdf_load(responses_file, 'test')\n",
    "print(Ytrain.shape, Ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nfeaturesm = Mtrain.shape[1]\n",
    "nfeatureso = Otrain.shape[1]\n",
    "delays = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1: temporal 1/1=1.000, features 1/1=(1.0000, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1998, (25,50,75)pctl=(0.0805,0.1721,0.3064),(0.0<r>0.5): (2557,077)\n",
      "lambda 02:    2.783, mean=0.1998, (25,50,75)pctl=(0.0806,0.1721,0.3065),(0.0<r>0.5): (2557,078)\n",
      "lambda 03:    7.743, mean=0.2002, (25,50,75)pctl=(0.0807,0.1726,0.3072),(0.0<r>0.5): (2557,078)\n",
      "lambda 04:   21.544, mean=0.2028, (25,50,75)pctl=(0.0824,0.1753,0.3109),(0.0<r>0.5): (2559,085)\n",
      "lambda 05:   59.948, mean=0.2153, (25,50,75)pctl=(0.0900,0.1908,0.3289),(0.0<r>0.5): (2561,114)\n",
      "lambda 06:  166.810, mean=0.2420, (25,50,75)pctl=(0.1076,0.2235,0.3666),(0.0<r>0.5): (2576,194)\n",
      "lambda 07:  464.159, mean=0.2544, (25,50,75)pctl=(0.1268,0.2433,0.3753),(0.0<r>0.5): (2587,191)\n",
      "lambda 08: 1291.550, mean=0.2328, (25,50,75)pctl=(0.1216,0.2239,0.3372),(0.0<r>0.5): (2585,086)\n",
      "lambda 09: 3593.814, mean=0.1976, (25,50,75)pctl=(0.0982,0.1851,0.2902),(0.0<r>0.5): (2549,014)\n",
      "lambda 10: 10000.000, mean=0.1803, (25,50,75)pctl=(0.0879,0.1676,0.2662),(0.0<r>0.5): (2515,000)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2084, (25,50,75)pctl=(0.0846,0.1842,0.3198),(0.0<r>0.5): (2521,111)\n",
      "lambda 02:    2.783, mean=0.2085, (25,50,75)pctl=(0.0846,0.1843,0.3198),(0.0<r>0.5): (2521,111)\n",
      "lambda 03:    7.743, mean=0.2089, (25,50,75)pctl=(0.0851,0.1847,0.3201),(0.0<r>0.5): (2521,113)\n",
      "lambda 04:   21.544, mean=0.2117, (25,50,75)pctl=(0.0866,0.1885,0.3244),(0.0<r>0.5): (2526,127)\n",
      "lambda 05:   59.948, mean=0.2254, (25,50,75)pctl=(0.0952,0.2030,0.3451),(0.0<r>0.5): (2545,166)\n",
      "lambda 06:  166.810, mean=0.2574, (25,50,75)pctl=(0.1111,0.2392,0.3922),(0.0<r>0.5): (2569,280)\n",
      "lambda 07:  464.159, mean=0.2760, (25,50,75)pctl=(0.1273,0.2654,0.4088),(0.0<r>0.5): (2579,339)\n",
      "lambda 08: 1291.550, mean=0.2517, (25,50,75)pctl=(0.1180,0.2327,0.3747),(0.0<r>0.5): (2565,247)\n",
      "lambda 09: 3593.814, mean=0.2122, (25,50,75)pctl=(0.0888,0.1832,0.3295),(0.0<r>0.5): (2523,125)\n",
      "lambda 10: 10000.000, mean=0.1923, (25,50,75)pctl=(0.0738,0.1602,0.3056),(0.0<r>0.5): (2489,066)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2034, (25,50,75)pctl=(0.0832,0.1716,0.3093),(0.0<r>0.5): (2546,117)\n",
      "lambda 02:    2.783, mean=0.2035, (25,50,75)pctl=(0.0833,0.1716,0.3094),(0.0<r>0.5): (2545,117)\n",
      "lambda 03:    7.743, mean=0.2040, (25,50,75)pctl=(0.0835,0.1720,0.3102),(0.0<r>0.5): (2544,118)\n",
      "lambda 04:   21.544, mean=0.2072, (25,50,75)pctl=(0.0844,0.1755,0.3142),(0.0<r>0.5): (2548,128)\n",
      "lambda 05:   59.948, mean=0.2215, (25,50,75)pctl=(0.0902,0.1910,0.3370),(0.0<r>0.5): (2557,171)\n",
      "lambda 06:  166.810, mean=0.2533, (25,50,75)pctl=(0.1086,0.2253,0.3839),(0.0<r>0.5): (2574,290)\n",
      "lambda 07:  464.159, mean=0.2759, (25,50,75)pctl=(0.1276,0.2544,0.4087),(0.0<r>0.5): (2580,364)\n",
      "lambda 08: 1291.550, mean=0.2591, (25,50,75)pctl=(0.1267,0.2436,0.3815),(0.0<r>0.5): (2578,239)\n",
      "lambda 09: 3593.814, mean=0.2208, (25,50,75)pctl=(0.1017,0.2035,0.3239),(0.0<r>0.5): (2536,102)\n",
      "lambda 10: 10000.000, mean=0.2004, (25,50,75)pctl=(0.0896,0.1834,0.2976),(0.0<r>0.5): (2508,057)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2240, (25,50,75)pctl=(0.0876,0.1952,0.3407),(0.0<r>0.5): (2554,193)\n",
      "lambda 02:    2.783, mean=0.2240, (25,50,75)pctl=(0.0876,0.1953,0.3408),(0.0<r>0.5): (2554,193)\n",
      "lambda 03:    7.743, mean=0.2244, (25,50,75)pctl=(0.0878,0.1957,0.3413),(0.0<r>0.5): (2555,194)\n",
      "lambda 04:   21.544, mean=0.2267, (25,50,75)pctl=(0.0891,0.1984,0.3453),(0.0<r>0.5): (2556,200)\n",
      "lambda 05:   59.948, mean=0.2390, (25,50,75)pctl=(0.0966,0.2100,0.3653),(0.0<r>0.5): (2555,239)\n",
      "lambda 06:  166.810, mean=0.2696, (25,50,75)pctl=(0.1182,0.2443,0.4074),(0.0<r>0.5): (2569,372)\n",
      "lambda 07:  464.159, mean=0.2933, (25,50,75)pctl=(0.1429,0.2759,0.4358),(0.0<r>0.5): (2589,439)\n",
      "lambda 08: 1291.550, mean=0.2865, (25,50,75)pctl=(0.1465,0.2753,0.4195),(0.0<r>0.5): (2589,353)\n",
      "lambda 09: 3593.814, mean=0.2583, (25,50,75)pctl=(0.1294,0.2488,0.3774),(0.0<r>0.5): (2571,241)\n",
      "lambda 10: 10000.000, mean=0.2411, (25,50,75)pctl=(0.1194,0.2290,0.3536),(0.0<r>0.5): (2553,193)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2147, (25,50,75)pctl=(0.0922,0.1864,0.3283),(0.0<r>0.5): (2539,130)\n",
      "lambda 02:    2.783, mean=0.2148, (25,50,75)pctl=(0.0922,0.1865,0.3284),(0.0<r>0.5): (2539,130)\n",
      "lambda 03:    7.743, mean=0.2152, (25,50,75)pctl=(0.0929,0.1869,0.3289),(0.0<r>0.5): (2538,137)\n",
      "lambda 04:   21.544, mean=0.2180, (25,50,75)pctl=(0.0945,0.1901,0.3329),(0.0<r>0.5): (2542,146)\n",
      "lambda 05:   59.948, mean=0.2320, (25,50,75)pctl=(0.0999,0.2080,0.3538),(0.0<r>0.5): (2550,194)\n",
      "lambda 06:  166.810, mean=0.2658, (25,50,75)pctl=(0.1217,0.2462,0.4037),(0.0<r>0.5): (2565,318)\n",
      "lambda 07:  464.159, mean=0.2914, (25,50,75)pctl=(0.1466,0.2827,0.4326),(0.0<r>0.5): (2591,409)\n",
      "lambda 08: 1291.550, mean=0.2791, (25,50,75)pctl=(0.1481,0.2712,0.4098),(0.0<r>0.5): (2588,287)\n",
      "lambda 09: 3593.814, mean=0.2413, (25,50,75)pctl=(0.1270,0.2259,0.3484),(0.0<r>0.5): (2564,156)\n",
      "lambda 10: 10000.000, mean=0.2190, (25,50,75)pctl=(0.1086,0.1993,0.3220),(0.0<r>0.5): (2550,091)\n",
      "pop.cv.best: 464.159, mean=0.2301, (25,50,75)pctl=(0.1388,0.2649,0.4123),(0.0<r>0.5): (2640,334)\n",
      "Duration 0.8477[mins]\n",
      "lambda 01:  464.159, mean=0.3988, (25,50,75)pctl=(0.2338,0.4255,0.5826),(0.0<r>0.5): (2545,1006)\n",
      "2650 responses: ridge=  464.159, temporal=1.000, spatial=(1.000, 1.000) perf=0.3988\n",
      "Total duration 1.4214[mins]\n"
     ]
    }
   ],
   "source": [
    "moten_prior = spatial_priors.SphericalPrior(nfeaturesm)\n",
    "obcat_prior = spatial_priors.SphericalPrior(nfeatureso)\n",
    "temporal_prior = temporal_priors.SphericalPrior(delays=delays)\n",
    "\n",
    "fit_spherical_pop = models.estimate_stem_wmvnp([Mtrain, Otrain], Ytrain, \n",
    "                                               [Mtest, Otest],Ytest,\n",
    "                                               feature_priors=[moten_prior, obcat_prior],\n",
    "                                               temporal_prior=temporal_prior,\n",
    "                                               ridges=np.logspace(0,4,10),\n",
    "                                               folds=(1,5),\n",
    "                                               performance=True,\n",
    "                                               population_optimal=True,\n",
    "                                               verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1: temporal 1/1=1.000, features 1/1=(1.0000, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0835,0.1808,0.3121),(0.0<r>0.5): (2534,122)\n",
      "lambda 02:    2.783, mean=0.2068, (25,50,75)pctl=(0.0835,0.1808,0.3122),(0.0<r>0.5): (2534,123)\n",
      "lambda 03:    7.743, mean=0.2072, (25,50,75)pctl=(0.0836,0.1813,0.3127),(0.0<r>0.5): (2534,124)\n",
      "lambda 04:   21.544, mean=0.2099, (25,50,75)pctl=(0.0854,0.1844,0.3163),(0.0<r>0.5): (2533,131)\n",
      "lambda 05:   59.948, mean=0.2232, (25,50,75)pctl=(0.0943,0.1982,0.3378),(0.0<r>0.5): (2541,163)\n",
      "lambda 06:  166.810, mean=0.2548, (25,50,75)pctl=(0.1154,0.2359,0.3834),(0.0<r>0.5): (2565,262)\n",
      "lambda 07:  464.159, mean=0.2782, (25,50,75)pctl=(0.1413,0.2669,0.4109),(0.0<r>0.5): (2582,307)\n",
      "lambda 08: 1291.550, mean=0.2599, (25,50,75)pctl=(0.1375,0.2508,0.3760),(0.0<r>0.5): (2580,177)\n",
      "lambda 09: 3593.814, mean=0.2191, (25,50,75)pctl=(0.1134,0.2080,0.3185),(0.0<r>0.5): (2544,066)\n",
      "lambda 10: 10000.000, mean=0.1980, (25,50,75)pctl=(0.0953,0.1817,0.2918),(0.0<r>0.5): (2515,044)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2273, (25,50,75)pctl=(0.0933,0.1992,0.3455),(0.0<r>0.5): (2558,220)\n",
      "lambda 02:    2.783, mean=0.2273, (25,50,75)pctl=(0.0934,0.1993,0.3456),(0.0<r>0.5): (2558,220)\n",
      "lambda 03:    7.743, mean=0.2276, (25,50,75)pctl=(0.0935,0.1997,0.3461),(0.0<r>0.5): (2558,221)\n",
      "lambda 04:   21.544, mean=0.2300, (25,50,75)pctl=(0.0943,0.2027,0.3503),(0.0<r>0.5): (2560,225)\n",
      "lambda 05:   59.948, mean=0.2423, (25,50,75)pctl=(0.1015,0.2173,0.3679),(0.0<r>0.5): (2566,262)\n",
      "lambda 06:  166.810, mean=0.2700, (25,50,75)pctl=(0.1219,0.2525,0.4052),(0.0<r>0.5): (2579,345)\n",
      "lambda 07:  464.159, mean=0.2870, (25,50,75)pctl=(0.1464,0.2757,0.4250),(0.0<r>0.5): (2587,384)\n",
      "lambda 08: 1291.550, mean=0.2718, (25,50,75)pctl=(0.1441,0.2628,0.3982),(0.0<r>0.5): (2576,264)\n",
      "lambda 09: 3593.814, mean=0.2404, (25,50,75)pctl=(0.1238,0.2316,0.3557),(0.0<r>0.5): (2553,112)\n",
      "lambda 10: 10000.000, mean=0.2241, (25,50,75)pctl=(0.1143,0.2159,0.3337),(0.0<r>0.5): (2539,044)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2053, (25,50,75)pctl=(0.0794,0.1793,0.3177),(0.0<r>0.5): (2506,116)\n",
      "lambda 02:    2.783, mean=0.2053, (25,50,75)pctl=(0.0795,0.1793,0.3178),(0.0<r>0.5): (2506,116)\n",
      "lambda 03:    7.743, mean=0.2058, (25,50,75)pctl=(0.0798,0.1796,0.3184),(0.0<r>0.5): (2509,122)\n",
      "lambda 04:   21.544, mean=0.2087, (25,50,75)pctl=(0.0807,0.1820,0.3236),(0.0<r>0.5): (2509,130)\n",
      "lambda 05:   59.948, mean=0.2231, (25,50,75)pctl=(0.0881,0.1975,0.3446),(0.0<r>0.5): (2521,181)\n",
      "lambda 06:  166.810, mean=0.2569, (25,50,75)pctl=(0.1068,0.2320,0.3946),(0.0<r>0.5): (2542,310)\n",
      "lambda 07:  464.159, mean=0.2821, (25,50,75)pctl=(0.1295,0.2659,0.4212),(0.0<r>0.5): (2577,377)\n",
      "lambda 08: 1291.550, mean=0.2681, (25,50,75)pctl=(0.1320,0.2549,0.3945),(0.0<r>0.5): (2586,279)\n",
      "lambda 09: 3593.814, mean=0.2304, (25,50,75)pctl=(0.1081,0.2119,0.3424),(0.0<r>0.5): (2561,170)\n",
      "lambda 10: 10000.000, mean=0.2076, (25,50,75)pctl=(0.0899,0.1880,0.3144),(0.0<r>0.5): (2520,100)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2181, (25,50,75)pctl=(0.0838,0.1849,0.3345),(0.0<r>0.5): (2545,178)\n",
      "lambda 02:    2.783, mean=0.2181, (25,50,75)pctl=(0.0838,0.1849,0.3346),(0.0<r>0.5): (2545,178)\n",
      "lambda 03:    7.743, mean=0.2186, (25,50,75)pctl=(0.0840,0.1850,0.3352),(0.0<r>0.5): (2547,179)\n",
      "lambda 04:   21.544, mean=0.2216, (25,50,75)pctl=(0.0858,0.1882,0.3398),(0.0<r>0.5): (2549,191)\n",
      "lambda 05:   59.948, mean=0.2354, (25,50,75)pctl=(0.0944,0.2039,0.3597),(0.0<r>0.5): (2560,230)\n",
      "lambda 06:  166.810, mean=0.2648, (25,50,75)pctl=(0.1134,0.2407,0.4002),(0.0<r>0.5): (2575,329)\n",
      "lambda 07:  464.159, mean=0.2828, (25,50,75)pctl=(0.1333,0.2649,0.4183),(0.0<r>0.5): (2581,377)\n",
      "lambda 08: 1291.550, mean=0.2664, (25,50,75)pctl=(0.1308,0.2468,0.3936),(0.0<r>0.5): (2572,303)\n",
      "lambda 09: 3593.814, mean=0.2312, (25,50,75)pctl=(0.1033,0.2024,0.3455),(0.0<r>0.5): (2532,194)\n",
      "lambda 10: 10000.000, mean=0.2111, (25,50,75)pctl=(0.0876,0.1788,0.3239),(0.0<r>0.5): (2514,153)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1897, (25,50,75)pctl=(0.0766,0.1627,0.2858),(0.0<r>0.5): (2534,068)\n",
      "lambda 02:    2.783, mean=0.1898, (25,50,75)pctl=(0.0766,0.1627,0.2859),(0.0<r>0.5): (2534,069)\n",
      "lambda 03:    7.743, mean=0.1902, (25,50,75)pctl=(0.0768,0.1630,0.2868),(0.0<r>0.5): (2533,070)\n",
      "lambda 04:   21.544, mean=0.1931, (25,50,75)pctl=(0.0786,0.1661,0.2915),(0.0<r>0.5): (2532,079)\n",
      "lambda 05:   59.948, mean=0.2072, (25,50,75)pctl=(0.0869,0.1826,0.3146),(0.0<r>0.5): (2545,114)\n",
      "lambda 06:  166.810, mean=0.2429, (25,50,75)pctl=(0.1082,0.2217,0.3645),(0.0<r>0.5): (2558,223)\n",
      "lambda 07:  464.159, mean=0.2672, (25,50,75)pctl=(0.1309,0.2502,0.3928),(0.0<r>0.5): (2584,271)\n",
      "lambda 08: 1291.550, mean=0.2480, (25,50,75)pctl=(0.1289,0.2329,0.3607),(0.0<r>0.5): (2570,169)\n",
      "lambda 09: 3593.814, mean=0.2103, (25,50,75)pctl=(0.1020,0.1974,0.3065),(0.0<r>0.5): (2538,063)\n",
      "lambda 10: 10000.000, mean=0.1912, (25,50,75)pctl=(0.0870,0.1761,0.2832),(0.0<r>0.5): (2516,018)\n",
      "pop.cv.best: 464.159, mean=0.2300, (25,50,75)pctl=(0.1408,0.2658,0.4120),(0.0<r>0.5): (2642,333)\n",
      "Duration 0.7560[mins]\n",
      "lambda 01:    7.743, mean=-0.0370, (25,50,75)pctl=(-0.0810,0.0128,0.0319),(0.0<r>0.5): (002,000)\n",
      "3 responses: ridge=    7.743, temporal=1.000, spatial=(1.000, 1.000) perf=-0.0370\n",
      "lambda 01:   59.948, mean=0.0505, (25,50,75)pctl=(-0.0048,0.0613,0.0920),(0.0<r>0.5): (015,000)\n",
      "21 responses: ridge=   59.948, temporal=1.000, spatial=(1.000, 1.000) perf=0.0505\n",
      "lambda 01:    1.000, mean=0.0229, (25,50,75)pctl=(-0.0413,0.0110,0.0834),(0.0<r>0.5): (024,000)\n",
      "46 responses: ridge=    1.000, temporal=1.000, spatial=(1.000, 1.000) perf=0.0229\n",
      "lambda 01:  464.159, mean=0.4650, (25,50,75)pctl=(0.3359,0.4958,0.6138),(0.0<r>0.5): (1871,933)\n",
      "1898 responses: ridge=  464.159, temporal=1.000, spatial=(1.000, 1.000) perf=0.4650\n",
      "lambda 01:  166.810, mean=0.3166, (25,50,75)pctl=(0.0568,0.3290,0.5693),(0.0<r>0.5): (109,045)\n",
      "131 responses: ridge=  166.810, temporal=1.000, spatial=(1.000, 1.000) perf=0.3166\n",
      "lambda 01: 10000.000, mean=0.0330, (25,50,75)pctl=(-0.0305,0.0192,0.0664),(0.0<r>0.5): (020,000)\n",
      "34 responses: ridge=10000.000, temporal=1.000, spatial=(1.000, 1.000) perf=0.0330\n",
      "lambda 01: 3593.814, mean=0.1451, (25,50,75)pctl=(0.0426,0.1121,0.2639),(0.0<r>0.5): (039,000)\n",
      "46 responses: ridge= 3593.814, temporal=1.000, spatial=(1.000, 1.000) perf=0.1451\n",
      "lambda 01: 1291.550, mean=0.2694, (25,50,75)pctl=(0.1583,0.2566,0.3856),(0.0<r>0.5): (448,032)\n",
      "466 responses: ridge= 1291.550, temporal=1.000, spatial=(1.000, 1.000) perf=0.2694\n",
      "lambda 01:   21.544, mean=0.0193, (25,50,75)pctl=(0.0085,0.0245,0.0652),(0.0<r>0.5): (004,000)\n",
      "5 responses: ridge=   21.544, temporal=1.000, spatial=(1.000, 1.000) perf=0.0193\n",
      "Total duration 6.0936[mins]\n"
     ]
    }
   ],
   "source": [
    "moten_prior = spatial_priors.SphericalPrior(nfeaturesm)\n",
    "obcat_prior = spatial_priors.SphericalPrior(nfeatureso)\n",
    "temporal_prior = temporal_priors.SphericalPrior(delays=delays)\n",
    "\n",
    "fit_spherical_vox = models.estimate_stem_wmvnp([Mtrain, Otrain], Ytrain,\n",
    "                                               [Mtest, Otest],Ytest,\n",
    "                                               feature_priors=[moten_prior, obcat_prior],\n",
    "                                               temporal_prior=temporal_prior,\n",
    "                                               ridges=np.logspace(0,4,10),\n",
    "                                               folds=(1,5),\n",
    "                                               performance=True,\n",
    "                                               population_optimal=False,\n",
    "                                               verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0175  0.1727  0.3365  0.5184  0.7319  1.      1.3663  1.9292  2.9714\n",
      "  5.7894 57.29  ]\n",
      "[0.0175 0.1702 0.319  0.4602 0.5906 0.7071 0.807  0.8878 0.9478 0.9854\n",
      " 0.9998]\n",
      "[0.9998 0.9854 0.9478 0.8878 0.807  0.7071 0.5906 0.4602 0.319  0.1702\n",
      " 0.0175]\n"
     ]
    }
   ],
   "source": [
    "offset = 1.0\n",
    "angle = np.linspace(0+offset, 90 - offset, 11)\n",
    "angle = np.deg2rad(angle)\n",
    "alpha1 = np.sin(angle)\n",
    "alpha2 = np.cos(angle)\n",
    "alphas = zip(alpha1, alpha2)\n",
    "ratios = alpha1/alpha2\n",
    "print(ratios)\n",
    "print(alpha1)\n",
    "print(alpha2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0175, array([0.0175, 0.9998]))\n",
      "(0.1727, array([0.1702, 0.9854]))\n",
      "(0.3365, array([0.319 , 0.9478]))\n",
      "(0.5184, array([0.4602, 0.8878]))\n",
      "(0.7319, array([0.5906, 0.807 ]))\n",
      "(1.0, array([0.7071, 0.7071]))\n",
      "(1.3663, array([0.807 , 0.5906]))\n",
      "(1.9292, array([0.8878, 0.4602]))\n",
      "(2.9714, array([0.9478, 0.319 ]))\n",
      "(5.7894, array([0.9854, 0.1702]))\n",
      "(57.29, array([0.9998, 0.0175]))\n"
     ]
    }
   ],
   "source": [
    "for ratio in ratios:\n",
    "    dat = np.asarray([ratio, 1.0])\n",
    "    dat /= np.linalg.norm(dat)\n",
    "    print(round(ratio, 4), dat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridges = ratios\n",
    "moten_prior = spatial_priors.SphericalPrior(nfeaturesm, hyparams=[1.0])\n",
    "obcat_prior = spatial_priors.SphericalPrior(nfeatureso, hyparams=ridges)\n",
    "temporal_prior = temporal_priors.SphericalPrior(delays=delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/11: temporal 1/1=1.000, features 1/11=(0.9998, 0.0175)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1929, (25,50,75)pctl=(0.0878,0.1690,0.2880),(0.0<r>0.5): (2549,030)\n",
      "lambda 02:    2.783, mean=0.1929, (25,50,75)pctl=(0.0878,0.1690,0.2880),(0.0<r>0.5): (2549,030)\n",
      "lambda 03:    7.743, mean=0.1930, (25,50,75)pctl=(0.0878,0.1691,0.2881),(0.0<r>0.5): (2550,030)\n",
      "lambda 04:   21.544, mean=0.1937, (25,50,75)pctl=(0.0884,0.1697,0.2888),(0.0<r>0.5): (2552,030)\n",
      "lambda 05:   59.948, mean=0.1986, (25,50,75)pctl=(0.0924,0.1754,0.2946),(0.0<r>0.5): (2557,034)\n",
      "lambda 06:  166.810, mean=0.2168, (25,50,75)pctl=(0.1037,0.1942,0.3224),(0.0<r>0.5): (2575,071)\n",
      "lambda 07:  464.159, mean=0.2470, (25,50,75)pctl=(0.1212,0.2269,0.3652),(0.0<r>0.5): (2586,173)\n",
      "lambda 08: 1291.550, mean=0.2730, (25,50,75)pctl=(0.1394,0.2627,0.4020),(0.0<r>0.5): (2590,279)\n",
      "lambda 09: 3593.814, mean=0.2755, (25,50,75)pctl=(0.1452,0.2694,0.4052),(0.0<r>0.5): (2581,270)\n",
      "lambda 10: 10000.000, mean=0.2577, (25,50,75)pctl=(0.1336,0.2520,0.3779),(0.0<r>0.5): (2564,166)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1822, (25,50,75)pctl=(0.0713,0.1580,0.2841),(0.0<r>0.5): (2505,039)\n",
      "lambda 02:    2.783, mean=0.1822, (25,50,75)pctl=(0.0713,0.1580,0.2841),(0.0<r>0.5): (2505,039)\n",
      "lambda 03:    7.743, mean=0.1823, (25,50,75)pctl=(0.0715,0.1581,0.2842),(0.0<r>0.5): (2506,039)\n",
      "lambda 04:   21.544, mean=0.1833, (25,50,75)pctl=(0.0721,0.1587,0.2852),(0.0<r>0.5): (2506,040)\n",
      "lambda 05:   59.948, mean=0.1887, (25,50,75)pctl=(0.0748,0.1644,0.2930),(0.0<r>0.5): (2514,050)\n",
      "lambda 06:  166.810, mean=0.2055, (25,50,75)pctl=(0.0836,0.1797,0.3146),(0.0<r>0.5): (2522,084)\n",
      "lambda 07:  464.159, mean=0.2301, (25,50,75)pctl=(0.0976,0.2082,0.3549),(0.0<r>0.5): (2540,158)\n",
      "lambda 08: 1291.550, mean=0.2474, (25,50,75)pctl=(0.1102,0.2338,0.3762),(0.0<r>0.5): (2548,209)\n",
      "lambda 09: 3593.814, mean=0.2439, (25,50,75)pctl=(0.1137,0.2365,0.3648),(0.0<r>0.5): (2547,133)\n",
      "lambda 10: 10000.000, mean=0.2260, (25,50,75)pctl=(0.1026,0.2189,0.3419),(0.0<r>0.5): (2535,047)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1848, (25,50,75)pctl=(0.0795,0.1574,0.2767),(0.0<r>0.5): (2554,035)\n",
      "lambda 02:    2.783, mean=0.1848, (25,50,75)pctl=(0.0795,0.1574,0.2767),(0.0<r>0.5): (2554,035)\n",
      "lambda 03:    7.743, mean=0.1849, (25,50,75)pctl=(0.0796,0.1575,0.2769),(0.0<r>0.5): (2554,035)\n",
      "lambda 04:   21.544, mean=0.1858, (25,50,75)pctl=(0.0802,0.1582,0.2777),(0.0<r>0.5): (2560,035)\n",
      "lambda 05:   59.948, mean=0.1909, (25,50,75)pctl=(0.0834,0.1640,0.2859),(0.0<r>0.5): (2566,046)\n",
      "lambda 06:  166.810, mean=0.2076, (25,50,75)pctl=(0.0915,0.1813,0.3122),(0.0<r>0.5): (2577,079)\n",
      "lambda 07:  464.159, mean=0.2346, (25,50,75)pctl=(0.1072,0.2109,0.3518),(0.0<r>0.5): (2591,165)\n",
      "lambda 08: 1291.550, mean=0.2584, (25,50,75)pctl=(0.1247,0.2429,0.3824),(0.0<r>0.5): (2602,236)\n",
      "lambda 09: 3593.814, mean=0.2555, (25,50,75)pctl=(0.1282,0.2464,0.3749),(0.0<r>0.5): (2589,165)\n",
      "lambda 10: 10000.000, mean=0.2305, (25,50,75)pctl=(0.1159,0.2239,0.3352),(0.0<r>0.5): (2574,053)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1832, (25,50,75)pctl=(0.0759,0.1557,0.2757),(0.0<r>0.5): (2542,027)\n",
      "lambda 02:    2.783, mean=0.1832, (25,50,75)pctl=(0.0759,0.1557,0.2757),(0.0<r>0.5): (2542,027)\n",
      "lambda 03:    7.743, mean=0.1833, (25,50,75)pctl=(0.0760,0.1559,0.2759),(0.0<r>0.5): (2544,027)\n",
      "lambda 04:   21.544, mean=0.1843, (25,50,75)pctl=(0.0767,0.1569,0.2774),(0.0<r>0.5): (2544,033)\n",
      "lambda 05:   59.948, mean=0.1899, (25,50,75)pctl=(0.0819,0.1627,0.2859),(0.0<r>0.5): (2555,043)\n",
      "lambda 06:  166.810, mean=0.2089, (25,50,75)pctl=(0.0923,0.1850,0.3149),(0.0<r>0.5): (2570,081)\n",
      "lambda 07:  464.159, mean=0.2407, (25,50,75)pctl=(0.1130,0.2171,0.3632),(0.0<r>0.5): (2589,179)\n",
      "lambda 08: 1291.550, mean=0.2664, (25,50,75)pctl=(0.1307,0.2529,0.3967),(0.0<r>0.5): (2592,249)\n",
      "lambda 09: 3593.814, mean=0.2656, (25,50,75)pctl=(0.1405,0.2579,0.3896),(0.0<r>0.5): (2592,196)\n",
      "lambda 10: 10000.000, mean=0.2437, (25,50,75)pctl=(0.1274,0.2361,0.3552),(0.0<r>0.5): (2585,101)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1715, (25,50,75)pctl=(0.0735,0.1470,0.2553),(0.0<r>0.5): (2537,012)\n",
      "lambda 02:    2.783, mean=0.1715, (25,50,75)pctl=(0.0735,0.1471,0.2553),(0.0<r>0.5): (2537,012)\n",
      "lambda 03:    7.743, mean=0.1716, (25,50,75)pctl=(0.0735,0.1473,0.2554),(0.0<r>0.5): (2538,012)\n",
      "lambda 04:   21.544, mean=0.1726, (25,50,75)pctl=(0.0739,0.1483,0.2568),(0.0<r>0.5): (2541,013)\n",
      "lambda 05:   59.948, mean=0.1780, (25,50,75)pctl=(0.0764,0.1533,0.2654),(0.0<r>0.5): (2539,018)\n",
      "lambda 06:  166.810, mean=0.1958, (25,50,75)pctl=(0.0864,0.1715,0.2924),(0.0<r>0.5): (2555,036)\n",
      "lambda 07:  464.159, mean=0.2243, (25,50,75)pctl=(0.0993,0.2034,0.3386),(0.0<r>0.5): (2565,112)\n",
      "lambda 08: 1291.550, mean=0.2473, (25,50,75)pctl=(0.1170,0.2348,0.3678),(0.0<r>0.5): (2584,163)\n",
      "lambda 09: 3593.814, mean=0.2453, (25,50,75)pctl=(0.1272,0.2384,0.3583),(0.0<r>0.5): (2585,091)\n",
      "lambda 10: 10000.000, mean=0.2243, (25,50,75)pctl=(0.1210,0.2162,0.3291),(0.0<r>0.5): (2563,035)\n",
      "pop.cv.best: 1291.550, mean=0.2116, (25,50,75)pctl=(0.1346,0.2526,0.3863),(0.0<r>0.5): (2643,228)\n",
      "2/11: temporal 1/1=1.000, features 2/11=(0.9854, 0.1702)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2371, (25,50,75)pctl=(0.1063,0.2146,0.3597),(0.0<r>0.5): (2567,200)\n",
      "lambda 02:    2.783, mean=0.2371, (25,50,75)pctl=(0.1063,0.2146,0.3598),(0.0<r>0.5): (2567,200)\n",
      "lambda 03:    7.743, mean=0.2374, (25,50,75)pctl=(0.1066,0.2149,0.3603),(0.0<r>0.5): (2567,202)\n",
      "lambda 04:   21.544, mean=0.2398, (25,50,75)pctl=(0.1085,0.2168,0.3642),(0.0<r>0.5): (2565,210)\n",
      "lambda 05:   59.948, mean=0.2528, (25,50,75)pctl=(0.1179,0.2319,0.3829),(0.0<r>0.5): (2574,254)\n",
      "lambda 06:  166.810, mean=0.2870, (25,50,75)pctl=(0.1391,0.2721,0.4331),(0.0<r>0.5): (2593,401)\n",
      "lambda 07:  464.159, mean=0.3117, (25,50,75)pctl=(0.1592,0.3067,0.4637),(0.0<r>0.5): (2597,514)\n",
      "lambda 08: 1291.550, mean=0.2924, (25,50,75)pctl=(0.1516,0.2908,0.4348),(0.0<r>0.5): (2577,373)\n",
      "lambda 09: 3593.814, mean=0.2539, (25,50,75)pctl=(0.1270,0.2441,0.3754),(0.0<r>0.5): (2559,201)\n",
      "lambda 10: 10000.000, mean=0.2329, (25,50,75)pctl=(0.1132,0.2166,0.3453),(0.0<r>0.5): (2538,151)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2277, (25,50,75)pctl=(0.0949,0.2038,0.3436),(0.0<r>0.5): (2560,175)\n",
      "lambda 02:    2.783, mean=0.2278, (25,50,75)pctl=(0.0950,0.2039,0.3437),(0.0<r>0.5): (2560,175)\n",
      "lambda 03:    7.743, mean=0.2281, (25,50,75)pctl=(0.0952,0.2046,0.3442),(0.0<r>0.5): (2558,176)\n",
      "lambda 04:   21.544, mean=0.2304, (25,50,75)pctl=(0.0970,0.2071,0.3484),(0.0<r>0.5): (2564,180)\n",
      "lambda 05:   59.948, mean=0.2414, (25,50,75)pctl=(0.1030,0.2200,0.3657),(0.0<r>0.5): (2564,216)\n",
      "lambda 06:  166.810, mean=0.2672, (25,50,75)pctl=(0.1203,0.2533,0.4019),(0.0<r>0.5): (2589,313)\n",
      "lambda 07:  464.159, mean=0.2831, (25,50,75)pctl=(0.1371,0.2766,0.4195),(0.0<r>0.5): (2595,341)\n",
      "lambda 08: 1291.550, mean=0.2585, (25,50,75)pctl=(0.1304,0.2437,0.3760),(0.0<r>0.5): (2589,231)\n",
      "lambda 09: 3593.814, mean=0.2199, (25,50,75)pctl=(0.1072,0.1980,0.3222),(0.0<r>0.5): (2560,115)\n",
      "lambda 10: 10000.000, mean=0.2034, (25,50,75)pctl=(0.0947,0.1774,0.3024),(0.0<r>0.5): (2546,078)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2327, (25,50,75)pctl=(0.1020,0.2084,0.3530),(0.0<r>0.5): (2584,180)\n",
      "lambda 02:    2.783, mean=0.2328, (25,50,75)pctl=(0.1020,0.2084,0.3531),(0.0<r>0.5): (2584,180)\n",
      "lambda 03:    7.743, mean=0.2332, (25,50,75)pctl=(0.1023,0.2091,0.3536),(0.0<r>0.5): (2586,181)\n",
      "lambda 04:   21.544, mean=0.2359, (25,50,75)pctl=(0.1042,0.2125,0.3578),(0.0<r>0.5): (2592,189)\n",
      "lambda 05:   59.948, mean=0.2489, (25,50,75)pctl=(0.1107,0.2261,0.3749),(0.0<r>0.5): (2595,233)\n",
      "lambda 06:  166.810, mean=0.2776, (25,50,75)pctl=(0.1320,0.2645,0.4163),(0.0<r>0.5): (2613,336)\n",
      "lambda 07:  464.159, mean=0.2947, (25,50,75)pctl=(0.1480,0.2898,0.4368),(0.0<r>0.5): (2622,375)\n",
      "lambda 08: 1291.550, mean=0.2750, (25,50,75)pctl=(0.1446,0.2685,0.3993),(0.0<r>0.5): (2614,254)\n",
      "lambda 09: 3593.814, mean=0.2375, (25,50,75)pctl=(0.1211,0.2271,0.3438),(0.0<r>0.5): (2582,135)\n",
      "lambda 10: 10000.000, mean=0.2150, (25,50,75)pctl=(0.1044,0.2001,0.3116),(0.0<r>0.5): (2557,083)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2254, (25,50,75)pctl=(0.0978,0.2017,0.3421),(0.0<r>0.5): (2570,146)\n",
      "lambda 02:    2.783, mean=0.2254, (25,50,75)pctl=(0.0979,0.2017,0.3422),(0.0<r>0.5): (2570,146)\n",
      "lambda 03:    7.743, mean=0.2258, (25,50,75)pctl=(0.0981,0.2021,0.3429),(0.0<r>0.5): (2570,147)\n",
      "lambda 04:   21.544, mean=0.2286, (25,50,75)pctl=(0.0992,0.2046,0.3471),(0.0<r>0.5): (2572,164)\n",
      "lambda 05:   59.948, mean=0.2425, (25,50,75)pctl=(0.1076,0.2172,0.3679),(0.0<r>0.5): (2575,213)\n",
      "lambda 06:  166.810, mean=0.2775, (25,50,75)pctl=(0.1307,0.2587,0.4193),(0.0<r>0.5): (2592,364)\n",
      "lambda 07:  464.159, mean=0.3043, (25,50,75)pctl=(0.1565,0.2961,0.4487),(0.0<r>0.5): (2604,446)\n",
      "lambda 08: 1291.550, mean=0.2850, (25,50,75)pctl=(0.1536,0.2760,0.4160),(0.0<r>0.5): (2603,289)\n",
      "lambda 09: 3593.814, mean=0.2352, (25,50,75)pctl=(0.1206,0.2267,0.3426),(0.0<r>0.5): (2578,083)\n",
      "lambda 10: 10000.000, mean=0.2052, (25,50,75)pctl=(0.0993,0.1929,0.2995),(0.0<r>0.5): (2554,027)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2242, (25,50,75)pctl=(0.1036,0.1992,0.3344),(0.0<r>0.5): (2597,143)\n",
      "lambda 02:    2.783, mean=0.2243, (25,50,75)pctl=(0.1036,0.1992,0.3345),(0.0<r>0.5): (2597,143)\n",
      "lambda 03:    7.743, mean=0.2246, (25,50,75)pctl=(0.1038,0.1995,0.3348),(0.0<r>0.5): (2597,143)\n",
      "lambda 04:   21.544, mean=0.2269, (25,50,75)pctl=(0.1045,0.2017,0.3385),(0.0<r>0.5): (2597,151)\n",
      "lambda 05:   59.948, mean=0.2383, (25,50,75)pctl=(0.1095,0.2139,0.3553),(0.0<r>0.5): (2594,190)\n",
      "lambda 06:  166.810, mean=0.2672, (25,50,75)pctl=(0.1238,0.2480,0.3980),(0.0<r>0.5): (2598,300)\n",
      "lambda 07:  464.159, mean=0.2929, (25,50,75)pctl=(0.1485,0.2824,0.4341),(0.0<r>0.5): (2600,373)\n",
      "lambda 08: 1291.550, mean=0.2825, (25,50,75)pctl=(0.1541,0.2755,0.4091),(0.0<r>0.5): (2602,273)\n",
      "lambda 09: 3593.814, mean=0.2502, (25,50,75)pctl=(0.1340,0.2427,0.3629),(0.0<r>0.5): (2596,129)\n",
      "lambda 10: 10000.000, mean=0.2312, (25,50,75)pctl=(0.1199,0.2238,0.3389),(0.0<r>0.5): (2572,045)\n",
      "pop.cv.best: 464.159, mean=0.2474, (25,50,75)pctl=(0.1554,0.2900,0.4380),(0.0<r>0.5): (2648,404)\n",
      "3/11: temporal 1/1=1.000, features 3/11=(0.9478, 0.3190)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2295, (25,50,75)pctl=(0.0977,0.2046,0.3488),(0.0<r>0.5): (2552,183)\n",
      "lambda 02:    2.783, mean=0.2295, (25,50,75)pctl=(0.0977,0.2047,0.3489),(0.0<r>0.5): (2552,183)\n",
      "lambda 03:    7.743, mean=0.2298, (25,50,75)pctl=(0.0979,0.2052,0.3495),(0.0<r>0.5): (2552,184)\n",
      "lambda 04:   21.544, mean=0.2322, (25,50,75)pctl=(0.0994,0.2083,0.3529),(0.0<r>0.5): (2555,190)\n",
      "lambda 05:   59.948, mean=0.2453, (25,50,75)pctl=(0.1088,0.2246,0.3727),(0.0<r>0.5): (2562,246)\n",
      "lambda 06:  166.810, mean=0.2793, (25,50,75)pctl=(0.1311,0.2618,0.4209),(0.0<r>0.5): (2588,376)\n",
      "lambda 07:  464.159, mean=0.3021, (25,50,75)pctl=(0.1512,0.2936,0.4508),(0.0<r>0.5): (2588,469)\n",
      "lambda 08: 1291.550, mean=0.2844, (25,50,75)pctl=(0.1433,0.2804,0.4209),(0.0<r>0.5): (2576,338)\n",
      "lambda 09: 3593.814, mean=0.2486, (25,50,75)pctl=(0.1209,0.2346,0.3679),(0.0<r>0.5): (2553,203)\n",
      "lambda 10: 10000.000, mean=0.2280, (25,50,75)pctl=(0.1088,0.2099,0.3377),(0.0<r>0.5): (2529,152)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2236, (25,50,75)pctl=(0.0915,0.1990,0.3380),(0.0<r>0.5): (2562,173)\n",
      "lambda 02:    2.783, mean=0.2237, (25,50,75)pctl=(0.0916,0.1990,0.3380),(0.0<r>0.5): (2562,173)\n",
      "lambda 03:    7.743, mean=0.2240, (25,50,75)pctl=(0.0917,0.1995,0.3383),(0.0<r>0.5): (2561,174)\n",
      "lambda 04:   21.544, mean=0.2262, (25,50,75)pctl=(0.0930,0.2016,0.3414),(0.0<r>0.5): (2561,180)\n",
      "lambda 05:   59.948, mean=0.2368, (25,50,75)pctl=(0.0982,0.2127,0.3575),(0.0<r>0.5): (2565,212)\n",
      "lambda 06:  166.810, mean=0.2617, (25,50,75)pctl=(0.1139,0.2452,0.3939),(0.0<r>0.5): (2579,289)\n",
      "lambda 07:  464.159, mean=0.2753, (25,50,75)pctl=(0.1280,0.2625,0.4082),(0.0<r>0.5): (2586,327)\n",
      "lambda 08: 1291.550, mean=0.2517, (25,50,75)pctl=(0.1230,0.2332,0.3697),(0.0<r>0.5): (2585,230)\n",
      "lambda 09: 3593.814, mean=0.2152, (25,50,75)pctl=(0.1026,0.1904,0.3188),(0.0<r>0.5): (2554,115)\n",
      "lambda 10: 10000.000, mean=0.1993, (25,50,75)pctl=(0.0893,0.1688,0.3006),(0.0<r>0.5): (2537,078)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2261, (25,50,75)pctl=(0.0954,0.2015,0.3438),(0.0<r>0.5): (2577,162)\n",
      "lambda 02:    2.783, mean=0.2262, (25,50,75)pctl=(0.0954,0.2016,0.3439),(0.0<r>0.5): (2577,162)\n",
      "lambda 03:    7.743, mean=0.2266, (25,50,75)pctl=(0.0956,0.2020,0.3445),(0.0<r>0.5): (2577,162)\n",
      "lambda 04:   21.544, mean=0.2293, (25,50,75)pctl=(0.0969,0.2044,0.3491),(0.0<r>0.5): (2578,170)\n",
      "lambda 05:   59.948, mean=0.2422, (25,50,75)pctl=(0.1055,0.2190,0.3675),(0.0<r>0.5): (2588,209)\n",
      "lambda 06:  166.810, mean=0.2703, (25,50,75)pctl=(0.1240,0.2532,0.4051),(0.0<r>0.5): (2611,302)\n",
      "lambda 07:  464.159, mean=0.2853, (25,50,75)pctl=(0.1403,0.2768,0.4234),(0.0<r>0.5): (2619,328)\n",
      "lambda 08: 1291.550, mean=0.2675, (25,50,75)pctl=(0.1374,0.2575,0.3851),(0.0<r>0.5): (2609,251)\n",
      "lambda 09: 3593.814, mean=0.2327, (25,50,75)pctl=(0.1142,0.2185,0.3379),(0.0<r>0.5): (2574,137)\n",
      "lambda 10: 10000.000, mean=0.2106, (25,50,75)pctl=(0.0982,0.1946,0.3090),(0.0<r>0.5): (2550,083)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2179, (25,50,75)pctl=(0.0922,0.1925,0.3306),(0.0<r>0.5): (2561,131)\n",
      "lambda 02:    2.783, mean=0.2180, (25,50,75)pctl=(0.0923,0.1926,0.3307),(0.0<r>0.5): (2561,131)\n",
      "lambda 03:    7.743, mean=0.2184, (25,50,75)pctl=(0.0925,0.1932,0.3313),(0.0<r>0.5): (2561,133)\n",
      "lambda 04:   21.544, mean=0.2211, (25,50,75)pctl=(0.0941,0.1954,0.3354),(0.0<r>0.5): (2562,141)\n",
      "lambda 05:   59.948, mean=0.2350, (25,50,75)pctl=(0.1016,0.2099,0.3575),(0.0<r>0.5): (2565,178)\n",
      "lambda 06:  166.810, mean=0.2698, (25,50,75)pctl=(0.1242,0.2489,0.4085),(0.0<r>0.5): (2586,330)\n",
      "lambda 07:  464.159, mean=0.2940, (25,50,75)pctl=(0.1483,0.2846,0.4385),(0.0<r>0.5): (2601,407)\n",
      "lambda 08: 1291.550, mean=0.2759, (25,50,75)pctl=(0.1437,0.2668,0.4035),(0.0<r>0.5): (2593,257)\n",
      "lambda 09: 3593.814, mean=0.2290, (25,50,75)pctl=(0.1137,0.2198,0.3345),(0.0<r>0.5): (2569,083)\n",
      "lambda 10: 10000.000, mean=0.1997, (25,50,75)pctl=(0.0938,0.1847,0.2924),(0.0<r>0.5): (2547,027)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2215, (25,50,75)pctl=(0.0992,0.1952,0.3315),(0.0<r>0.5): (2588,141)\n",
      "lambda 02:    2.783, mean=0.2215, (25,50,75)pctl=(0.0992,0.1952,0.3316),(0.0<r>0.5): (2588,141)\n",
      "lambda 03:    7.743, mean=0.2219, (25,50,75)pctl=(0.0993,0.1956,0.3320),(0.0<r>0.5): (2588,141)\n",
      "lambda 04:   21.544, mean=0.2241, (25,50,75)pctl=(0.0999,0.1975,0.3352),(0.0<r>0.5): (2588,148)\n",
      "lambda 05:   59.948, mean=0.2350, (25,50,75)pctl=(0.1047,0.2120,0.3520),(0.0<r>0.5): (2591,193)\n",
      "lambda 06:  166.810, mean=0.2623, (25,50,75)pctl=(0.1195,0.2418,0.3955),(0.0<r>0.5): (2592,288)\n",
      "lambda 07:  464.159, mean=0.2851, (25,50,75)pctl=(0.1429,0.2719,0.4228),(0.0<r>0.5): (2595,350)\n",
      "lambda 08: 1291.550, mean=0.2757, (25,50,75)pctl=(0.1478,0.2651,0.4020),(0.0<r>0.5): (2595,270)\n",
      "lambda 09: 3593.814, mean=0.2454, (25,50,75)pctl=(0.1296,0.2357,0.3564),(0.0<r>0.5): (2593,130)\n",
      "lambda 10: 10000.000, mean=0.2269, (25,50,75)pctl=(0.1161,0.2184,0.3311),(0.0<r>0.5): (2568,045)\n",
      "pop.cv.best: 464.159, mean=0.2412, (25,50,75)pctl=(0.1479,0.2781,0.4255),(0.0<r>0.5): (2646,364)\n",
      "4/11: temporal 1/1=1.000, features 4/11=(0.8878, 0.4602)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2232, (25,50,75)pctl=(0.0924,0.1970,0.3385),(0.0<r>0.5): (2548,171)\n",
      "lambda 02:    2.783, mean=0.2233, (25,50,75)pctl=(0.0924,0.1970,0.3386),(0.0<r>0.5): (2549,171)\n",
      "lambda 03:    7.743, mean=0.2235, (25,50,75)pctl=(0.0927,0.1973,0.3390),(0.0<r>0.5): (2549,171)\n",
      "lambda 04:   21.544, mean=0.2257, (25,50,75)pctl=(0.0945,0.1995,0.3424),(0.0<r>0.5): (2550,172)\n",
      "lambda 05:   59.948, mean=0.2378, (25,50,75)pctl=(0.1026,0.2133,0.3614),(0.0<r>0.5): (2561,219)\n",
      "lambda 06:  166.810, mean=0.2712, (25,50,75)pctl=(0.1237,0.2523,0.4076),(0.0<r>0.5): (2579,350)\n",
      "lambda 07:  464.159, mean=0.2970, (25,50,75)pctl=(0.1458,0.2855,0.4428),(0.0<r>0.5): (2589,449)\n",
      "lambda 08: 1291.550, mean=0.2837, (25,50,75)pctl=(0.1409,0.2766,0.4179),(0.0<r>0.5): (2574,335)\n",
      "lambda 09: 3593.814, mean=0.2490, (25,50,75)pctl=(0.1200,0.2346,0.3688),(0.0<r>0.5): (2552,210)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 10: 10000.000, mean=0.2275, (25,50,75)pctl=(0.1085,0.2093,0.3377),(0.0<r>0.5): (2527,154)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2190, (25,50,75)pctl=(0.0897,0.1938,0.3318),(0.0<r>0.5): (2561,162)\n",
      "lambda 02:    2.783, mean=0.2191, (25,50,75)pctl=(0.0897,0.1938,0.3318),(0.0<r>0.5): (2561,162)\n",
      "lambda 03:    7.743, mean=0.2194, (25,50,75)pctl=(0.0898,0.1940,0.3321),(0.0<r>0.5): (2560,162)\n",
      "lambda 04:   21.544, mean=0.2213, (25,50,75)pctl=(0.0907,0.1959,0.3349),(0.0<r>0.5): (2558,169)\n",
      "lambda 05:   59.948, mean=0.2311, (25,50,75)pctl=(0.0961,0.2066,0.3463),(0.0<r>0.5): (2563,203)\n",
      "lambda 06:  166.810, mean=0.2554, (25,50,75)pctl=(0.1100,0.2369,0.3846),(0.0<r>0.5): (2574,274)\n",
      "lambda 07:  464.159, mean=0.2715, (25,50,75)pctl=(0.1258,0.2587,0.4030),(0.0<r>0.5): (2584,321)\n",
      "lambda 08: 1291.550, mean=0.2517, (25,50,75)pctl=(0.1221,0.2332,0.3701),(0.0<r>0.5): (2584,239)\n",
      "lambda 09: 3593.814, mean=0.2156, (25,50,75)pctl=(0.1028,0.1898,0.3204),(0.0<r>0.5): (2555,117)\n",
      "lambda 10: 10000.000, mean=0.1988, (25,50,75)pctl=(0.0886,0.1678,0.3011),(0.0<r>0.5): (2533,078)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2205, (25,50,75)pctl=(0.0913,0.1950,0.3347),(0.0<r>0.5): (2576,147)\n",
      "lambda 02:    2.783, mean=0.2206, (25,50,75)pctl=(0.0913,0.1950,0.3348),(0.0<r>0.5): (2576,147)\n",
      "lambda 03:    7.743, mean=0.2209, (25,50,75)pctl=(0.0916,0.1953,0.3355),(0.0<r>0.5): (2576,147)\n",
      "lambda 04:   21.544, mean=0.2234, (25,50,75)pctl=(0.0934,0.1974,0.3390),(0.0<r>0.5): (2578,152)\n",
      "lambda 05:   59.948, mean=0.2353, (25,50,75)pctl=(0.1000,0.2107,0.3571),(0.0<r>0.5): (2583,187)\n",
      "lambda 06:  166.810, mean=0.2630, (25,50,75)pctl=(0.1184,0.2452,0.3965),(0.0<r>0.5): (2604,271)\n",
      "lambda 07:  464.159, mean=0.2807, (25,50,75)pctl=(0.1365,0.2691,0.4165),(0.0<r>0.5): (2616,317)\n",
      "lambda 08: 1291.550, mean=0.2668, (25,50,75)pctl=(0.1364,0.2558,0.3847),(0.0<r>0.5): (2608,252)\n",
      "lambda 09: 3593.814, mean=0.2334, (25,50,75)pctl=(0.1131,0.2185,0.3387),(0.0<r>0.5): (2576,141)\n",
      "lambda 10: 10000.000, mean=0.2102, (25,50,75)pctl=(0.0972,0.1934,0.3094),(0.0<r>0.5): (2549,083)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2120, (25,50,75)pctl=(0.0882,0.1855,0.3233),(0.0<r>0.5): (2552,121)\n",
      "lambda 02:    2.783, mean=0.2120, (25,50,75)pctl=(0.0883,0.1855,0.3233),(0.0<r>0.5): (2552,121)\n",
      "lambda 03:    7.743, mean=0.2124, (25,50,75)pctl=(0.0886,0.1858,0.3237),(0.0<r>0.5): (2551,121)\n",
      "lambda 04:   21.544, mean=0.2149, (25,50,75)pctl=(0.0899,0.1884,0.3282),(0.0<r>0.5): (2551,127)\n",
      "lambda 05:   59.948, mean=0.2277, (25,50,75)pctl=(0.0972,0.2016,0.3471),(0.0<r>0.5): (2561,164)\n",
      "lambda 06:  166.810, mean=0.2617, (25,50,75)pctl=(0.1194,0.2401,0.3968),(0.0<r>0.5): (2575,292)\n",
      "lambda 07:  464.159, mean=0.2884, (25,50,75)pctl=(0.1430,0.2780,0.4317),(0.0<r>0.5): (2597,378)\n",
      "lambda 08: 1291.550, mean=0.2750, (25,50,75)pctl=(0.1423,0.2659,0.4042),(0.0<r>0.5): (2591,258)\n",
      "lambda 09: 3593.814, mean=0.2300, (25,50,75)pctl=(0.1137,0.2203,0.3366),(0.0<r>0.5): (2568,092)\n",
      "lambda 10: 10000.000, mean=0.1993, (25,50,75)pctl=(0.0924,0.1838,0.2930),(0.0<r>0.5): (2543,027)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2176, (25,50,75)pctl=(0.0955,0.1918,0.3250),(0.0<r>0.5): (2581,138)\n",
      "lambda 02:    2.783, mean=0.2177, (25,50,75)pctl=(0.0955,0.1918,0.3251),(0.0<r>0.5): (2581,138)\n",
      "lambda 03:    7.743, mean=0.2180, (25,50,75)pctl=(0.0958,0.1920,0.3256),(0.0<r>0.5): (2581,139)\n",
      "lambda 04:   21.544, mean=0.2199, (25,50,75)pctl=(0.0965,0.1939,0.3293),(0.0<r>0.5): (2580,146)\n",
      "lambda 05:   59.948, mean=0.2299, (25,50,75)pctl=(0.1012,0.2061,0.3458),(0.0<r>0.5): (2587,178)\n",
      "lambda 06:  166.810, mean=0.2560, (25,50,75)pctl=(0.1151,0.2333,0.3883),(0.0<r>0.5): (2588,274)\n",
      "lambda 07:  464.159, mean=0.2804, (25,50,75)pctl=(0.1380,0.2662,0.4194),(0.0<r>0.5): (2593,348)\n",
      "lambda 08: 1291.550, mean=0.2747, (25,50,75)pctl=(0.1457,0.2624,0.4019),(0.0<r>0.5): (2592,272)\n",
      "lambda 09: 3593.814, mean=0.2458, (25,50,75)pctl=(0.1292,0.2354,0.3569),(0.0<r>0.5): (2592,137)\n",
      "lambda 10: 10000.000, mean=0.2264, (25,50,75)pctl=(0.1157,0.2174,0.3300),(0.0<r>0.5): (2568,047)\n",
      "pop.cv.best: 464.159, mean=0.2372, (25,50,75)pctl=(0.1438,0.2715,0.4176),(0.0<r>0.5): (2646,345)\n",
      "5/11: temporal 1/1=1.000, features 5/11=(0.8070, 0.5906)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2187, (25,50,75)pctl=(0.0895,0.1912,0.3321),(0.0<r>0.5): (2541,159)\n",
      "lambda 02:    2.783, mean=0.2187, (25,50,75)pctl=(0.0896,0.1913,0.3321),(0.0<r>0.5): (2541,160)\n",
      "lambda 03:    7.743, mean=0.2189, (25,50,75)pctl=(0.0898,0.1916,0.3327),(0.0<r>0.5): (2542,160)\n",
      "lambda 04:   21.544, mean=0.2207, (25,50,75)pctl=(0.0905,0.1936,0.3358),(0.0<r>0.5): (2543,168)\n",
      "lambda 05:   59.948, mean=0.2314, (25,50,75)pctl=(0.0973,0.2045,0.3514),(0.0<r>0.5): (2553,198)\n",
      "lambda 06:  166.810, mean=0.2637, (25,50,75)pctl=(0.1178,0.2430,0.3974),(0.0<r>0.5): (2577,326)\n",
      "lambda 07:  464.159, mean=0.2937, (25,50,75)pctl=(0.1431,0.2810,0.4386),(0.0<r>0.5): (2589,437)\n",
      "lambda 08: 1291.550, mean=0.2852, (25,50,75)pctl=(0.1413,0.2771,0.4197),(0.0<r>0.5): (2576,352)\n",
      "lambda 09: 3593.814, mean=0.2515, (25,50,75)pctl=(0.1218,0.2379,0.3712),(0.0<r>0.5): (2554,223)\n",
      "lambda 10: 10000.000, mean=0.2281, (25,50,75)pctl=(0.1080,0.2102,0.3386),(0.0<r>0.5): (2527,158)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2155, (25,50,75)pctl=(0.0882,0.1891,0.3254),(0.0<r>0.5): (2555,155)\n",
      "lambda 02:    2.783, mean=0.2155, (25,50,75)pctl=(0.0882,0.1891,0.3255),(0.0<r>0.5): (2555,155)\n",
      "lambda 03:    7.743, mean=0.2157, (25,50,75)pctl=(0.0883,0.1894,0.3257),(0.0<r>0.5): (2554,155)\n",
      "lambda 04:   21.544, mean=0.2174, (25,50,75)pctl=(0.0891,0.1909,0.3278),(0.0<r>0.5): (2555,163)\n",
      "lambda 05:   59.948, mean=0.2260, (25,50,75)pctl=(0.0929,0.1998,0.3393),(0.0<r>0.5): (2562,191)\n",
      "lambda 06:  166.810, mean=0.2497, (25,50,75)pctl=(0.1058,0.2295,0.3761),(0.0<r>0.5): (2574,253)\n",
      "lambda 07:  464.159, mean=0.2693, (25,50,75)pctl=(0.1226,0.2544,0.4002),(0.0<r>0.5): (2582,320)\n",
      "lambda 08: 1291.550, mean=0.2539, (25,50,75)pctl=(0.1227,0.2360,0.3735),(0.0<r>0.5): (2584,251)\n",
      "lambda 09: 3593.814, mean=0.2178, (25,50,75)pctl=(0.1038,0.1921,0.3241),(0.0<r>0.5): (2556,124)\n",
      "lambda 10: 10000.000, mean=0.1992, (25,50,75)pctl=(0.0893,0.1680,0.3016),(0.0<r>0.5): (2533,078)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2163, (25,50,75)pctl=(0.0886,0.1888,0.3284),(0.0<r>0.5): (2574,138)\n",
      "lambda 02:    2.783, mean=0.2163, (25,50,75)pctl=(0.0886,0.1889,0.3284),(0.0<r>0.5): (2574,138)\n",
      "lambda 03:    7.743, mean=0.2166, (25,50,75)pctl=(0.0889,0.1891,0.3291),(0.0<r>0.5): (2574,138)\n",
      "lambda 04:   21.544, mean=0.2187, (25,50,75)pctl=(0.0906,0.1917,0.3324),(0.0<r>0.5): (2574,148)\n",
      "lambda 05:   59.948, mean=0.2293, (25,50,75)pctl=(0.0972,0.2022,0.3475),(0.0<r>0.5): (2583,171)\n",
      "lambda 06:  166.810, mean=0.2564, (25,50,75)pctl=(0.1143,0.2370,0.3869),(0.0<r>0.5): (2597,254)\n",
      "lambda 07:  464.159, mean=0.2779, (25,50,75)pctl=(0.1335,0.2643,0.4127),(0.0<r>0.5): (2612,314)\n",
      "lambda 08: 1291.550, mean=0.2681, (25,50,75)pctl=(0.1364,0.2576,0.3874),(0.0<r>0.5): (2604,261)\n",
      "lambda 09: 3593.814, mean=0.2360, (25,50,75)pctl=(0.1150,0.2218,0.3431),(0.0<r>0.5): (2576,150)\n",
      "lambda 10: 10000.000, mean=0.2110, (25,50,75)pctl=(0.0978,0.1942,0.3104),(0.0<r>0.5): (2548,083)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2078, (25,50,75)pctl=(0.0862,0.1801,0.3172),(0.0<r>0.5): (2541,112)\n",
      "lambda 02:    2.783, mean=0.2078, (25,50,75)pctl=(0.0863,0.1801,0.3172),(0.0<r>0.5): (2541,112)\n",
      "lambda 03:    7.743, mean=0.2081, (25,50,75)pctl=(0.0863,0.1804,0.3177),(0.0<r>0.5): (2541,114)\n",
      "lambda 04:   21.544, mean=0.2102, (25,50,75)pctl=(0.0877,0.1825,0.3207),(0.0<r>0.5): (2542,118)\n",
      "lambda 05:   59.948, mean=0.2215, (25,50,75)pctl=(0.0942,0.1937,0.3384),(0.0<r>0.5): (2548,155)\n",
      "lambda 06:  166.810, mean=0.2543, (25,50,75)pctl=(0.1141,0.2311,0.3852),(0.0<r>0.5): (2573,270)\n",
      "lambda 07:  464.159, mean=0.2847, (25,50,75)pctl=(0.1384,0.2736,0.4258),(0.0<r>0.5): (2594,363)\n",
      "lambda 08: 1291.550, mean=0.2767, (25,50,75)pctl=(0.1419,0.2678,0.4059),(0.0<r>0.5): (2594,280)\n",
      "lambda 09: 3593.814, mean=0.2336, (25,50,75)pctl=(0.1163,0.2242,0.3415),(0.0<r>0.5): (2568,106)\n",
      "lambda 10: 10000.000, mean=0.2004, (25,50,75)pctl=(0.0927,0.1845,0.2942),(0.0<r>0.5): (2545,029)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    1.000, mean=0.2146, (25,50,75)pctl=(0.0931,0.1880,0.3217),(0.0<r>0.5): (2575,134)\n",
      "lambda 02:    2.783, mean=0.2146, (25,50,75)pctl=(0.0931,0.1880,0.3217),(0.0<r>0.5): (2575,134)\n",
      "lambda 03:    7.743, mean=0.2148, (25,50,75)pctl=(0.0933,0.1882,0.3220),(0.0<r>0.5): (2575,134)\n",
      "lambda 04:   21.544, mean=0.2164, (25,50,75)pctl=(0.0940,0.1896,0.3247),(0.0<r>0.5): (2576,142)\n",
      "lambda 05:   59.948, mean=0.2252, (25,50,75)pctl=(0.0981,0.1986,0.3392),(0.0<r>0.5): (2582,170)\n",
      "lambda 06:  166.810, mean=0.2501, (25,50,75)pctl=(0.1107,0.2271,0.3791),(0.0<r>0.5): (2583,264)\n",
      "lambda 07:  464.159, mean=0.2770, (25,50,75)pctl=(0.1345,0.2606,0.4147),(0.0<r>0.5): (2593,346)\n",
      "lambda 08: 1291.550, mean=0.2756, (25,50,75)pctl=(0.1457,0.2629,0.4037),(0.0<r>0.5): (2593,282)\n",
      "lambda 09: 3593.814, mean=0.2479, (25,50,75)pctl=(0.1302,0.2368,0.3604),(0.0<r>0.5): (2592,147)\n",
      "lambda 10: 10000.000, mean=0.2270, (25,50,75)pctl=(0.1161,0.2181,0.3310),(0.0<r>0.5): (2568,052)\n",
      "pop.cv.best: 464.159, mean=0.2345, (25,50,75)pctl=(0.1421,0.2691,0.4151),(0.0<r>0.5): (2646,342)\n",
      "6/11: temporal 1/1=1.000, features 6/11=(0.7071, 0.7071)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2154, (25,50,75)pctl=(0.0880,0.1874,0.3286),(0.0<r>0.5): (2537,157)\n",
      "lambda 02:    2.783, mean=0.2154, (25,50,75)pctl=(0.0881,0.1874,0.3286),(0.0<r>0.5): (2537,157)\n",
      "lambda 03:    7.743, mean=0.2156, (25,50,75)pctl=(0.0882,0.1876,0.3289),(0.0<r>0.5): (2537,157)\n",
      "lambda 04:   21.544, mean=0.2170, (25,50,75)pctl=(0.0890,0.1888,0.3307),(0.0<r>0.5): (2539,161)\n",
      "lambda 05:   59.948, mean=0.2258, (25,50,75)pctl=(0.0937,0.1978,0.3431),(0.0<r>0.5): (2549,181)\n",
      "lambda 06:  166.810, mean=0.2560, (25,50,75)pctl=(0.1123,0.2321,0.3860),(0.0<r>0.5): (2572,296)\n",
      "lambda 07:  464.159, mean=0.2902, (25,50,75)pctl=(0.1391,0.2758,0.4339),(0.0<r>0.5): (2590,430)\n",
      "lambda 08: 1291.550, mean=0.2878, (25,50,75)pctl=(0.1428,0.2805,0.4249),(0.0<r>0.5): (2576,373)\n",
      "lambda 09: 3593.814, mean=0.2556, (25,50,75)pctl=(0.1250,0.2423,0.3775),(0.0<r>0.5): (2556,235)\n",
      "lambda 10: 10000.000, mean=0.2297, (25,50,75)pctl=(0.1086,0.2117,0.3405),(0.0<r>0.5): (2527,161)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2128, (25,50,75)pctl=(0.0866,0.1864,0.3216),(0.0<r>0.5): (2553,151)\n",
      "lambda 02:    2.783, mean=0.2129, (25,50,75)pctl=(0.0866,0.1864,0.3217),(0.0<r>0.5): (2552,151)\n",
      "lambda 03:    7.743, mean=0.2130, (25,50,75)pctl=(0.0867,0.1865,0.3221),(0.0<r>0.5): (2553,151)\n",
      "lambda 04:   21.544, mean=0.2143, (25,50,75)pctl=(0.0878,0.1873,0.3233),(0.0<r>0.5): (2554,155)\n",
      "lambda 05:   59.948, mean=0.2216, (25,50,75)pctl=(0.0906,0.1946,0.3336),(0.0<r>0.5): (2562,176)\n",
      "lambda 06:  166.810, mean=0.2438, (25,50,75)pctl=(0.1030,0.2232,0.3675),(0.0<r>0.5): (2572,247)\n",
      "lambda 07:  464.159, mean=0.2671, (25,50,75)pctl=(0.1215,0.2507,0.3965),(0.0<r>0.5): (2578,316)\n",
      "lambda 08: 1291.550, mean=0.2574, (25,50,75)pctl=(0.1240,0.2398,0.3798),(0.0<r>0.5): (2588,266)\n",
      "lambda 09: 3593.814, mean=0.2216, (25,50,75)pctl=(0.1048,0.1959,0.3293),(0.0<r>0.5): (2559,135)\n",
      "lambda 10: 10000.000, mean=0.2003, (25,50,75)pctl=(0.0897,0.1689,0.3029),(0.0<r>0.5): (2536,079)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2132, (25,50,75)pctl=(0.0871,0.1846,0.3216),(0.0<r>0.5): (2569,132)\n",
      "lambda 02:    2.783, mean=0.2133, (25,50,75)pctl=(0.0871,0.1846,0.3217),(0.0<r>0.5): (2569,132)\n",
      "lambda 03:    7.743, mean=0.2135, (25,50,75)pctl=(0.0872,0.1848,0.3221),(0.0<r>0.5): (2569,133)\n",
      "lambda 04:   21.544, mean=0.2151, (25,50,75)pctl=(0.0880,0.1867,0.3248),(0.0<r>0.5): (2571,142)\n",
      "lambda 05:   59.948, mean=0.2240, (25,50,75)pctl=(0.0940,0.1971,0.3382),(0.0<r>0.5): (2577,160)\n",
      "lambda 06:  166.810, mean=0.2498, (25,50,75)pctl=(0.1097,0.2281,0.3771),(0.0<r>0.5): (2592,235)\n",
      "lambda 07:  464.159, mean=0.2752, (25,50,75)pctl=(0.1305,0.2613,0.4097),(0.0<r>0.5): (2612,313)\n",
      "lambda 08: 1291.550, mean=0.2705, (25,50,75)pctl=(0.1356,0.2599,0.3921),(0.0<r>0.5): (2604,266)\n",
      "lambda 09: 3593.814, mean=0.2402, (25,50,75)pctl=(0.1181,0.2267,0.3483),(0.0<r>0.5): (2580,161)\n",
      "lambda 10: 10000.000, mean=0.2128, (25,50,75)pctl=(0.0990,0.1963,0.3121),(0.0<r>0.5): (2552,090)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2049, (25,50,75)pctl=(0.0843,0.1753,0.3108),(0.0<r>0.5): (2537,111)\n",
      "lambda 02:    2.783, mean=0.2049, (25,50,75)pctl=(0.0843,0.1753,0.3109),(0.0<r>0.5): (2537,111)\n",
      "lambda 03:    7.743, mean=0.2051, (25,50,75)pctl=(0.0845,0.1755,0.3114),(0.0<r>0.5): (2537,112)\n",
      "lambda 04:   21.544, mean=0.2068, (25,50,75)pctl=(0.0857,0.1770,0.3144),(0.0<r>0.5): (2539,114)\n",
      "lambda 05:   59.948, mean=0.2162, (25,50,75)pctl=(0.0910,0.1867,0.3301),(0.0<r>0.5): (2543,142)\n",
      "lambda 06:  166.810, mean=0.2466, (25,50,75)pctl=(0.1093,0.2219,0.3749),(0.0<r>0.5): (2566,243)\n",
      "lambda 07:  464.159, mean=0.2809, (25,50,75)pctl=(0.1346,0.2680,0.4209),(0.0<r>0.5): (2588,353)\n",
      "lambda 08: 1291.550, mean=0.2794, (25,50,75)pctl=(0.1426,0.2709,0.4107),(0.0<r>0.5): (2595,300)\n",
      "lambda 09: 3593.814, mean=0.2393, (25,50,75)pctl=(0.1200,0.2310,0.3483),(0.0<r>0.5): (2571,128)\n",
      "lambda 10: 10000.000, mean=0.2026, (25,50,75)pctl=(0.0937,0.1872,0.2969),(0.0<r>0.5): (2546,031)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2123, (25,50,75)pctl=(0.0913,0.1848,0.3169),(0.0<r>0.5): (2572,129)\n",
      "lambda 02:    2.783, mean=0.2123, (25,50,75)pctl=(0.0913,0.1848,0.3170),(0.0<r>0.5): (2572,130)\n",
      "lambda 03:    7.743, mean=0.2125, (25,50,75)pctl=(0.0913,0.1849,0.3174),(0.0<r>0.5): (2572,131)\n",
      "lambda 04:   21.544, mean=0.2138, (25,50,75)pctl=(0.0916,0.1865,0.3200),(0.0<r>0.5): (2574,139)\n",
      "lambda 05:   59.948, mean=0.2210, (25,50,75)pctl=(0.0953,0.1939,0.3318),(0.0<r>0.5): (2575,160)\n",
      "lambda 06:  166.810, mean=0.2441, (25,50,75)pctl=(0.1076,0.2193,0.3711),(0.0<r>0.5): (2579,247)\n",
      "lambda 07:  464.159, mean=0.2734, (25,50,75)pctl=(0.1296,0.2564,0.4102),(0.0<r>0.5): (2592,335)\n",
      "lambda 08: 1291.550, mean=0.2773, (25,50,75)pctl=(0.1454,0.2638,0.4061),(0.0<r>0.5): (2592,293)\n",
      "lambda 09: 3593.814, mean=0.2514, (25,50,75)pctl=(0.1324,0.2406,0.3668),(0.0<r>0.5): (2592,169)\n",
      "lambda 10: 10000.000, mean=0.2284, (25,50,75)pctl=(0.1168,0.2190,0.3333),(0.0<r>0.5): (2568,060)\n",
      "pop.cv.best: 464.159, mean=0.2327, (25,50,75)pctl=(0.1417,0.2679,0.4123),(0.0<r>0.5): (2645,340)\n",
      "7/11: temporal 1/1=1.000, features 7/11=(0.5906, 0.8070)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2131, (25,50,75)pctl=(0.0864,0.1849,0.3260),(0.0<r>0.5): (2533,156)\n",
      "lambda 02:    2.783, mean=0.2131, (25,50,75)pctl=(0.0864,0.1849,0.3261),(0.0<r>0.5): (2533,156)\n",
      "lambda 03:    7.743, mean=0.2133, (25,50,75)pctl=(0.0865,0.1851,0.3263),(0.0<r>0.5): (2533,156)\n",
      "lambda 04:   21.544, mean=0.2143, (25,50,75)pctl=(0.0868,0.1860,0.3275),(0.0<r>0.5): (2534,157)\n",
      "lambda 05:   59.948, mean=0.2209, (25,50,75)pctl=(0.0904,0.1927,0.3356),(0.0<r>0.5): (2545,172)\n",
      "lambda 06:  166.810, mean=0.2472, (25,50,75)pctl=(0.1064,0.2219,0.3735),(0.0<r>0.5): (2563,265)\n",
      "lambda 07:  464.159, mean=0.2852, (25,50,75)pctl=(0.1342,0.2709,0.4279),(0.0<r>0.5): (2581,411)\n",
      "lambda 08: 1291.550, mean=0.2908, (25,50,75)pctl=(0.1439,0.2832,0.4296),(0.0<r>0.5): (2576,401)\n",
      "lambda 09: 3593.814, mean=0.2616, (25,50,75)pctl=(0.1286,0.2503,0.3861),(0.0<r>0.5): (2562,254)\n",
      "lambda 10: 10000.000, mean=0.2325, (25,50,75)pctl=(0.1103,0.2149,0.3441),(0.0<r>0.5): (2533,168)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2110, (25,50,75)pctl=(0.0856,0.1834,0.3185),(0.0<r>0.5): (2551,147)\n",
      "lambda 02:    2.783, mean=0.2110, (25,50,75)pctl=(0.0857,0.1834,0.3186),(0.0<r>0.5): (2551,147)\n",
      "lambda 03:    7.743, mean=0.2111, (25,50,75)pctl=(0.0859,0.1835,0.3186),(0.0<r>0.5): (2551,148)\n",
      "lambda 04:   21.544, mean=0.2121, (25,50,75)pctl=(0.0868,0.1847,0.3196),(0.0<r>0.5): (2551,152)\n",
      "lambda 05:   59.948, mean=0.2176, (25,50,75)pctl=(0.0891,0.1897,0.3282),(0.0<r>0.5): (2560,167)\n",
      "lambda 06:  166.810, mean=0.2373, (25,50,75)pctl=(0.0997,0.2151,0.3572),(0.0<r>0.5): (2569,224)\n",
      "lambda 07:  464.159, mean=0.2639, (25,50,75)pctl=(0.1175,0.2462,0.3944),(0.0<r>0.5): (2579,313)\n",
      "lambda 08: 1291.550, mean=0.2617, (25,50,75)pctl=(0.1249,0.2422,0.3852),(0.0<r>0.5): (2586,282)\n",
      "lambda 09: 3593.814, mean=0.2275, (25,50,75)pctl=(0.1088,0.2020,0.3385),(0.0<r>0.5): (2567,152)\n",
      "lambda 10: 10000.000, mean=0.2023, (25,50,75)pctl=(0.0916,0.1714,0.3053),(0.0<r>0.5): (2539,084)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    1.000, mean=0.2111, (25,50,75)pctl=(0.0857,0.1826,0.3173),(0.0<r>0.5): (2569,128)\n",
      "lambda 02:    2.783, mean=0.2111, (25,50,75)pctl=(0.0857,0.1827,0.3174),(0.0<r>0.5): (2569,128)\n",
      "lambda 03:    7.743, mean=0.2112, (25,50,75)pctl=(0.0858,0.1829,0.3176),(0.0<r>0.5): (2569,130)\n",
      "lambda 04:   21.544, mean=0.2124, (25,50,75)pctl=(0.0863,0.1842,0.3194),(0.0<r>0.5): (2569,134)\n",
      "lambda 05:   59.948, mean=0.2193, (25,50,75)pctl=(0.0904,0.1912,0.3298),(0.0<r>0.5): (2574,153)\n",
      "lambda 06:  166.810, mean=0.2424, (25,50,75)pctl=(0.1055,0.2191,0.3649),(0.0<r>0.5): (2592,213)\n",
      "lambda 07:  464.159, mean=0.2715, (25,50,75)pctl=(0.1260,0.2565,0.4072),(0.0<r>0.5): (2612,309)\n",
      "lambda 08: 1291.550, mean=0.2732, (25,50,75)pctl=(0.1357,0.2636,0.3982),(0.0<r>0.5): (2608,283)\n",
      "lambda 09: 3593.814, mean=0.2461, (25,50,75)pctl=(0.1232,0.2330,0.3571),(0.0<r>0.5): (2589,185)\n",
      "lambda 10: 10000.000, mean=0.2160, (25,50,75)pctl=(0.1007,0.2004,0.3162),(0.0<r>0.5): (2556,096)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2029, (25,50,75)pctl=(0.0835,0.1724,0.3085),(0.0<r>0.5): (2535,111)\n",
      "lambda 02:    2.783, mean=0.2029, (25,50,75)pctl=(0.0836,0.1724,0.3085),(0.0<r>0.5): (2535,111)\n",
      "lambda 03:    7.743, mean=0.2030, (25,50,75)pctl=(0.0837,0.1726,0.3087),(0.0<r>0.5): (2535,111)\n",
      "lambda 04:   21.544, mean=0.2042, (25,50,75)pctl=(0.0841,0.1738,0.3105),(0.0<r>0.5): (2536,112)\n",
      "lambda 05:   59.948, mean=0.2114, (25,50,75)pctl=(0.0881,0.1814,0.3218),(0.0<r>0.5): (2540,130)\n",
      "lambda 06:  166.810, mean=0.2380, (25,50,75)pctl=(0.1037,0.2108,0.3621),(0.0<r>0.5): (2560,205)\n",
      "lambda 07:  464.159, mean=0.2756, (25,50,75)pctl=(0.1298,0.2599,0.4143),(0.0<r>0.5): (2585,339)\n",
      "lambda 08: 1291.550, mean=0.2825, (25,50,75)pctl=(0.1447,0.2746,0.4144),(0.0<r>0.5): (2594,320)\n",
      "lambda 09: 3593.814, mean=0.2475, (25,50,75)pctl=(0.1256,0.2388,0.3612),(0.0<r>0.5): (2574,153)\n",
      "lambda 10: 10000.000, mean=0.2068, (25,50,75)pctl=(0.0967,0.1917,0.3019),(0.0<r>0.5): (2552,041)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2107, (25,50,75)pctl=(0.0905,0.1835,0.3143),(0.0<r>0.5): (2571,127)\n",
      "lambda 02:    2.783, mean=0.2107, (25,50,75)pctl=(0.0905,0.1835,0.3143),(0.0<r>0.5): (2571,127)\n",
      "lambda 03:    7.743, mean=0.2108, (25,50,75)pctl=(0.0905,0.1836,0.3146),(0.0<r>0.5): (2571,128)\n",
      "lambda 04:   21.544, mean=0.2118, (25,50,75)pctl=(0.0908,0.1846,0.3158),(0.0<r>0.5): (2570,133)\n",
      "lambda 05:   59.948, mean=0.2173, (25,50,75)pctl=(0.0935,0.1897,0.3247),(0.0<r>0.5): (2574,152)\n",
      "lambda 06:  166.810, mean=0.2375, (25,50,75)pctl=(0.1027,0.2107,0.3585),(0.0<r>0.5): (2578,223)\n",
      "lambda 07:  464.159, mean=0.2684, (25,50,75)pctl=(0.1250,0.2491,0.4045),(0.0<r>0.5): (2586,324)\n",
      "lambda 08: 1291.550, mean=0.2789, (25,50,75)pctl=(0.1447,0.2647,0.4091),(0.0<r>0.5): (2591,312)\n",
      "lambda 09: 3593.814, mean=0.2566, (25,50,75)pctl=(0.1349,0.2458,0.3741),(0.0<r>0.5): (2596,196)\n",
      "lambda 10: 10000.000, mean=0.2310, (25,50,75)pctl=(0.1184,0.2219,0.3369),(0.0<r>0.5): (2573,065)\n",
      "pop.cv.best: 1291.550, mean=0.2314, (25,50,75)pctl=(0.1411,0.2663,0.4101),(0.0<r>0.5): (2645,334)\n",
      "8/11: temporal 1/1=1.000, features 8/11=(0.4602, 0.8878)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2115, (25,50,75)pctl=(0.0856,0.1835,0.3231),(0.0<r>0.5): (2530,153)\n",
      "lambda 02:    2.783, mean=0.2115, (25,50,75)pctl=(0.0856,0.1835,0.3231),(0.0<r>0.5): (2530,153)\n",
      "lambda 03:    7.743, mean=0.2116, (25,50,75)pctl=(0.0857,0.1836,0.3232),(0.0<r>0.5): (2530,153)\n",
      "lambda 04:   21.544, mean=0.2122, (25,50,75)pctl=(0.0858,0.1842,0.3239),(0.0<r>0.5): (2531,155)\n",
      "lambda 05:   59.948, mean=0.2165, (25,50,75)pctl=(0.0881,0.1885,0.3306),(0.0<r>0.5): (2540,168)\n",
      "lambda 06:  166.810, mean=0.2371, (25,50,75)pctl=(0.1005,0.2091,0.3582),(0.0<r>0.5): (2557,230)\n",
      "lambda 07:  464.159, mean=0.2767, (25,50,75)pctl=(0.1271,0.2577,0.4174),(0.0<r>0.5): (2578,370)\n",
      "lambda 08: 1291.550, mean=0.2932, (25,50,75)pctl=(0.1444,0.2834,0.4344),(0.0<r>0.5): (2581,423)\n",
      "lambda 09: 3593.814, mean=0.2701, (25,50,75)pctl=(0.1342,0.2618,0.3975),(0.0<r>0.5): (2568,281)\n",
      "lambda 10: 10000.000, mean=0.2377, (25,50,75)pctl=(0.1125,0.2206,0.3529),(0.0<r>0.5): (2539,175)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2097, (25,50,75)pctl=(0.0853,0.1826,0.3148),(0.0<r>0.5): (2551,143)\n",
      "lambda 02:    2.783, mean=0.2097, (25,50,75)pctl=(0.0853,0.1826,0.3148),(0.0<r>0.5): (2551,143)\n",
      "lambda 03:    7.743, mean=0.2098, (25,50,75)pctl=(0.0853,0.1826,0.3149),(0.0<r>0.5): (2551,145)\n",
      "lambda 04:   21.544, mean=0.2104, (25,50,75)pctl=(0.0857,0.1831,0.3156),(0.0<r>0.5): (2551,146)\n",
      "lambda 05:   59.948, mean=0.2141, (25,50,75)pctl=(0.0874,0.1868,0.3217),(0.0<r>0.5): (2555,155)\n",
      "lambda 06:  166.810, mean=0.2298, (25,50,75)pctl=(0.0952,0.2045,0.3457),(0.0<r>0.5): (2563,204)\n",
      "lambda 07:  464.159, mean=0.2582, (25,50,75)pctl=(0.1134,0.2386,0.3863),(0.0<r>0.5): (2572,298)\n",
      "lambda 08: 1291.550, mean=0.2658, (25,50,75)pctl=(0.1261,0.2464,0.3922),(0.0<r>0.5): (2582,300)\n",
      "lambda 09: 3593.814, mean=0.2365, (25,50,75)pctl=(0.1123,0.2131,0.3503),(0.0<r>0.5): (2576,185)\n",
      "lambda 10: 10000.000, mean=0.2063, (25,50,75)pctl=(0.0954,0.1766,0.3100),(0.0<r>0.5): (2541,093)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2095, (25,50,75)pctl=(0.0844,0.1809,0.3140),(0.0<r>0.5): (2568,127)\n",
      "lambda 02:    2.783, mean=0.2095, (25,50,75)pctl=(0.0844,0.1809,0.3140),(0.0<r>0.5): (2568,127)\n",
      "lambda 03:    7.743, mean=0.2096, (25,50,75)pctl=(0.0844,0.1810,0.3142),(0.0<r>0.5): (2568,127)\n",
      "lambda 04:   21.544, mean=0.2104, (25,50,75)pctl=(0.0849,0.1818,0.3155),(0.0<r>0.5): (2569,129)\n",
      "lambda 05:   59.948, mean=0.2150, (25,50,75)pctl=(0.0879,0.1864,0.3230),(0.0<r>0.5): (2573,144)\n",
      "lambda 06:  166.810, mean=0.2337, (25,50,75)pctl=(0.1006,0.2090,0.3518),(0.0<r>0.5): (2587,196)\n",
      "lambda 07:  464.159, mean=0.2652, (25,50,75)pctl=(0.1214,0.2470,0.3984),(0.0<r>0.5): (2608,286)\n",
      "lambda 08: 1291.550, mean=0.2757, (25,50,75)pctl=(0.1360,0.2629,0.4029),(0.0<r>0.5): (2611,304)\n",
      "lambda 09: 3593.814, mean=0.2543, (25,50,75)pctl=(0.1280,0.2426,0.3679),(0.0<r>0.5): (2598,217)\n",
      "lambda 10: 10000.000, mean=0.2216, (25,50,75)pctl=(0.1039,0.2061,0.3233),(0.0<r>0.5): (2557,114)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2015, (25,50,75)pctl=(0.0822,0.1709,0.3061),(0.0<r>0.5): (2535,108)\n",
      "lambda 02:    2.783, mean=0.2015, (25,50,75)pctl=(0.0822,0.1709,0.3061),(0.0<r>0.5): (2535,108)\n",
      "lambda 03:    7.743, mean=0.2016, (25,50,75)pctl=(0.0823,0.1710,0.3063),(0.0<r>0.5): (2535,109)\n",
      "lambda 04:   21.544, mean=0.2024, (25,50,75)pctl=(0.0828,0.1715,0.3071),(0.0<r>0.5): (2534,111)\n",
      "lambda 05:   59.948, mean=0.2071, (25,50,75)pctl=(0.0858,0.1765,0.3156),(0.0<r>0.5): (2536,117)\n",
      "lambda 06:  166.810, mean=0.2280, (25,50,75)pctl=(0.0969,0.1987,0.3473),(0.0<r>0.5): (2557,170)\n",
      "lambda 07:  464.159, mean=0.2671, (25,50,75)pctl=(0.1226,0.2487,0.4032),(0.0<r>0.5): (2578,311)\n",
      "lambda 08: 1291.550, mean=0.2846, (25,50,75)pctl=(0.1433,0.2764,0.4193),(0.0<r>0.5): (2595,341)\n",
      "lambda 09: 3593.814, mean=0.2586, (25,50,75)pctl=(0.1317,0.2487,0.3788),(0.0<r>0.5): (2578,201)\n",
      "lambda 10: 10000.000, mean=0.2142, (25,50,75)pctl=(0.1020,0.2018,0.3108),(0.0<r>0.5): (2561,053)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2096, (25,50,75)pctl=(0.0903,0.1820,0.3122),(0.0<r>0.5): (2568,125)\n",
      "lambda 02:    2.783, mean=0.2096, (25,50,75)pctl=(0.0903,0.1821,0.3122),(0.0<r>0.5): (2568,125)\n",
      "lambda 03:    7.743, mean=0.2097, (25,50,75)pctl=(0.0903,0.1822,0.3124),(0.0<r>0.5): (2568,127)\n",
      "lambda 04:   21.544, mean=0.2103, (25,50,75)pctl=(0.0905,0.1829,0.3134),(0.0<r>0.5): (2568,130)\n",
      "lambda 05:   59.948, mean=0.2140, (25,50,75)pctl=(0.0920,0.1868,0.3196),(0.0<r>0.5): (2573,144)\n",
      "lambda 06:  166.810, mean=0.2298, (25,50,75)pctl=(0.0993,0.2014,0.3458),(0.0<r>0.5): (2579,196)\n",
      "lambda 07:  464.159, mean=0.2609, (25,50,75)pctl=(0.1190,0.2383,0.3956),(0.0<r>0.5): (2585,302)\n",
      "lambda 08: 1291.550, mean=0.2794, (25,50,75)pctl=(0.1418,0.2656,0.4133),(0.0<r>0.5): (2591,338)\n",
      "lambda 09: 3593.814, mean=0.2637, (25,50,75)pctl=(0.1388,0.2523,0.3856),(0.0<r>0.5): (2596,225)\n",
      "lambda 10: 10000.000, mean=0.2357, (25,50,75)pctl=(0.1217,0.2254,0.3445),(0.0<r>0.5): (2583,096)\n",
      "pop.cv.best: 1291.550, mean=0.2304, (25,50,75)pctl=(0.1407,0.2658,0.4092),(0.0<r>0.5): (2645,336)\n",
      "9/11: temporal 1/1=1.000, features 9/11=(0.3190, 0.9478)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2104, (25,50,75)pctl=(0.0847,0.1822,0.3210),(0.0<r>0.5): (2531,152)\n",
      "lambda 02:    2.783, mean=0.2104, (25,50,75)pctl=(0.0847,0.1822,0.3210),(0.0<r>0.5): (2531,152)\n",
      "lambda 03:    7.743, mean=0.2104, (25,50,75)pctl=(0.0847,0.1822,0.3211),(0.0<r>0.5): (2532,153)\n",
      "lambda 04:   21.544, mean=0.2107, (25,50,75)pctl=(0.0849,0.1826,0.3215),(0.0<r>0.5): (2532,153)\n",
      "lambda 05:   59.948, mean=0.2129, (25,50,75)pctl=(0.0860,0.1843,0.3240),(0.0<r>0.5): (2534,156)\n",
      "lambda 06:  166.810, mean=0.2258, (25,50,75)pctl=(0.0930,0.1968,0.3431),(0.0<r>0.5): (2551,188)\n",
      "lambda 07:  464.159, mean=0.2618, (25,50,75)pctl=(0.1143,0.2373,0.3941),(0.0<r>0.5): (2574,322)\n",
      "lambda 08: 1291.550, mean=0.2921, (25,50,75)pctl=(0.1420,0.2790,0.4352),(0.0<r>0.5): (2586,433)\n",
      "lambda 09: 3593.814, mean=0.2815, (25,50,75)pctl=(0.1393,0.2716,0.4138),(0.0<r>0.5): (2573,331)\n",
      "lambda 10: 10000.000, mean=0.2478, (25,50,75)pctl=(0.1185,0.2327,0.3654),(0.0<r>0.5): (2549,210)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2089, (25,50,75)pctl=(0.0851,0.1812,0.3125),(0.0<r>0.5): (2550,140)\n",
      "lambda 02:    2.783, mean=0.2089, (25,50,75)pctl=(0.0851,0.1812,0.3125),(0.0<r>0.5): (2550,140)\n",
      "lambda 03:    7.743, mean=0.2089, (25,50,75)pctl=(0.0852,0.1812,0.3126),(0.0<r>0.5): (2550,140)\n",
      "lambda 04:   21.544, mean=0.2092, (25,50,75)pctl=(0.0854,0.1814,0.3128),(0.0<r>0.5): (2550,145)\n",
      "lambda 05:   59.948, mean=0.2112, (25,50,75)pctl=(0.0864,0.1832,0.3160),(0.0<r>0.5): (2553,150)\n",
      "lambda 06:  166.810, mean=0.2214, (25,50,75)pctl=(0.0907,0.1944,0.3317),(0.0<r>0.5): (2562,179)\n",
      "lambda 07:  464.159, mean=0.2478, (25,50,75)pctl=(0.1052,0.2250,0.3723),(0.0<r>0.5): (2572,260)\n",
      "lambda 08: 1291.550, mean=0.2673, (25,50,75)pctl=(0.1229,0.2507,0.3977),(0.0<r>0.5): (2582,314)\n",
      "lambda 09: 3593.814, mean=0.2497, (25,50,75)pctl=(0.1199,0.2299,0.3686),(0.0<r>0.5): (2583,239)\n",
      "lambda 10: 10000.000, mean=0.2146, (25,50,75)pctl=(0.1013,0.1870,0.3197),(0.0<r>0.5): (2550,117)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2085, (25,50,75)pctl=(0.0842,0.1803,0.3124),(0.0<r>0.5): (2568,127)\n",
      "lambda 02:    2.783, mean=0.2085, (25,50,75)pctl=(0.0842,0.1803,0.3124),(0.0<r>0.5): (2568,127)\n",
      "lambda 03:    7.743, mean=0.2085, (25,50,75)pctl=(0.0842,0.1804,0.3125),(0.0<r>0.5): (2568,127)\n",
      "lambda 04:   21.544, mean=0.2089, (25,50,75)pctl=(0.0842,0.1809,0.3130),(0.0<r>0.5): (2568,127)\n",
      "lambda 05:   59.948, mean=0.2114, (25,50,75)pctl=(0.0861,0.1836,0.3170),(0.0<r>0.5): (2570,135)\n",
      "lambda 06:  166.810, mean=0.2237, (25,50,75)pctl=(0.0937,0.1963,0.3370),(0.0<r>0.5): (2578,165)\n",
      "lambda 07:  464.159, mean=0.2538, (25,50,75)pctl=(0.1124,0.2327,0.3820),(0.0<r>0.5): (2596,251)\n",
      "lambda 08: 1291.550, mean=0.2757, (25,50,75)pctl=(0.1333,0.2615,0.4089),(0.0<r>0.5): (2610,313)\n",
      "lambda 09: 3593.814, mean=0.2647, (25,50,75)pctl=(0.1340,0.2530,0.3817),(0.0<r>0.5): (2606,251)\n",
      "lambda 10: 10000.000, mean=0.2323, (25,50,75)pctl=(0.1121,0.2169,0.3380),(0.0<r>0.5): (2574,141)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2007, (25,50,75)pctl=(0.0819,0.1699,0.3045),(0.0<r>0.5): (2533,107)\n",
      "lambda 02:    2.783, mean=0.2007, (25,50,75)pctl=(0.0819,0.1699,0.3045),(0.0<r>0.5): (2533,107)\n",
      "lambda 03:    7.743, mean=0.2007, (25,50,75)pctl=(0.0819,0.1700,0.3046),(0.0<r>0.5): (2533,108)\n",
      "lambda 04:   21.544, mean=0.2011, (25,50,75)pctl=(0.0822,0.1702,0.3053),(0.0<r>0.5): (2533,109)\n",
      "lambda 05:   59.948, mean=0.2036, (25,50,75)pctl=(0.0836,0.1724,0.3104),(0.0<r>0.5): (2535,113)\n",
      "lambda 06:  166.810, mean=0.2168, (25,50,75)pctl=(0.0910,0.1861,0.3295),(0.0<r>0.5): (2541,153)\n",
      "lambda 07:  464.159, mean=0.2525, (25,50,75)pctl=(0.1137,0.2292,0.3848),(0.0<r>0.5): (2569,264)\n",
      "lambda 08: 1291.550, mean=0.2828, (25,50,75)pctl=(0.1388,0.2712,0.4211),(0.0<r>0.5): (2592,349)\n",
      "lambda 09: 3593.814, mean=0.2724, (25,50,75)pctl=(0.1402,0.2634,0.3990),(0.0<r>0.5): (2589,255)\n",
      "lambda 10: 10000.000, mean=0.2286, (25,50,75)pctl=(0.1120,0.2190,0.3328),(0.0<r>0.5): (2565,091)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2090, (25,50,75)pctl=(0.0900,0.1804,0.3109),(0.0<r>0.5): (2567,126)\n",
      "lambda 02:    2.783, mean=0.2090, (25,50,75)pctl=(0.0900,0.1804,0.3109),(0.0<r>0.5): (2567,126)\n",
      "lambda 03:    7.743, mean=0.2090, (25,50,75)pctl=(0.0900,0.1804,0.3109),(0.0<r>0.5): (2567,126)\n",
      "lambda 04:   21.544, mean=0.2093, (25,50,75)pctl=(0.0901,0.1808,0.3113),(0.0<r>0.5): (2567,128)\n",
      "lambda 05:   59.948, mean=0.2113, (25,50,75)pctl=(0.0909,0.1836,0.3144),(0.0<r>0.5): (2570,134)\n",
      "lambda 06:  166.810, mean=0.2214, (25,50,75)pctl=(0.0959,0.1938,0.3308),(0.0<r>0.5): (2571,169)\n",
      "lambda 07:  464.159, mean=0.2488, (25,50,75)pctl=(0.1101,0.2229,0.3768),(0.0<r>0.5): (2578,265)\n",
      "lambda 08: 1291.550, mean=0.2761, (25,50,75)pctl=(0.1346,0.2604,0.4123),(0.0<r>0.5): (2592,344)\n",
      "lambda 09: 3593.814, mean=0.2727, (25,50,75)pctl=(0.1426,0.2599,0.4004),(0.0<r>0.5): (2589,271)\n",
      "lambda 10: 10000.000, mean=0.2447, (25,50,75)pctl=(0.1283,0.2339,0.3563),(0.0<r>0.5): (2592,137)\n",
      "pop.cv.best: 1291.550, mean=0.2296, (25,50,75)pctl=(0.1402,0.2649,0.4099),(0.0<r>0.5): (2645,339)\n",
      "10/11: temporal 1/1=1.000, features 10/11=(0.1702, 0.9854)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2095, (25,50,75)pctl=(0.0846,0.1809,0.3194),(0.0<r>0.5): (2532,148)\n",
      "lambda 02:    2.783, mean=0.2095, (25,50,75)pctl=(0.0846,0.1809,0.3194),(0.0<r>0.5): (2532,148)\n",
      "lambda 03:    7.743, mean=0.2096, (25,50,75)pctl=(0.0846,0.1809,0.3195),(0.0<r>0.5): (2532,148)\n",
      "lambda 04:   21.544, mean=0.2096, (25,50,75)pctl=(0.0847,0.1810,0.3196),(0.0<r>0.5): (2532,149)\n",
      "lambda 05:   59.948, mean=0.2103, (25,50,75)pctl=(0.0848,0.1816,0.3205),(0.0<r>0.5): (2532,153)\n",
      "lambda 06:  166.810, mean=0.2149, (25,50,75)pctl=(0.0870,0.1859,0.3274),(0.0<r>0.5): (2537,164)\n",
      "lambda 07:  464.159, mean=0.2366, (25,50,75)pctl=(0.0998,0.2090,0.3563),(0.0<r>0.5): (2555,231)\n",
      "lambda 08: 1291.550, mean=0.2769, (25,50,75)pctl=(0.1271,0.2579,0.4171),(0.0<r>0.5): (2576,372)\n",
      "lambda 09: 3593.814, mean=0.2928, (25,50,75)pctl=(0.1440,0.2827,0.4338),(0.0<r>0.5): (2578,417)\n",
      "lambda 10: 10000.000, mean=0.2690, (25,50,75)pctl=(0.1329,0.2607,0.3961),(0.0<r>0.5): (2567,277)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2085, (25,50,75)pctl=(0.0847,0.1812,0.3114),(0.0<r>0.5): (2550,139)\n",
      "lambda 02:    2.783, mean=0.2085, (25,50,75)pctl=(0.0847,0.1812,0.3114),(0.0<r>0.5): (2550,139)\n",
      "lambda 03:    7.743, mean=0.2086, (25,50,75)pctl=(0.0847,0.1812,0.3114),(0.0<r>0.5): (2550,139)\n",
      "lambda 04:   21.544, mean=0.2086, (25,50,75)pctl=(0.0848,0.1812,0.3115),(0.0<r>0.5): (2550,139)\n",
      "lambda 05:   59.948, mean=0.2093, (25,50,75)pctl=(0.0853,0.1818,0.3123),(0.0<r>0.5): (2551,144)\n",
      "lambda 06:  166.810, mean=0.2132, (25,50,75)pctl=(0.0874,0.1861,0.3179),(0.0<r>0.5): (2554,154)\n",
      "lambda 07:  464.159, mean=0.2295, (25,50,75)pctl=(0.0955,0.2040,0.3438),(0.0<r>0.5): (2564,201)\n",
      "lambda 08: 1291.550, mean=0.2584, (25,50,75)pctl=(0.1136,0.2386,0.3865),(0.0<r>0.5): (2573,298)\n",
      "lambda 09: 3593.814, mean=0.2652, (25,50,75)pctl=(0.1251,0.2460,0.3913),(0.0<r>0.5): (2582,298)\n",
      "lambda 10: 10000.000, mean=0.2353, (25,50,75)pctl=(0.1119,0.2117,0.3482),(0.0<r>0.5): (2574,180)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2078, (25,50,75)pctl=(0.0843,0.1793,0.3121),(0.0<r>0.5): (2568,126)\n",
      "lambda 02:    2.783, mean=0.2078, (25,50,75)pctl=(0.0843,0.1793,0.3121),(0.0<r>0.5): (2568,126)\n",
      "lambda 03:    7.743, mean=0.2079, (25,50,75)pctl=(0.0843,0.1793,0.3121),(0.0<r>0.5): (2568,126)\n",
      "lambda 04:   21.544, mean=0.2080, (25,50,75)pctl=(0.0843,0.1793,0.3122),(0.0<r>0.5): (2568,126)\n",
      "lambda 05:   59.948, mean=0.2087, (25,50,75)pctl=(0.0845,0.1800,0.3132),(0.0<r>0.5): (2568,127)\n",
      "lambda 06:  166.810, mean=0.2137, (25,50,75)pctl=(0.0875,0.1856,0.3217),(0.0<r>0.5): (2572,142)\n",
      "lambda 07:  464.159, mean=0.2332, (25,50,75)pctl=(0.0997,0.2079,0.3517),(0.0<r>0.5): (2587,192)\n",
      "lambda 08: 1291.550, mean=0.2653, (25,50,75)pctl=(0.1210,0.2478,0.3985),(0.0<r>0.5): (2607,288)\n",
      "lambda 09: 3593.814, mean=0.2752, (25,50,75)pctl=(0.1362,0.2624,0.4020),(0.0<r>0.5): (2611,299)\n",
      "lambda 10: 10000.000, mean=0.2532, (25,50,75)pctl=(0.1275,0.2413,0.3668),(0.0<r>0.5): (2598,214)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    1.000, mean=0.2002, (25,50,75)pctl=(0.0817,0.1693,0.3042),(0.0<r>0.5): (2533,105)\n",
      "lambda 02:    2.783, mean=0.2002, (25,50,75)pctl=(0.0817,0.1693,0.3042),(0.0<r>0.5): (2533,105)\n",
      "lambda 03:    7.743, mean=0.2002, (25,50,75)pctl=(0.0817,0.1693,0.3042),(0.0<r>0.5): (2533,105)\n",
      "lambda 04:   21.544, mean=0.2003, (25,50,75)pctl=(0.0817,0.1694,0.3045),(0.0<r>0.5): (2533,105)\n",
      "lambda 05:   59.948, mean=0.2011, (25,50,75)pctl=(0.0820,0.1703,0.3066),(0.0<r>0.5): (2534,109)\n",
      "lambda 06:  166.810, mean=0.2061, (25,50,75)pctl=(0.0845,0.1743,0.3142),(0.0<r>0.5): (2538,119)\n",
      "lambda 07:  464.159, mean=0.2277, (25,50,75)pctl=(0.0966,0.1975,0.3474),(0.0<r>0.5): (2557,172)\n",
      "lambda 08: 1291.550, mean=0.2674, (25,50,75)pctl=(0.1227,0.2480,0.4027),(0.0<r>0.5): (2577,309)\n",
      "lambda 09: 3593.814, mean=0.2842, (25,50,75)pctl=(0.1429,0.2753,0.4184),(0.0<r>0.5): (2595,338)\n",
      "lambda 10: 10000.000, mean=0.2572, (25,50,75)pctl=(0.1310,0.2474,0.3764),(0.0<r>0.5): (2578,192)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2088, (25,50,75)pctl=(0.0898,0.1802,0.3106),(0.0<r>0.5): (2568,126)\n",
      "lambda 02:    2.783, mean=0.2088, (25,50,75)pctl=(0.0898,0.1802,0.3106),(0.0<r>0.5): (2568,126)\n",
      "lambda 03:    7.743, mean=0.2088, (25,50,75)pctl=(0.0898,0.1803,0.3106),(0.0<r>0.5): (2568,126)\n",
      "lambda 04:   21.544, mean=0.2089, (25,50,75)pctl=(0.0899,0.1803,0.3108),(0.0<r>0.5): (2568,126)\n",
      "lambda 05:   59.948, mean=0.2095, (25,50,75)pctl=(0.0902,0.1812,0.3121),(0.0<r>0.5): (2569,130)\n",
      "lambda 06:  166.810, mean=0.2134, (25,50,75)pctl=(0.0922,0.1863,0.3178),(0.0<r>0.5): (2571,147)\n",
      "lambda 07:  464.159, mean=0.2297, (25,50,75)pctl=(0.0989,0.2006,0.3454),(0.0<r>0.5): (2576,198)\n",
      "lambda 08: 1291.550, mean=0.2611, (25,50,75)pctl=(0.1195,0.2394,0.3966),(0.0<r>0.5): (2582,303)\n",
      "lambda 09: 3593.814, mean=0.2792, (25,50,75)pctl=(0.1422,0.2654,0.4124),(0.0<r>0.5): (2591,336)\n",
      "lambda 10: 10000.000, mean=0.2627, (25,50,75)pctl=(0.1389,0.2515,0.3841),(0.0<r>0.5): (2597,223)\n",
      "pop.cv.best: 3593.814, mean=0.2280, (25,50,75)pctl=(0.1409,0.2649,0.4079),(0.0<r>0.5): (2643,333)\n",
      "11/11: temporal 1/1=1.000, features 11/11=(0.0175, 0.9998)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2086, (25,50,75)pctl=(0.0835,0.1794,0.3175),(0.0<r>0.5): (2531,138)\n",
      "lambda 02:    2.783, mean=0.2086, (25,50,75)pctl=(0.0835,0.1794,0.3175),(0.0<r>0.5): (2531,138)\n",
      "lambda 03:    7.743, mean=0.2086, (25,50,75)pctl=(0.0835,0.1794,0.3175),(0.0<r>0.5): (2531,138)\n",
      "lambda 04:   21.544, mean=0.2086, (25,50,75)pctl=(0.0835,0.1794,0.3175),(0.0<r>0.5): (2531,138)\n",
      "lambda 05:   59.948, mean=0.2086, (25,50,75)pctl=(0.0835,0.1794,0.3175),(0.0<r>0.5): (2531,138)\n",
      "lambda 06:  166.810, mean=0.2087, (25,50,75)pctl=(0.0835,0.1794,0.3176),(0.0<r>0.5): (2531,139)\n",
      "lambda 07:  464.159, mean=0.2091, (25,50,75)pctl=(0.0838,0.1798,0.3185),(0.0<r>0.5): (2532,142)\n",
      "lambda 08: 1291.550, mean=0.2123, (25,50,75)pctl=(0.0857,0.1830,0.3235),(0.0<r>0.5): (2536,158)\n",
      "lambda 09: 3593.814, mean=0.2289, (25,50,75)pctl=(0.0957,0.2003,0.3469),(0.0<r>0.5): (2554,199)\n",
      "lambda 10: 10000.000, mean=0.2681, (25,50,75)pctl=(0.1194,0.2450,0.4032),(0.0<r>0.5): (2576,342)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2086, (25,50,75)pctl=(0.0846,0.1807,0.3111),(0.0<r>0.5): (2550,138)\n",
      "lambda 02:    2.783, mean=0.2086, (25,50,75)pctl=(0.0846,0.1807,0.3111),(0.0<r>0.5): (2550,138)\n",
      "lambda 03:    7.743, mean=0.2086, (25,50,75)pctl=(0.0846,0.1807,0.3111),(0.0<r>0.5): (2550,138)\n",
      "lambda 04:   21.544, mean=0.2086, (25,50,75)pctl=(0.0846,0.1807,0.3111),(0.0<r>0.5): (2550,138)\n",
      "lambda 05:   59.948, mean=0.2086, (25,50,75)pctl=(0.0846,0.1807,0.3111),(0.0<r>0.5): (2550,138)\n",
      "lambda 06:  166.810, mean=0.2086, (25,50,75)pctl=(0.0846,0.1808,0.3111),(0.0<r>0.5): (2550,138)\n",
      "lambda 07:  464.159, mean=0.2090, (25,50,75)pctl=(0.0851,0.1816,0.3116),(0.0<r>0.5): (2550,140)\n",
      "lambda 08: 1291.550, mean=0.2117, (25,50,75)pctl=(0.0865,0.1850,0.3154),(0.0<r>0.5): (2552,150)\n",
      "lambda 09: 3593.814, mean=0.2241, (25,50,75)pctl=(0.0922,0.1974,0.3349),(0.0<r>0.5): (2561,191)\n",
      "lambda 10: 10000.000, mean=0.2522, (25,50,75)pctl=(0.1078,0.2311,0.3778),(0.0<r>0.5): (2575,275)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2074, (25,50,75)pctl=(0.0840,0.1783,0.3110),(0.0<r>0.5): (2567,125)\n",
      "lambda 02:    2.783, mean=0.2074, (25,50,75)pctl=(0.0840,0.1783,0.3110),(0.0<r>0.5): (2567,125)\n",
      "lambda 03:    7.743, mean=0.2074, (25,50,75)pctl=(0.0840,0.1783,0.3110),(0.0<r>0.5): (2567,125)\n",
      "lambda 04:   21.544, mean=0.2074, (25,50,75)pctl=(0.0840,0.1783,0.3110),(0.0<r>0.5): (2567,125)\n",
      "lambda 05:   59.948, mean=0.2074, (25,50,75)pctl=(0.0840,0.1783,0.3110),(0.0<r>0.5): (2567,125)\n",
      "lambda 06:  166.810, mean=0.2075, (25,50,75)pctl=(0.0841,0.1784,0.3112),(0.0<r>0.5): (2567,125)\n",
      "lambda 07:  464.159, mean=0.2080, (25,50,75)pctl=(0.0843,0.1788,0.3121),(0.0<r>0.5): (2568,127)\n",
      "lambda 08: 1291.550, mean=0.2114, (25,50,75)pctl=(0.0863,0.1829,0.3177),(0.0<r>0.5): (2571,136)\n",
      "lambda 09: 3593.814, mean=0.2267, (25,50,75)pctl=(0.0957,0.2000,0.3416),(0.0<r>0.5): (2582,175)\n",
      "lambda 10: 10000.000, mean=0.2585, (25,50,75)pctl=(0.1156,0.2391,0.3877),(0.0<r>0.5): (2602,269)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1999, (25,50,75)pctl=(0.0812,0.1684,0.3055),(0.0<r>0.5): (2530,103)\n",
      "lambda 02:    2.783, mean=0.1999, (25,50,75)pctl=(0.0812,0.1684,0.3055),(0.0<r>0.5): (2530,103)\n",
      "lambda 03:    7.743, mean=0.1999, (25,50,75)pctl=(0.0812,0.1684,0.3055),(0.0<r>0.5): (2530,103)\n",
      "lambda 04:   21.544, mean=0.1999, (25,50,75)pctl=(0.0812,0.1684,0.3055),(0.0<r>0.5): (2530,103)\n",
      "lambda 05:   59.948, mean=0.2000, (25,50,75)pctl=(0.0812,0.1685,0.3055),(0.0<r>0.5): (2530,103)\n",
      "lambda 06:  166.810, mean=0.2000, (25,50,75)pctl=(0.0812,0.1685,0.3057),(0.0<r>0.5): (2530,103)\n",
      "lambda 07:  464.159, mean=0.2005, (25,50,75)pctl=(0.0815,0.1691,0.3066),(0.0<r>0.5): (2532,105)\n",
      "lambda 08: 1291.550, mean=0.2039, (25,50,75)pctl=(0.0836,0.1721,0.3122),(0.0<r>0.5): (2536,114)\n",
      "lambda 09: 3593.814, mean=0.2204, (25,50,75)pctl=(0.0924,0.1892,0.3365),(0.0<r>0.5): (2549,162)\n",
      "lambda 10: 10000.000, mean=0.2587, (25,50,75)pctl=(0.1172,0.2379,0.3922),(0.0<r>0.5): (2573,289)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2090, (25,50,75)pctl=(0.0904,0.1809,0.3112),(0.0<r>0.5): (2571,125)\n",
      "lambda 02:    2.783, mean=0.2090, (25,50,75)pctl=(0.0904,0.1809,0.3112),(0.0<r>0.5): (2571,125)\n",
      "lambda 03:    7.743, mean=0.2090, (25,50,75)pctl=(0.0904,0.1809,0.3112),(0.0<r>0.5): (2571,125)\n",
      "lambda 04:   21.544, mean=0.2090, (25,50,75)pctl=(0.0904,0.1809,0.3112),(0.0<r>0.5): (2571,125)\n",
      "lambda 05:   59.948, mean=0.2090, (25,50,75)pctl=(0.0904,0.1809,0.3112),(0.0<r>0.5): (2571,125)\n",
      "lambda 06:  166.810, mean=0.2091, (25,50,75)pctl=(0.0904,0.1810,0.3113),(0.0<r>0.5): (2571,125)\n",
      "lambda 07:  464.159, mean=0.2094, (25,50,75)pctl=(0.0906,0.1817,0.3119),(0.0<r>0.5): (2570,127)\n",
      "lambda 08: 1291.550, mean=0.2120, (25,50,75)pctl=(0.0914,0.1849,0.3166),(0.0<r>0.5): (2572,138)\n",
      "lambda 09: 3593.814, mean=0.2243, (25,50,75)pctl=(0.0968,0.1959,0.3364),(0.0<r>0.5): (2572,174)\n",
      "lambda 10: 10000.000, mean=0.2538, (25,50,75)pctl=(0.1134,0.2297,0.3852),(0.0<r>0.5): (2582,278)\n",
      "pop.cv.best: 10000.000, mean=0.2141, (25,50,75)pctl=(0.1128,0.2352,0.3856),(0.0<r>0.5): (2632,281)\n",
      "Duration 8.1484[mins]\n",
      "lambda 01:   59.948, mean=0.1502, (25,50,75)pctl=(0.1502,0.1502,0.1502),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.888, 0.460) perf=0.1502\n",
      "lambda 01: 10000.000, mean=-0.0222, (25,50,75)pctl=(-0.0613,0.0050,0.0305),(0.0<r>0.5): (002,000)\n",
      "3 responses: ridge=10000.000, temporal=1.000, spatial=(0.948, 0.319) perf=-0.0222\n",
      "lambda 01:  464.159, mean=0.6190, (25,50,75)pctl=(0.5448,0.6342,0.7069),(0.0<r>0.5): (023,020)\n",
      "23 responses: ridge=  464.159, temporal=1.000, spatial=(0.888, 0.460) perf=0.6190\n",
      "lambda 01: 1291.550, mean=0.4955, (25,50,75)pctl=(0.3512,0.5767,0.6544),(0.0<r>0.5): (208,137)\n",
      "212 responses: ridge= 1291.550, temporal=1.000, spatial=(1.000, 0.017) perf=0.4955\n",
      "lambda 01:    1.000, mean=0.0178, (25,50,75)pctl=(-0.0063,0.0062,0.0191),(0.0<r>0.5): (003,000)\n",
      "6 responses: ridge=    1.000, temporal=1.000, spatial=(0.985, 0.170) perf=0.0178\n",
      "lambda 01: 3593.814, mean=-0.0767, (25,50,75)pctl=(-0.0767,-0.0767,-0.0767),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge= 3593.814, temporal=1.000, spatial=(0.017, 1.000) perf=-0.0767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:  464.159, mean=0.0458, (25,50,75)pctl=(0.0073,0.0460,0.0886),(0.0<r>0.5): (013,000)\n",
      "16 responses: ridge=  464.159, temporal=1.000, spatial=(1.000, 0.017) perf=0.0458\n",
      "lambda 01: 1291.550, mean=0.3976, (25,50,75)pctl=(0.2487,0.2487,0.4721),(0.0<r>0.5): (003,001)\n",
      "3 responses: ridge= 1291.550, temporal=1.000, spatial=(0.170, 0.985) perf=0.3976\n",
      "lambda 01:  166.810, mean=-0.0001, (25,50,75)pctl=(-0.0412,0.0105,0.0335),(0.0<r>0.5): (013,000)\n",
      "23 responses: ridge=  166.810, temporal=1.000, spatial=(1.000, 0.017) perf=-0.0001\n",
      "lambda 01:    1.000, mean=0.0565, (25,50,75)pctl=(0.0078,0.0608,0.1096),(0.0<r>0.5): (003,000)\n",
      "4 responses: ridge=    1.000, temporal=1.000, spatial=(1.000, 0.017) perf=0.0565\n",
      "lambda 01:  166.810, mean=0.0196, (25,50,75)pctl=(-0.0492,-0.0492,0.0540),(0.0<r>0.5): (001,000)\n",
      "3 responses: ridge=  166.810, temporal=1.000, spatial=(0.319, 0.948) perf=0.0196\n",
      "lambda 01:  464.159, mean=0.5158, (25,50,75)pctl=(0.4185,0.5652,0.6568),(0.0<r>0.5): (864,538)\n",
      "869 responses: ridge=  464.159, temporal=1.000, spatial=(0.985, 0.170) perf=0.5158\n",
      "lambda 01: 3593.814, mean=0.0785, (25,50,75)pctl=(0.0357,0.0785,0.1214),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge= 3593.814, temporal=1.000, spatial=(0.707, 0.707) perf=0.0785\n",
      "lambda 01: 10000.000, mean=0.2321, (25,50,75)pctl=(0.1761,0.2215,0.2775),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge=10000.000, temporal=1.000, spatial=(0.460, 0.888) perf=0.2321\n",
      "lambda 01: 3593.814, mean=0.2357, (25,50,75)pctl=(0.1185,0.3043,0.3387),(0.0<r>0.5): (007,000)\n",
      "8 responses: ridge= 3593.814, temporal=1.000, spatial=(0.591, 0.807) perf=0.2357\n",
      "lambda 01:   21.544, mean=0.0280, (25,50,75)pctl=(0.0025,0.0280,0.0535),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=   21.544, temporal=1.000, spatial=(0.985, 0.170) perf=0.0280\n",
      "lambda 01: 10000.000, mean=0.0157, (25,50,75)pctl=(0.0157,0.0157,0.0157),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=10000.000, temporal=1.000, spatial=(0.807, 0.591) perf=0.0157\n",
      "lambda 01: 3593.814, mean=0.4723, (25,50,75)pctl=(0.3971,0.5017,0.5788),(0.0<r>0.5): (076,040)\n",
      "79 responses: ridge= 3593.814, temporal=1.000, spatial=(0.170, 0.985) perf=0.4723\n",
      "lambda 01: 3593.814, mean=0.2405, (25,50,75)pctl=(0.1660,0.2949,0.3466),(0.0<r>0.5): (023,000)\n",
      "26 responses: ridge= 3593.814, temporal=1.000, spatial=(0.319, 0.948) perf=0.2405\n",
      "lambda 01:   21.544, mean=0.0553, (25,50,75)pctl=(0.0482,0.0553,0.0624),(0.0<r>0.5): (002,000)\n",
      "2 responses: ridge=   21.544, temporal=1.000, spatial=(1.000, 0.017) perf=0.0553\n",
      "lambda 01:   59.948, mean=-0.0612, (25,50,75)pctl=(-0.0612,-0.0612,-0.0612),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.807, 0.591) perf=-0.0612\n",
      "lambda 01: 10000.000, mean=0.0052, (25,50,75)pctl=(-0.0558,0.0052,0.0661),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=10000.000, temporal=1.000, spatial=(0.017, 1.000) perf=0.0052\n",
      "lambda 01:  464.159, mean=0.5324, (25,50,75)pctl=(0.5647,0.6329,0.6836),(0.0<r>0.5): (037,029)\n",
      "38 responses: ridge=  464.159, temporal=1.000, spatial=(0.591, 0.807) perf=0.5324\n",
      "lambda 01:   59.948, mean=0.0184, (25,50,75)pctl=(0.0031,0.0120,0.0305),(0.0<r>0.5): (002,000)\n",
      "3 responses: ridge=   59.948, temporal=1.000, spatial=(1.000, 0.017) perf=0.0184\n",
      "lambda 01:    1.000, mean=0.1772, (25,50,75)pctl=(0.1772,0.1772,0.1772),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(0.948, 0.319) perf=0.1772\n",
      "lambda 01: 1291.550, mean=0.4295, (25,50,75)pctl=(0.3363,0.4588,0.5473),(0.0<r>0.5): (089,030)\n",
      "89 responses: ridge= 1291.550, temporal=1.000, spatial=(0.591, 0.807) perf=0.4295\n",
      "lambda 01:   59.948, mean=0.0487, (25,50,75)pctl=(0.0369,0.0460,0.0578),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge=   59.948, temporal=1.000, spatial=(0.985, 0.170) perf=0.0487\n",
      "lambda 01: 1291.550, mean=0.3732, (25,50,75)pctl=(0.2551,0.4122,0.5016),(0.0<r>0.5): (038,011)\n",
      "39 responses: ridge= 1291.550, temporal=1.000, spatial=(0.707, 0.707) perf=0.3732\n",
      "lambda 01: 3593.814, mean=0.3607, (25,50,75)pctl=(0.3607,0.3607,0.3607),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge= 3593.814, temporal=1.000, spatial=(0.807, 0.591) perf=0.3607\n",
      "lambda 01:    1.000, mean=0.0073, (25,50,75)pctl=(-0.0338,0.0073,0.0484),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=    1.000, temporal=1.000, spatial=(0.888, 0.460) perf=0.0073\n",
      "lambda 01: 10000.000, mean=0.2905, (25,50,75)pctl=(0.1981,0.3630,0.3853),(0.0<r>0.5): (010,000)\n",
      "10 responses: ridge=10000.000, temporal=1.000, spatial=(0.170, 0.985) perf=0.2905\n",
      "lambda 01:  166.810, mean=0.3829, (25,50,75)pctl=(0.2558,0.5910,0.6141),(0.0<r>0.5): (002,002)\n",
      "3 responses: ridge=  166.810, temporal=1.000, spatial=(0.948, 0.319) perf=0.3829\n",
      "lambda 01: 10000.000, mean=0.1183, (25,50,75)pctl=(0.1085,0.1183,0.1282),(0.0<r>0.5): (002,000)\n",
      "2 responses: ridge=10000.000, temporal=1.000, spatial=(0.985, 0.170) perf=0.1183\n",
      "lambda 01:  166.810, mean=0.0826, (25,50,75)pctl=(0.0826,0.0826,0.0826),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=  166.810, temporal=1.000, spatial=(0.170, 0.985) perf=0.0826\n",
      "lambda 01:  464.159, mean=0.0849, (25,50,75)pctl=(0.0849,0.0849,0.0849),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=  464.159, temporal=1.000, spatial=(0.319, 0.948) perf=0.0849\n",
      "lambda 01: 10000.000, mean=0.1975, (25,50,75)pctl=(0.1052,0.2166,0.2856),(0.0<r>0.5): (090,001)\n",
      "99 responses: ridge=10000.000, temporal=1.000, spatial=(1.000, 0.017) perf=0.1975\n",
      "lambda 01:  464.159, mean=0.5882, (25,50,75)pctl=(0.5632,0.6062,0.6757),(0.0<r>0.5): (053,044)\n",
      "53 responses: ridge=  464.159, temporal=1.000, spatial=(0.807, 0.591) perf=0.5882\n",
      "lambda 01:   59.948, mean=-0.0767, (25,50,75)pctl=(-0.0767,-0.0767,-0.0767),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.170, 0.985) perf=-0.0767\n",
      "lambda 01: 3593.814, mean=0.1534, (25,50,75)pctl=(0.0709,0.0947,0.2183),(0.0<r>0.5): (013,000)\n",
      "13 responses: ridge= 3593.814, temporal=1.000, spatial=(0.985, 0.170) perf=0.1534\n",
      "lambda 01: 1291.550, mean=0.3171, (25,50,75)pctl=(0.2200,0.3204,0.4310),(0.0<r>0.5): (140,014)\n",
      "141 responses: ridge= 1291.550, temporal=1.000, spatial=(0.985, 0.170) perf=0.3171\n",
      "lambda 01:  464.159, mean=0.6196, (25,50,75)pctl=(0.6023,0.6722,0.7166),(0.0<r>0.5): (061,054)\n",
      "61 responses: ridge=  464.159, temporal=1.000, spatial=(0.707, 0.707) perf=0.6196\n",
      "lambda 01: 1291.550, mean=0.4162, (25,50,75)pctl=(0.3133,0.4838,0.5462),(0.0<r>0.5): (010,003)\n",
      "10 responses: ridge= 1291.550, temporal=1.000, spatial=(0.807, 0.591) perf=0.4162\n",
      "lambda 01: 3593.814, mean=0.2811, (25,50,75)pctl=(0.0925,0.3103,0.4167),(0.0<r>0.5): (009,000)\n",
      "9 responses: ridge= 3593.814, temporal=1.000, spatial=(0.460, 0.888) perf=0.2811\n",
      "lambda 01:  166.810, mean=0.1626, (25,50,75)pctl=(0.0381,0.1190,0.2044),(0.0<r>0.5): (026,003)\n",
      "31 responses: ridge=  166.810, temporal=1.000, spatial=(0.985, 0.170) perf=0.1626\n",
      "lambda 01:    1.000, mean=-0.0322, (25,50,75)pctl=(-0.0347,-0.0271,-0.0271),(0.0<r>0.5): (000,000)\n",
      "3 responses: ridge=    1.000, temporal=1.000, spatial=(0.170, 0.985) perf=-0.0322\n",
      "lambda 01: 1291.550, mean=0.4768, (25,50,75)pctl=(0.3334,0.5125,0.6143),(0.0<r>0.5): (089,046)\n",
      "89 responses: ridge= 1291.550, temporal=1.000, spatial=(0.460, 0.888) perf=0.4768\n",
      "lambda 01:   59.948, mean=-0.0098, (25,50,75)pctl=(-0.0311,-0.0098,0.0116),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=   59.948, temporal=1.000, spatial=(0.460, 0.888) perf=-0.0098\n",
      "lambda 01:  464.159, mean=0.5132, (25,50,75)pctl=(0.3913,0.5543,0.6312),(0.0<r>0.5): (116,072)\n",
      "116 responses: ridge=  464.159, temporal=1.000, spatial=(0.948, 0.319) perf=0.5132\n",
      "lambda 01:  464.159, mean=-0.0082, (25,50,75)pctl=(-0.0082,-0.0082,-0.0082),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=  464.159, temporal=1.000, spatial=(0.460, 0.888) perf=-0.0082\n",
      "lambda 01:   59.948, mean=0.0990, (25,50,75)pctl=(0.0990,0.0990,0.0990),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.948, 0.319) perf=0.0990\n",
      "lambda 01: 10000.000, mean=0.1694, (25,50,75)pctl=(0.1123,0.1511,0.2082),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge=10000.000, temporal=1.000, spatial=(0.319, 0.948) perf=0.1694\n",
      "lambda 01: 1291.550, mean=0.3296, (25,50,75)pctl=(0.2905,0.3296,0.3687),(0.0<r>0.5): (002,000)\n",
      "2 responses: ridge= 1291.550, temporal=1.000, spatial=(0.948, 0.319) perf=0.3296\n",
      "lambda 01: 1291.550, mean=0.4586, (25,50,75)pctl=(0.3073,0.5395,0.6302),(0.0<r>0.5): (059,037)\n",
      "60 responses: ridge= 1291.550, temporal=1.000, spatial=(0.319, 0.948) perf=0.4586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01: 3593.814, mean=0.4061, (25,50,75)pctl=(0.3154,0.4327,0.5346),(0.0<r>0.5): (442,156)\n",
      "451 responses: ridge= 3593.814, temporal=1.000, spatial=(1.000, 0.017) perf=0.4061\n",
      "lambda 01:  166.810, mean=-0.0619, (25,50,75)pctl=(-0.0619,-0.0619,-0.0619),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=  166.810, temporal=1.000, spatial=(0.017, 1.000) perf=-0.0619\n",
      "lambda 01:    1.000, mean=0.0434, (25,50,75)pctl=(0.0135,0.0280,0.0881),(0.0<r>0.5): (005,000)\n",
      "6 responses: ridge=    1.000, temporal=1.000, spatial=(0.017, 1.000) perf=0.0434\n",
      "lambda 01:  166.810, mean=-0.0093, (25,50,75)pctl=(-0.0093,-0.0093,-0.0093),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=  166.810, temporal=1.000, spatial=(0.591, 0.807) perf=-0.0093\n",
      "lambda 01: 10000.000, mean=-0.1693, (25,50,75)pctl=(-0.1693,-0.1693,-0.1693),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=10000.000, temporal=1.000, spatial=(0.707, 0.707) perf=-0.1693\n",
      "lambda 01: 10000.000, mean=0.2389, (25,50,75)pctl=(0.2389,0.2389,0.2389),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=10000.000, temporal=1.000, spatial=(0.888, 0.460) perf=0.2389\n",
      "lambda 01: 1291.550, mean=0.4220, (25,50,75)pctl=(0.3935,0.4501,0.4786),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge= 1291.550, temporal=1.000, spatial=(0.888, 0.460) perf=0.4220\n",
      "lambda 01:  166.810, mean=-0.0453, (25,50,75)pctl=(-0.0453,-0.0453,-0.0453),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=  166.810, temporal=1.000, spatial=(0.460, 0.888) perf=-0.0453\n",
      "lambda 01:   21.544, mean=-0.0592, (25,50,75)pctl=(-0.0592,-0.0592,-0.0592),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=   21.544, temporal=1.000, spatial=(0.948, 0.319) perf=-0.0592\n",
      "lambda 01:  464.159, mean=0.0644, (25,50,75)pctl=(0.0523,0.0644,0.0765),(0.0<r>0.5): (002,000)\n",
      "2 responses: ridge=  464.159, temporal=1.000, spatial=(0.170, 0.985) perf=0.0644\n",
      "Total duration 45.4690[mins]\n"
     ]
    }
   ],
   "source": [
    "fit_banded_polar = models.estimate_stem_wmvnp([Mtrain, Otrain], Ytrain, \n",
    "                                               [Mtest, Otest],Ytest,\n",
    "                                               feature_priors=[moten_prior, obcat_prior],\n",
    "                                               temporal_prior=temporal_prior,\n",
    "                                               ridges=np.logspace(0,4,10),\n",
    "                                               normalize_hyparams=True,\n",
    "                                               folds=(1,5),\n",
    "                                               performance=True,\n",
    "                                               verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3be666610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plot_model_2dhist_comparison(np.nan_to_num(fit_spherical_pop['performance'].squeeze()),\n",
    "                                  np.nan_to_num(fit_banded_polar['performance'].squeeze()),\n",
    "                                  'spherical (population)', 'banded (voxelwise)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3be666b50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plot_model_2dhist_comparison(np.nan_to_num(fit_spherical_vox['performance'].squeeze()),\n",
    "                                  np.nan_to_num(fit_banded_polar['performance'].squeeze()),\n",
    "                                  'ridge\\n(spherical prior)', 'banded ridge\\n(non-spherical prior)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100: temporal 1/1=1.000, features 1/100=(0.0100, 0.0100)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2156, (25,50,75)pctl=(0.0859,0.1845,0.3275),(0.0<r>0.5): (2549,174)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1992, (25,50,75)pctl=(0.0763,0.1651,0.3018),(0.0<r>0.5): (2517,137)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2111, (25,50,75)pctl=(0.0822,0.1824,0.3216),(0.0<r>0.5): (2539,143)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2080, (25,50,75)pctl=(0.0839,0.1792,0.3178),(0.0<r>0.5): (2548,113)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2077, (25,50,75)pctl=(0.0835,0.1759,0.3133),(0.0<r>0.5): (2541,136)\n",
      "pop.cv.best:  1.000, mean=0.2083, (25,50,75)pctl=(0.0802,0.1746,0.3136),(0.0<r>0.5): (2615,132)\n",
      "2/100: temporal 1/1=1.000, features 2/100=(0.0100, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2102, (25,50,75)pctl=(0.0826,0.1776,0.3180),(0.0<r>0.5): (2545,166)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1953, (25,50,75)pctl=(0.0723,0.1609,0.2964),(0.0<r>0.5): (2510,130)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0813,0.1767,0.3142),(0.0<r>0.5): (2530,129)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2036, (25,50,75)pctl=(0.0810,0.1741,0.3088),(0.0<r>0.5): (2539,107)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2026, (25,50,75)pctl=(0.0808,0.1716,0.3064),(0.0<r>0.5): (2538,130)\n",
      "pop.cv.best:  1.000, mean=0.2037, (25,50,75)pctl=(0.0771,0.1709,0.3072),(0.0<r>0.5): (2609,124)\n",
      "3/100: temporal 1/1=1.000, features 3/100=(0.0100, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2092, (25,50,75)pctl=(0.0822,0.1772,0.3175),(0.0<r>0.5): (2539,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1961, (25,50,75)pctl=(0.0735,0.1613,0.2975),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2071, (25,50,75)pctl=(0.0818,0.1773,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1713,0.3061),(0.0<r>0.5): (2534,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0769,0.1703,0.3077),(0.0<r>0.5): (2609,125)\n",
      "4/100: temporal 1/1=1.000, features 4/100=(0.0100, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0820,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "5/100: temporal 1/1=1.000, features 5/100=(0.0100, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "6/100: temporal 1/1=1.000, features 6/100=(0.0100, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "7/100: temporal 1/1=1.000, features 7/100=(0.0100, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "8/100: temporal 1/1=1.000, features 8/100=(0.0100, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "9/100: temporal 1/1=1.000, features 9/100=(0.0100, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "10/100: temporal 1/1=1.000, features 10/100=(0.0100, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "11/100: temporal 1/1=1.000, features 11/100=(0.0464, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2345, (25,50,75)pctl=(0.0980,0.2054,0.3562),(0.0<r>0.5): (2583,208)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2160, (25,50,75)pctl=(0.0868,0.1883,0.3280),(0.0<r>0.5): (2549,149)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2303, (25,50,75)pctl=(0.0945,0.2036,0.3507),(0.0<r>0.5): (2561,193)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2247, (25,50,75)pctl=(0.0979,0.1993,0.3405),(0.0<r>0.5): (2574,146)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2272, (25,50,75)pctl=(0.0960,0.1965,0.3421),(0.0<r>0.5): (2562,169)\n",
      "pop.cv.best:  1.000, mean=0.2265, (25,50,75)pctl=(0.0960,0.1963,0.3441),(0.0<r>0.5): (2624,166)\n",
      "12/100: temporal 1/1=1.000, features 12/100=(0.0464, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2156, (25,50,75)pctl=(0.0859,0.1845,0.3275),(0.0<r>0.5): (2549,174)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1992, (25,50,75)pctl=(0.0763,0.1651,0.3018),(0.0<r>0.5): (2517,137)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2111, (25,50,75)pctl=(0.0822,0.1824,0.3216),(0.0<r>0.5): (2539,143)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2080, (25,50,75)pctl=(0.0839,0.1792,0.3178),(0.0<r>0.5): (2548,113)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2077, (25,50,75)pctl=(0.0835,0.1759,0.3133),(0.0<r>0.5): (2541,136)\n",
      "pop.cv.best:  1.000, mean=0.2083, (25,50,75)pctl=(0.0802,0.1746,0.3136),(0.0<r>0.5): (2615,132)\n",
      "13/100: temporal 1/1=1.000, features 13/100=(0.0464, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2102, (25,50,75)pctl=(0.0826,0.1776,0.3180),(0.0<r>0.5): (2545,166)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1953, (25,50,75)pctl=(0.0723,0.1609,0.2964),(0.0<r>0.5): (2510,130)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0813,0.1767,0.3142),(0.0<r>0.5): (2530,129)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2036, (25,50,75)pctl=(0.0810,0.1741,0.3088),(0.0<r>0.5): (2539,107)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2026, (25,50,75)pctl=(0.0808,0.1716,0.3064),(0.0<r>0.5): (2538,130)\n",
      "pop.cv.best:  1.000, mean=0.2037, (25,50,75)pctl=(0.0771,0.1709,0.3072),(0.0<r>0.5): (2609,124)\n",
      "14/100: temporal 1/1=1.000, features 14/100=(0.0464, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2092, (25,50,75)pctl=(0.0822,0.1772,0.3175),(0.0<r>0.5): (2539,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1961, (25,50,75)pctl=(0.0735,0.1613,0.2975),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2071, (25,50,75)pctl=(0.0818,0.1773,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1713,0.3061),(0.0<r>0.5): (2534,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0769,0.1703,0.3077),(0.0<r>0.5): (2609,125)\n",
      "15/100: temporal 1/1=1.000, features 15/100=(0.0464, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0820,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "16/100: temporal 1/1=1.000, features 16/100=(0.0464, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "17/100: temporal 1/1=1.000, features 17/100=(0.0464, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "18/100: temporal 1/1=1.000, features 18/100=(0.0464, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "19/100: temporal 1/1=1.000, features 19/100=(0.0464, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "20/100: temporal 1/1=1.000, features 20/100=(0.0464, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "21/100: temporal 1/1=1.000, features 21/100=(0.2154, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2242, (25,50,75)pctl=(0.0954,0.1983,0.3369),(0.0<r>0.5): (2579,151)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2031, (25,50,75)pctl=(0.0888,0.1768,0.3103),(0.0<r>0.5): (2547,083)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2217, (25,50,75)pctl=(0.0956,0.1968,0.3380),(0.0<r>0.5): (2567,132)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2226, (25,50,75)pctl=(0.1026,0.1985,0.3288),(0.0<r>0.5): (2598,089)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2193, (25,50,75)pctl=(0.0961,0.1922,0.3260),(0.0<r>0.5): (2572,127)\n",
      "pop.cv.best:  1.000, mean=0.2182, (25,50,75)pctl=(0.0958,0.1858,0.3261),(0.0<r>0.5): (2618,115)\n",
      "22/100: temporal 1/1=1.000, features 22/100=(0.2154, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2345, (25,50,75)pctl=(0.0980,0.2054,0.3562),(0.0<r>0.5): (2583,208)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2160, (25,50,75)pctl=(0.0868,0.1883,0.3280),(0.0<r>0.5): (2549,149)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2303, (25,50,75)pctl=(0.0945,0.2036,0.3507),(0.0<r>0.5): (2561,193)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2247, (25,50,75)pctl=(0.0979,0.1993,0.3405),(0.0<r>0.5): (2574,146)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2272, (25,50,75)pctl=(0.0960,0.1965,0.3421),(0.0<r>0.5): (2562,169)\n",
      "pop.cv.best:  1.000, mean=0.2265, (25,50,75)pctl=(0.0960,0.1963,0.3441),(0.0<r>0.5): (2624,166)\n",
      "23/100: temporal 1/1=1.000, features 23/100=(0.2154, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2156, (25,50,75)pctl=(0.0859,0.1845,0.3275),(0.0<r>0.5): (2549,174)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1992, (25,50,75)pctl=(0.0763,0.1651,0.3018),(0.0<r>0.5): (2517,137)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2111, (25,50,75)pctl=(0.0822,0.1824,0.3216),(0.0<r>0.5): (2539,143)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2080, (25,50,75)pctl=(0.0839,0.1792,0.3178),(0.0<r>0.5): (2548,113)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2077, (25,50,75)pctl=(0.0835,0.1759,0.3133),(0.0<r>0.5): (2541,136)\n",
      "pop.cv.best:  1.000, mean=0.2083, (25,50,75)pctl=(0.0802,0.1746,0.3136),(0.0<r>0.5): (2615,132)\n",
      "24/100: temporal 1/1=1.000, features 24/100=(0.2154, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2102, (25,50,75)pctl=(0.0826,0.1776,0.3180),(0.0<r>0.5): (2545,166)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1953, (25,50,75)pctl=(0.0723,0.1609,0.2964),(0.0<r>0.5): (2510,130)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0813,0.1767,0.3142),(0.0<r>0.5): (2530,129)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2036, (25,50,75)pctl=(0.0810,0.1741,0.3088),(0.0<r>0.5): (2539,107)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2026, (25,50,75)pctl=(0.0808,0.1716,0.3064),(0.0<r>0.5): (2538,130)\n",
      "pop.cv.best:  1.000, mean=0.2037, (25,50,75)pctl=(0.0771,0.1709,0.3072),(0.0<r>0.5): (2609,124)\n",
      "25/100: temporal 1/1=1.000, features 25/100=(0.2154, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2092, (25,50,75)pctl=(0.0822,0.1772,0.3175),(0.0<r>0.5): (2539,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1961, (25,50,75)pctl=(0.0735,0.1613,0.2975),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2071, (25,50,75)pctl=(0.0818,0.1773,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1713,0.3061),(0.0<r>0.5): (2534,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0769,0.1703,0.3077),(0.0<r>0.5): (2609,125)\n",
      "26/100: temporal 1/1=1.000, features 26/100=(0.2154, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0820,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "27/100: temporal 1/1=1.000, features 27/100=(0.2154, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "28/100: temporal 1/1=1.000, features 28/100=(0.2154, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "29/100: temporal 1/1=1.000, features 29/100=(0.2154, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "30/100: temporal 1/1=1.000, features 30/100=(0.2154, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "31/100: temporal 1/1=1.000, features 31/100=(1.0000, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1659, (25,50,75)pctl=(0.0707,0.1410,0.2490),(0.0<r>0.5): (2521,009)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1505, (25,50,75)pctl=(0.0620,0.1299,0.2264),(0.0<r>0.5): (2493,003)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1735, (25,50,75)pctl=(0.0699,0.1476,0.2647),(0.0<r>0.5): (2523,020)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1720, (25,50,75)pctl=(0.0819,0.1534,0.2510),(0.0<r>0.5): (2568,009)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1592, (25,50,75)pctl=(0.0672,0.1336,0.2340),(0.0<r>0.5): (2537,011)\n",
      "pop.cv.best:  1.000, mean=0.1642, (25,50,75)pctl=(0.0721,0.1353,0.2414),(0.0<r>0.5): (2607,004)\n",
      "32/100: temporal 1/1=1.000, features 32/100=(1.0000, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2242, (25,50,75)pctl=(0.0954,0.1983,0.3369),(0.0<r>0.5): (2579,151)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2031, (25,50,75)pctl=(0.0888,0.1768,0.3104),(0.0<r>0.5): (2547,083)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2218, (25,50,75)pctl=(0.0956,0.1968,0.3380),(0.0<r>0.5): (2567,132)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2226, (25,50,75)pctl=(0.1026,0.1985,0.3288),(0.0<r>0.5): (2598,089)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2193, (25,50,75)pctl=(0.0961,0.1922,0.3260),(0.0<r>0.5): (2572,128)\n",
      "pop.cv.best:  1.000, mean=0.2182, (25,50,75)pctl=(0.0958,0.1858,0.3261),(0.0<r>0.5): (2618,115)\n",
      "33/100: temporal 1/1=1.000, features 33/100=(1.0000, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2345, (25,50,75)pctl=(0.0980,0.2054,0.3562),(0.0<r>0.5): (2583,208)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2160, (25,50,75)pctl=(0.0868,0.1883,0.3280),(0.0<r>0.5): (2549,149)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2303, (25,50,75)pctl=(0.0945,0.2036,0.3507),(0.0<r>0.5): (2561,193)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2247, (25,50,75)pctl=(0.0979,0.1993,0.3405),(0.0<r>0.5): (2574,146)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2272, (25,50,75)pctl=(0.0960,0.1965,0.3421),(0.0<r>0.5): (2562,169)\n",
      "pop.cv.best:  1.000, mean=0.2265, (25,50,75)pctl=(0.0960,0.1963,0.3441),(0.0<r>0.5): (2624,166)\n",
      "34/100: temporal 1/1=1.000, features 34/100=(1.0000, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2156, (25,50,75)pctl=(0.0859,0.1845,0.3275),(0.0<r>0.5): (2549,174)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1992, (25,50,75)pctl=(0.0764,0.1651,0.3018),(0.0<r>0.5): (2517,137)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2111, (25,50,75)pctl=(0.0822,0.1824,0.3216),(0.0<r>0.5): (2539,143)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2080, (25,50,75)pctl=(0.0839,0.1792,0.3179),(0.0<r>0.5): (2548,113)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2077, (25,50,75)pctl=(0.0835,0.1759,0.3133),(0.0<r>0.5): (2541,136)\n",
      "pop.cv.best:  1.000, mean=0.2083, (25,50,75)pctl=(0.0802,0.1746,0.3136),(0.0<r>0.5): (2615,132)\n",
      "35/100: temporal 1/1=1.000, features 35/100=(1.0000, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2102, (25,50,75)pctl=(0.0826,0.1776,0.3180),(0.0<r>0.5): (2545,166)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1954, (25,50,75)pctl=(0.0723,0.1609,0.2964),(0.0<r>0.5): (2510,130)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0813,0.1768,0.3142),(0.0<r>0.5): (2530,129)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2036, (25,50,75)pctl=(0.0810,0.1741,0.3088),(0.0<r>0.5): (2539,107)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2026, (25,50,75)pctl=(0.0808,0.1716,0.3064),(0.0<r>0.5): (2538,130)\n",
      "pop.cv.best:  1.000, mean=0.2037, (25,50,75)pctl=(0.0771,0.1709,0.3072),(0.0<r>0.5): (2609,124)\n",
      "36/100: temporal 1/1=1.000, features 36/100=(1.0000, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2092, (25,50,75)pctl=(0.0822,0.1772,0.3175),(0.0<r>0.5): (2539,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1961, (25,50,75)pctl=(0.0735,0.1613,0.2975),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2071, (25,50,75)pctl=(0.0818,0.1773,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1713,0.3061),(0.0<r>0.5): (2534,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0769,0.1703,0.3077),(0.0<r>0.5): (2609,125)\n",
      "37/100: temporal 1/1=1.000, features 37/100=(1.0000, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0820,0.1771,0.3172),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,130)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "38/100: temporal 1/1=1.000, features 38/100=(1.0000, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "39/100: temporal 1/1=1.000, features 39/100=(1.0000, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "40/100: temporal 1/1=1.000, features 40/100=(1.0000, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2091, (25,50,75)pctl=(0.0819,0.1771,0.3171),(0.0<r>0.5): (2538,163)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0736,0.1614,0.2976),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2072, (25,50,75)pctl=(0.0819,0.1777,0.3147),(0.0<r>0.5): (2526,130)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2033, (25,50,75)pctl=(0.0806,0.1736,0.3083),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2022, (25,50,75)pctl=(0.0807,0.1711,0.3060),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2036, (25,50,75)pctl=(0.0768,0.1704,0.3077),(0.0<r>0.5): (2609,125)\n",
      "41/100: temporal 1/1=1.000, features 41/100=(4.6416, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1168, (25,50,75)pctl=(0.0463,0.0989,0.1754),(0.0<r>0.5): (2461,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1084, (25,50,75)pctl=(0.0436,0.0946,0.1621),(0.0<r>0.5): (2454,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1264, (25,50,75)pctl=(0.0507,0.1087,0.1920),(0.0<r>0.5): (2457,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1264, (25,50,75)pctl=(0.0610,0.1111,0.1757),(0.0<r>0.5): (2519,002)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1121, (25,50,75)pctl=(0.0440,0.0926,0.1662),(0.0<r>0.5): (2444,001)\n",
      "pop.cv.best:  1.000, mean=0.1181, (25,50,75)pctl=(0.0530,0.0980,0.1689),(0.0<r>0.5): (2589,000)\n",
      "42/100: temporal 1/1=1.000, features 42/100=(4.6416, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1659, (25,50,75)pctl=(0.0707,0.1410,0.2490),(0.0<r>0.5): (2521,009)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1506, (25,50,75)pctl=(0.0620,0.1299,0.2264),(0.0<r>0.5): (2494,003)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1736, (25,50,75)pctl=(0.0699,0.1477,0.2648),(0.0<r>0.5): (2523,020)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1721, (25,50,75)pctl=(0.0819,0.1534,0.2510),(0.0<r>0.5): (2568,009)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1592, (25,50,75)pctl=(0.0672,0.1336,0.2340),(0.0<r>0.5): (2537,011)\n",
      "pop.cv.best:  1.000, mean=0.1643, (25,50,75)pctl=(0.0721,0.1353,0.2415),(0.0<r>0.5): (2607,004)\n",
      "43/100: temporal 1/1=1.000, features 43/100=(4.6416, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2243, (25,50,75)pctl=(0.0955,0.1984,0.3371),(0.0<r>0.5): (2580,152)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2032, (25,50,75)pctl=(0.0888,0.1769,0.3106),(0.0<r>0.5): (2547,083)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2219, (25,50,75)pctl=(0.0956,0.1969,0.3381),(0.0<r>0.5): (2567,132)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2227, (25,50,75)pctl=(0.1026,0.1985,0.3289),(0.0<r>0.5): (2598,089)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2193, (25,50,75)pctl=(0.0961,0.1923,0.3261),(0.0<r>0.5): (2572,129)\n",
      "pop.cv.best:  1.000, mean=0.2183, (25,50,75)pctl=(0.0960,0.1860,0.3263),(0.0<r>0.5): (2618,116)\n",
      "44/100: temporal 1/1=1.000, features 44/100=(4.6416, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2346, (25,50,75)pctl=(0.0981,0.2055,0.3564),(0.0<r>0.5): (2583,210)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2162, (25,50,75)pctl=(0.0869,0.1885,0.3282),(0.0<r>0.5): (2549,150)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2305, (25,50,75)pctl=(0.0947,0.2037,0.3509),(0.0<r>0.5): (2561,194)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2249, (25,50,75)pctl=(0.0980,0.1995,0.3408),(0.0<r>0.5): (2574,149)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2273, (25,50,75)pctl=(0.0959,0.1967,0.3422),(0.0<r>0.5): (2562,169)\n",
      "pop.cv.best:  1.000, mean=0.2267, (25,50,75)pctl=(0.0961,0.1964,0.3442),(0.0<r>0.5): (2624,166)\n",
      "45/100: temporal 1/1=1.000, features 45/100=(4.6416, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2157, (25,50,75)pctl=(0.0859,0.1848,0.3277),(0.0<r>0.5): (2549,174)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1994, (25,50,75)pctl=(0.0765,0.1652,0.3021),(0.0<r>0.5): (2517,137)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2113, (25,50,75)pctl=(0.0823,0.1825,0.3219),(0.0<r>0.5): (2539,143)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2082, (25,50,75)pctl=(0.0840,0.1793,0.3182),(0.0<r>0.5): (2548,113)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2078, (25,50,75)pctl=(0.0836,0.1761,0.3134),(0.0<r>0.5): (2541,136)\n",
      "pop.cv.best:  1.000, mean=0.2085, (25,50,75)pctl=(0.0802,0.1747,0.3139),(0.0<r>0.5): (2616,133)\n",
      "46/100: temporal 1/1=1.000, features 46/100=(4.6416, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2104, (25,50,75)pctl=(0.0826,0.1777,0.3183),(0.0<r>0.5): (2545,166)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1955, (25,50,75)pctl=(0.0724,0.1611,0.2967),(0.0<r>0.5): (2510,131)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2069, (25,50,75)pctl=(0.0815,0.1770,0.3145),(0.0<r>0.5): (2530,131)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2038, (25,50,75)pctl=(0.0812,0.1743,0.3090),(0.0<r>0.5): (2539,107)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2027, (25,50,75)pctl=(0.0808,0.1717,0.3066),(0.0<r>0.5): (2538,130)\n",
      "pop.cv.best:  1.000, mean=0.2038, (25,50,75)pctl=(0.0771,0.1711,0.3075),(0.0<r>0.5): (2609,125)\n",
      "47/100: temporal 1/1=1.000, features 47/100=(4.6416, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2094, (25,50,75)pctl=(0.0823,0.1774,0.3176),(0.0<r>0.5): (2540,164)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1963, (25,50,75)pctl=(0.0736,0.1615,0.2977),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2073, (25,50,75)pctl=(0.0819,0.1775,0.3150),(0.0<r>0.5): (2526,131)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2035, (25,50,75)pctl=(0.0807,0.1737,0.3086),(0.0<r>0.5): (2539,107)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2023, (25,50,75)pctl=(0.0808,0.1716,0.3063),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2038, (25,50,75)pctl=(0.0769,0.1705,0.3079),(0.0<r>0.5): (2609,126)\n",
      "48/100: temporal 1/1=1.000, features 48/100=(4.6416, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2093, (25,50,75)pctl=(0.0821,0.1774,0.3173),(0.0<r>0.5): (2538,164)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1964, (25,50,75)pctl=(0.0736,0.1616,0.2978),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2073, (25,50,75)pctl=(0.0819,0.1778,0.3151),(0.0<r>0.5): (2526,131)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2035, (25,50,75)pctl=(0.0806,0.1738,0.3086),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2023, (25,50,75)pctl=(0.0808,0.1714,0.3063),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2038, (25,50,75)pctl=(0.0769,0.1705,0.3079),(0.0<r>0.5): (2609,126)\n",
      "49/100: temporal 1/1=1.000, features 49/100=(4.6416, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2093, (25,50,75)pctl=(0.0821,0.1774,0.3173),(0.0<r>0.5): (2538,164)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1964, (25,50,75)pctl=(0.0736,0.1616,0.2979),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2073, (25,50,75)pctl=(0.0819,0.1779,0.3151),(0.0<r>0.5): (2526,131)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2035, (25,50,75)pctl=(0.0806,0.1738,0.3086),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2023, (25,50,75)pctl=(0.0807,0.1714,0.3063),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2038, (25,50,75)pctl=(0.0769,0.1705,0.3079),(0.0<r>0.5): (2609,126)\n",
      "50/100: temporal 1/1=1.000, features 50/100=(4.6416, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2093, (25,50,75)pctl=(0.0821,0.1774,0.3173),(0.0<r>0.5): (2538,164)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1964, (25,50,75)pctl=(0.0736,0.1616,0.2979),(0.0<r>0.5): (2513,135)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2073, (25,50,75)pctl=(0.0819,0.1779,0.3151),(0.0<r>0.5): (2526,131)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2035, (25,50,75)pctl=(0.0806,0.1738,0.3086),(0.0<r>0.5): (2539,106)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2023, (25,50,75)pctl=(0.0807,0.1714,0.3063),(0.0<r>0.5): (2535,131)\n",
      "pop.cv.best:  1.000, mean=0.2038, (25,50,75)pctl=(0.0769,0.1705,0.3079),(0.0<r>0.5): (2609,126)\n",
      "51/100: temporal 1/1=1.000, features 51/100=(21.5443, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1013, (25,50,75)pctl=(0.0398,0.0849,0.1476),(0.0<r>0.5): (2423,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0917, (25,50,75)pctl=(0.0345,0.0801,0.1350),(0.0<r>0.5): (2413,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1089, (25,50,75)pctl=(0.0436,0.0963,0.1647),(0.0<r>0.5): (2427,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1109, (25,50,75)pctl=(0.0546,0.0971,0.1503),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0979, (25,50,75)pctl=(0.0364,0.0820,0.1430),(0.0<r>0.5): (2395,001)\n",
      "pop.cv.best:  1.000, mean=0.1021, (25,50,75)pctl=(0.0473,0.0861,0.1420),(0.0<r>0.5): (2582,000)\n",
      "52/100: temporal 1/1=1.000, features 52/100=(21.5443, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1169, (25,50,75)pctl=(0.0464,0.0990,0.1755),(0.0<r>0.5): (2461,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1085, (25,50,75)pctl=(0.0436,0.0947,0.1622),(0.0<r>0.5): (2454,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1265, (25,50,75)pctl=(0.0507,0.1088,0.1922),(0.0<r>0.5): (2457,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1265, (25,50,75)pctl=(0.0611,0.1111,0.1758),(0.0<r>0.5): (2519,002)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1122, (25,50,75)pctl=(0.0440,0.0927,0.1663),(0.0<r>0.5): (2444,001)\n",
      "pop.cv.best:  1.000, mean=0.1181, (25,50,75)pctl=(0.0530,0.0980,0.1690),(0.0<r>0.5): (2589,000)\n",
      "53/100: temporal 1/1=1.000, features 53/100=(21.5443, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1666, (25,50,75)pctl=(0.0710,0.1416,0.2498),(0.0<r>0.5): (2522,009)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1511, (25,50,75)pctl=(0.0620,0.1306,0.2270),(0.0<r>0.5): (2494,003)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1742, (25,50,75)pctl=(0.0703,0.1487,0.2656),(0.0<r>0.5): (2523,020)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1727, (25,50,75)pctl=(0.0824,0.1542,0.2522),(0.0<r>0.5): (2568,009)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1597, (25,50,75)pctl=(0.0675,0.1342,0.2346),(0.0<r>0.5): (2536,011)\n",
      "pop.cv.best:  1.000, mean=0.1649, (25,50,75)pctl=(0.0725,0.1359,0.2426),(0.0<r>0.5): (2608,004)\n",
      "54/100: temporal 1/1=1.000, features 54/100=(21.5443, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2261, (25,50,75)pctl=(0.0970,0.2009,0.3401),(0.0<r>0.5): (2581,164)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2053, (25,50,75)pctl=(0.0894,0.1787,0.3139),(0.0<r>0.5): (2546,087)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2238, (25,50,75)pctl=(0.0973,0.1981,0.3399),(0.0<r>0.5): (2569,141)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2246, (25,50,75)pctl=(0.1034,0.2004,0.3321),(0.0<r>0.5): (2599,096)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2209, (25,50,75)pctl=(0.0969,0.1942,0.3286),(0.0<r>0.5): (2574,134)\n",
      "pop.cv.best:  1.000, mean=0.2201, (25,50,75)pctl=(0.0969,0.1884,0.3285),(0.0<r>0.5): (2618,119)\n",
      "55/100: temporal 1/1=1.000, features 55/100=(21.5443, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2376, (25,50,75)pctl=(0.0999,0.2088,0.3613),(0.0<r>0.5): (2583,216)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2194, (25,50,75)pctl=(0.0888,0.1920,0.3330),(0.0<r>0.5): (2551,162)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2334, (25,50,75)pctl=(0.0969,0.2063,0.3557),(0.0<r>0.5): (2565,200)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2278, (25,50,75)pctl=(0.0996,0.2022,0.3450),(0.0<r>0.5): (2574,158)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2298, (25,50,75)pctl=(0.0972,0.1997,0.3458),(0.0<r>0.5): (2561,180)\n",
      "pop.cv.best:  1.000, mean=0.2296, (25,50,75)pctl=(0.0980,0.1991,0.3486),(0.0<r>0.5): (2624,174)\n",
      "56/100: temporal 1/1=1.000, features 56/100=(21.5443, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2188, (25,50,75)pctl=(0.0885,0.1881,0.3330),(0.0<r>0.5): (2551,182)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2027, (25,50,75)pctl=(0.0779,0.1693,0.3073),(0.0<r>0.5): (2523,143)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2144, (25,50,75)pctl=(0.0840,0.1849,0.3272),(0.0<r>0.5): (2539,150)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2114, (25,50,75)pctl=(0.0855,0.1833,0.3228),(0.0<r>0.5): (2547,121)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2104, (25,50,75)pctl=(0.0848,0.1795,0.3173),(0.0<r>0.5): (2545,138)\n",
      "pop.cv.best:  1.000, mean=0.2115, (25,50,75)pctl=(0.0816,0.1776,0.3187),(0.0<r>0.5): (2617,139)\n",
      "57/100: temporal 1/1=1.000, features 57/100=(21.5443, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2135, (25,50,75)pctl=(0.0845,0.1815,0.3230),(0.0<r>0.5): (2544,173)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1988, (25,50,75)pctl=(0.0747,0.1639,0.3017),(0.0<r>0.5): (2513,140)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2100, (25,50,75)pctl=(0.0825,0.1806,0.3189),(0.0<r>0.5): (2534,142)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2070, (25,50,75)pctl=(0.0827,0.1778,0.3138),(0.0<r>0.5): (2542,117)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2053, (25,50,75)pctl=(0.0815,0.1744,0.3108),(0.0<r>0.5): (2539,134)\n",
      "pop.cv.best:  1.000, mean=0.2069, (25,50,75)pctl=(0.0781,0.1738,0.3118),(0.0<r>0.5): (2610,135)\n",
      "58/100: temporal 1/1=1.000, features 58/100=(21.5443, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2126, (25,50,75)pctl=(0.0843,0.1809,0.3215),(0.0<r>0.5): (2542,173)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1994, (25,50,75)pctl=(0.0752,0.1639,0.3028),(0.0<r>0.5): (2517,141)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2103, (25,50,75)pctl=(0.0835,0.1814,0.3194),(0.0<r>0.5): (2529,142)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0827,0.1775,0.3133),(0.0<r>0.5): (2542,117)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2049, (25,50,75)pctl=(0.0821,0.1746,0.3106),(0.0<r>0.5): (2537,133)\n",
      "pop.cv.best:  1.000, mean=0.2068, (25,50,75)pctl=(0.0781,0.1739,0.3120),(0.0<r>0.5): (2610,134)\n",
      "59/100: temporal 1/1=1.000, features 59/100=(21.5443, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2125, (25,50,75)pctl=(0.0841,0.1808,0.3217),(0.0<r>0.5): (2542,173)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1995, (25,50,75)pctl=(0.0751,0.1638,0.3029),(0.0<r>0.5): (2517,141)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2104, (25,50,75)pctl=(0.0836,0.1814,0.3197),(0.0<r>0.5): (2529,142)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0828,0.1773,0.3133),(0.0<r>0.5): (2542,117)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2049, (25,50,75)pctl=(0.0821,0.1746,0.3105),(0.0<r>0.5): (2537,133)\n",
      "pop.cv.best:  1.000, mean=0.2068, (25,50,75)pctl=(0.0781,0.1740,0.3121),(0.0<r>0.5): (2610,134)\n",
      "60/100: temporal 1/1=1.000, features 60/100=(21.5443, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2125, (25,50,75)pctl=(0.0841,0.1808,0.3217),(0.0<r>0.5): (2542,173)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1995, (25,50,75)pctl=(0.0751,0.1638,0.3029),(0.0<r>0.5): (2517,141)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2104, (25,50,75)pctl=(0.0836,0.1814,0.3197),(0.0<r>0.5): (2529,142)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2067, (25,50,75)pctl=(0.0828,0.1773,0.3133),(0.0<r>0.5): (2542,117)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2049, (25,50,75)pctl=(0.0821,0.1746,0.3105),(0.0<r>0.5): (2537,133)\n",
      "pop.cv.best:  1.000, mean=0.2068, (25,50,75)pctl=(0.0781,0.1740,0.3121),(0.0<r>0.5): (2610,134)\n",
      "61/100: temporal 1/1=1.000, features 61/100=(100.0000, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1001, (25,50,75)pctl=(0.0396,0.0842,0.1454),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0902, (25,50,75)pctl=(0.0334,0.0792,0.1327),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1074, (25,50,75)pctl=(0.0432,0.0945,0.1615),(0.0<r>0.5): (2422,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1097, (25,50,75)pctl=(0.0537,0.0955,0.1484),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0967, (25,50,75)pctl=(0.0357,0.0807,0.1414),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1008, (25,50,75)pctl=(0.0465,0.0846,0.1393),(0.0<r>0.5): (2581,000)\n",
      "62/100: temporal 1/1=1.000, features 62/100=(100.0000, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1014, (25,50,75)pctl=(0.0399,0.0850,0.1477),(0.0<r>0.5): (2424,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0917, (25,50,75)pctl=(0.0345,0.0801,0.1351),(0.0<r>0.5): (2415,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1089, (25,50,75)pctl=(0.0436,0.0964,0.1648),(0.0<r>0.5): (2427,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1110, (25,50,75)pctl=(0.0547,0.0971,0.1502),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0980, (25,50,75)pctl=(0.0365,0.0821,0.1431),(0.0<r>0.5): (2396,001)\n",
      "pop.cv.best:  1.000, mean=0.1022, (25,50,75)pctl=(0.0474,0.0862,0.1420),(0.0<r>0.5): (2582,000)\n",
      "63/100: temporal 1/1=1.000, features 63/100=(100.0000, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1183, (25,50,75)pctl=(0.0475,0.0997,0.1773),(0.0<r>0.5): (2467,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1094, (25,50,75)pctl=(0.0440,0.0959,0.1635),(0.0<r>0.5): (2454,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1278, (25,50,75)pctl=(0.0511,0.1102,0.1942),(0.0<r>0.5): (2457,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1276, (25,50,75)pctl=(0.0615,0.1119,0.1771),(0.0<r>0.5): (2518,002)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1132, (25,50,75)pctl=(0.0446,0.0944,0.1675),(0.0<r>0.5): (2445,001)\n",
      "pop.cv.best:  1.000, mean=0.1192, (25,50,75)pctl=(0.0537,0.0989,0.1706),(0.0<r>0.5): (2590,000)\n",
      "64/100: temporal 1/1=1.000, features 64/100=(100.0000, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1757, (25,50,75)pctl=(0.0743,0.1510,0.2625),(0.0<r>0.5): (2537,018)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1591, (25,50,75)pctl=(0.0665,0.1375,0.2384),(0.0<r>0.5): (2504,007)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1830, (25,50,75)pctl=(0.0753,0.1566,0.2802),(0.0<r>0.5): (2537,032)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1810, (25,50,75)pctl=(0.0867,0.1618,0.2642),(0.0<r>0.5): (2574,011)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1675, (25,50,75)pctl=(0.0716,0.1420,0.2464),(0.0<r>0.5): (2541,016)\n",
      "pop.cv.best:  1.000, mean=0.1733, (25,50,75)pctl=(0.0761,0.1434,0.2557),(0.0<r>0.5): (2609,007)\n",
      "65/100: temporal 1/1=1.000, features 65/100=(100.0000, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2456, (25,50,75)pctl=(0.1076,0.2208,0.3688),(0.0<r>0.5): (2589,231)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2270, (25,50,75)pctl=(0.1017,0.2017,0.3453),(0.0<r>0.5): (2575,156)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2423, (25,50,75)pctl=(0.1087,0.2178,0.3664),(0.0<r>0.5): (2585,210)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2417, (25,50,75)pctl=(0.1130,0.2207,0.3579),(0.0<r>0.5): (2609,158)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2400, (25,50,75)pctl=(0.1092,0.2139,0.3584),(0.0<r>0.5): (2589,191)\n",
      "pop.cv.best:  1.000, mean=0.2393, (25,50,75)pctl=(0.1075,0.2118,0.3565),(0.0<r>0.5): (2628,183)\n",
      "66/100: temporal 1/1=1.000, features 66/100=(100.0000, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2653, (25,50,75)pctl=(0.1151,0.2423,0.4016),(0.0<r>0.5): (2601,324)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2468, (25,50,75)pctl=(0.1047,0.2222,0.3719),(0.0<r>0.5): (2571,251)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2586, (25,50,75)pctl=(0.1125,0.2353,0.3937),(0.0<r>0.5): (2581,305)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2517, (25,50,75)pctl=(0.1127,0.2330,0.3818),(0.0<r>0.5): (2586,246)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2548, (25,50,75)pctl=(0.1140,0.2302,0.3836),(0.0<r>0.5): (2583,265)\n",
      "pop.cv.best:  1.000, mean=0.2554, (25,50,75)pctl=(0.1123,0.2303,0.3853),(0.0<r>0.5): (2629,267)\n",
      "67/100: temporal 1/1=1.000, features 67/100=(100.0000, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2469, (25,50,75)pctl=(0.1040,0.2214,0.3739),(0.0<r>0.5): (2575,273)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2309, (25,50,75)pctl=(0.0913,0.1999,0.3530),(0.0<r>0.5): (2556,217)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2412, (25,50,75)pctl=(0.0988,0.2143,0.3670),(0.0<r>0.5): (2555,239)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2377, (25,50,75)pctl=(0.0990,0.2148,0.3621),(0.0<r>0.5): (2562,202)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2357, (25,50,75)pctl=(0.0984,0.2099,0.3586),(0.0<r>0.5): (2568,221)\n",
      "pop.cv.best:  1.000, mean=0.2385, (25,50,75)pctl=(0.0963,0.2111,0.3622),(0.0<r>0.5): (2620,225)\n",
      "68/100: temporal 1/1=1.000, features 68/100=(100.0000, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2425, (25,50,75)pctl=(0.1007,0.2143,0.3640),(0.0<r>0.5): (2570,266)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2277, (25,50,75)pctl=(0.0897,0.1957,0.3473),(0.0<r>0.5): (2551,213)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2377, (25,50,75)pctl=(0.0964,0.2099,0.3612),(0.0<r>0.5): (2554,233)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2344, (25,50,75)pctl=(0.0960,0.2104,0.3561),(0.0<r>0.5): (2556,192)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2316, (25,50,75)pctl=(0.0964,0.2052,0.3523),(0.0<r>0.5): (2563,213)\n",
      "pop.cv.best:  1.000, mean=0.2348, (25,50,75)pctl=(0.0928,0.2062,0.3552),(0.0<r>0.5): (2618,217)\n",
      "69/100: temporal 1/1=1.000, features 69/100=(100.0000, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2421, (25,50,75)pctl=(0.1004,0.2139,0.3635),(0.0<r>0.5): (2571,265)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2277, (25,50,75)pctl=(0.0900,0.1955,0.3469),(0.0<r>0.5): (2551,213)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2377, (25,50,75)pctl=(0.0964,0.2100,0.3611),(0.0<r>0.5): (2555,233)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2342, (25,50,75)pctl=(0.0959,0.2103,0.3558),(0.0<r>0.5): (2555,192)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2313, (25,50,75)pctl=(0.0964,0.2048,0.3517),(0.0<r>0.5): (2563,213)\n",
      "pop.cv.best:  1.000, mean=0.2346, (25,50,75)pctl=(0.0927,0.2056,0.3549),(0.0<r>0.5): (2618,217)\n",
      "70/100: temporal 1/1=1.000, features 70/100=(100.0000, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2421, (25,50,75)pctl=(0.1004,0.2139,0.3634),(0.0<r>0.5): (2571,265)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2277, (25,50,75)pctl=(0.0900,0.1955,0.3469),(0.0<r>0.5): (2551,213)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2377, (25,50,75)pctl=(0.0964,0.2100,0.3611),(0.0<r>0.5): (2555,233)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2342, (25,50,75)pctl=(0.0959,0.2103,0.3558),(0.0<r>0.5): (2555,192)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2313, (25,50,75)pctl=(0.0965,0.2048,0.3517),(0.0<r>0.5): (2563,213)\n",
      "pop.cv.best:  1.000, mean=0.2346, (25,50,75)pctl=(0.0927,0.2056,0.3549),(0.0<r>0.5): (2618,217)\n",
      "71/100: temporal 1/1=1.000, features 71/100=(464.1589, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1000, (25,50,75)pctl=(0.0395,0.0841,0.1453),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0902, (25,50,75)pctl=(0.0333,0.0792,0.1327),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1073, (25,50,75)pctl=(0.0431,0.0945,0.1613),(0.0<r>0.5): (2422,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1096, (25,50,75)pctl=(0.0537,0.0954,0.1484),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0967, (25,50,75)pctl=(0.0356,0.0806,0.1413),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1008, (25,50,75)pctl=(0.0465,0.0845,0.1393),(0.0<r>0.5): (2581,000)\n",
      "72/100: temporal 1/1=1.000, features 72/100=(464.1589, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1001, (25,50,75)pctl=(0.0396,0.0842,0.1454),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0903, (25,50,75)pctl=(0.0334,0.0792,0.1329),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1075, (25,50,75)pctl=(0.0432,0.0945,0.1616),(0.0<r>0.5): (2421,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1097, (25,50,75)pctl=(0.0538,0.0956,0.1485),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0968, (25,50,75)pctl=(0.0357,0.0808,0.1415),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1009, (25,50,75)pctl=(0.0465,0.0847,0.1394),(0.0<r>0.5): (2581,000)\n",
      "73/100: temporal 1/1=1.000, features 73/100=(464.1589, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1028, (25,50,75)pctl=(0.0405,0.0865,0.1491),(0.0<r>0.5): (2431,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0926, (25,50,75)pctl=(0.0347,0.0805,0.1359),(0.0<r>0.5): (2424,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1103, (25,50,75)pctl=(0.0441,0.0979,0.1663),(0.0<r>0.5): (2430,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1120, (25,50,75)pctl=(0.0550,0.0979,0.1518),(0.0<r>0.5): (2505,002)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0991, (25,50,75)pctl=(0.0371,0.0832,0.1451),(0.0<r>0.5): (2403,001)\n",
      "pop.cv.best:  1.000, mean=0.1033, (25,50,75)pctl=(0.0481,0.0873,0.1434),(0.0<r>0.5): (2582,000)\n",
      "74/100: temporal 1/1=1.000, features 74/100=(464.1589, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1311, (25,50,75)pctl=(0.0541,0.1108,0.1948),(0.0<r>0.5): (2483,006)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1182, (25,50,75)pctl=(0.0478,0.1033,0.1742),(0.0<r>0.5): (2468,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1400, (25,50,75)pctl=(0.0579,0.1205,0.2120),(0.0<r>0.5): (2484,001)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1385, (25,50,75)pctl=(0.0683,0.1223,0.1918),(0.0<r>0.5): (2540,003)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1245, (25,50,75)pctl=(0.0506,0.1031,0.1831),(0.0<r>0.5): (2475,001)\n",
      "pop.cv.best:  1.000, mean=0.1304, (25,50,75)pctl=(0.0581,0.1081,0.1878),(0.0<r>0.5): (2599,001)\n",
      "75/100: temporal 1/1=1.000, features 75/100=(464.1589, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2093, (25,50,75)pctl=(0.0879,0.1847,0.3134),(0.0<r>0.5): (2563,087)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1962, (25,50,75)pctl=(0.0875,0.1732,0.2922),(0.0<r>0.5): (2551,042)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2122, (25,50,75)pctl=(0.0931,0.1866,0.3267),(0.0<r>0.5): (2556,069)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2093, (25,50,75)pctl=(0.1007,0.1883,0.3061),(0.0<r>0.5): (2598,047)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2025, (25,50,75)pctl=(0.0910,0.1759,0.2983),(0.0<r>0.5): (2570,061)\n",
      "pop.cv.best:  1.000, mean=0.2059, (25,50,75)pctl=(0.0926,0.1763,0.3073),(0.0<r>0.5): (2616,046)\n",
      "76/100: temporal 1/1=1.000, features 76/100=(464.1589, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2997, (25,50,75)pctl=(0.1411,0.2869,0.4468),(0.0<r>0.5): (2606,451)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2762, (25,50,75)pctl=(0.1334,0.2609,0.4079),(0.0<r>0.5): (2602,314)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2835, (25,50,75)pctl=(0.1381,0.2727,0.4200),(0.0<r>0.5): (2603,365)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2745, (25,50,75)pctl=(0.1379,0.2644,0.4031),(0.0<r>0.5): (2608,270)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2839, (25,50,75)pctl=(0.1393,0.2713,0.4195),(0.0<r>0.5): (2606,345)\n",
      "pop.cv.best:  1.000, mean=0.2836, (25,50,75)pctl=(0.1396,0.2706,0.4167),(0.0<r>0.5): (2635,334)\n",
      "77/100: temporal 1/1=1.000, features 77/100=(464.1589, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.3093, (25,50,75)pctl=(0.1546,0.3017,0.4590),(0.0<r>0.5): (2605,480)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2800, (25,50,75)pctl=(0.1343,0.2626,0.4159),(0.0<r>0.5): (2595,354)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2887, (25,50,75)pctl=(0.1413,0.2773,0.4271),(0.0<r>0.5): (2590,392)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2794, (25,50,75)pctl=(0.1407,0.2730,0.4160),(0.0<r>0.5): (2591,308)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2875, (25,50,75)pctl=(0.1446,0.2764,0.4267),(0.0<r>0.5): (2604,349)\n",
      "pop.cv.best:  1.000, mean=0.2890, (25,50,75)pctl=(0.1429,0.2767,0.4285),(0.0<r>0.5): (2632,362)\n",
      "78/100: temporal 1/1=1.000, features 78/100=(464.1589, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2930, (25,50,75)pctl=(0.1393,0.2810,0.4350),(0.0<r>0.5): (2597,417)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2664, (25,50,75)pctl=(0.1214,0.2423,0.3996),(0.0<r>0.5): (2583,324)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2743, (25,50,75)pctl=(0.1306,0.2575,0.4055),(0.0<r>0.5): (2577,343)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2689, (25,50,75)pctl=(0.1302,0.2581,0.4041),(0.0<r>0.5): (2579,278)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2707, (25,50,75)pctl=(0.1311,0.2586,0.4064),(0.0<r>0.5): (2588,282)\n",
      "pop.cv.best:  1.000, mean=0.2747, (25,50,75)pctl=(0.1281,0.2575,0.4091),(0.0<r>0.5): (2625,319)\n",
      "79/100: temporal 1/1=1.000, features 79/100=(464.1589, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2915, (25,50,75)pctl=(0.1378,0.2791,0.4335),(0.0<r>0.5): (2596,414)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2652, (25,50,75)pctl=(0.1210,0.2405,0.3977),(0.0<r>0.5): (2583,324)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2730, (25,50,75)pctl=(0.1283,0.2557,0.4030),(0.0<r>0.5): (2574,340)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2679, (25,50,75)pctl=(0.1297,0.2568,0.4029),(0.0<r>0.5): (2578,274)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2692, (25,50,75)pctl=(0.1303,0.2566,0.4033),(0.0<r>0.5): (2586,280)\n",
      "pop.cv.best:  1.000, mean=0.2733, (25,50,75)pctl=(0.1270,0.2563,0.4071),(0.0<r>0.5): (2625,316)\n",
      "80/100: temporal 1/1=1.000, features 80/100=(464.1589, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2914, (25,50,75)pctl=(0.1378,0.2790,0.4334),(0.0<r>0.5): (2596,414)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2651, (25,50,75)pctl=(0.1210,0.2403,0.3976),(0.0<r>0.5): (2583,324)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2729, (25,50,75)pctl=(0.1283,0.2556,0.4030),(0.0<r>0.5): (2574,340)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2678, (25,50,75)pctl=(0.1297,0.2568,0.4028),(0.0<r>0.5): (2578,274)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2691, (25,50,75)pctl=(0.1303,0.2563,0.4033),(0.0<r>0.5): (2586,280)\n",
      "pop.cv.best:  1.000, mean=0.2733, (25,50,75)pctl=(0.1269,0.2561,0.4071),(0.0<r>0.5): (2625,316)\n",
      "81/100: temporal 1/1=1.000, features 81/100=(2154.4347, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1000, (25,50,75)pctl=(0.0395,0.0841,0.1453),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0902, (25,50,75)pctl=(0.0333,0.0792,0.1327),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1073, (25,50,75)pctl=(0.0431,0.0945,0.1613),(0.0<r>0.5): (2422,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1096, (25,50,75)pctl=(0.0537,0.0954,0.1484),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0967, (25,50,75)pctl=(0.0356,0.0806,0.1413),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1007, (25,50,75)pctl=(0.0465,0.0845,0.1393),(0.0<r>0.5): (2581,000)\n",
      "82/100: temporal 1/1=1.000, features 82/100=(2154.4347, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1001, (25,50,75)pctl=(0.0396,0.0842,0.1453),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0902, (25,50,75)pctl=(0.0334,0.0791,0.1327),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1074, (25,50,75)pctl=(0.0431,0.0945,0.1615),(0.0<r>0.5): (2421,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1096, (25,50,75)pctl=(0.0538,0.0954,0.1485),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0967, (25,50,75)pctl=(0.0356,0.0807,0.1414),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1008, (25,50,75)pctl=(0.0465,0.0846,0.1393),(0.0<r>0.5): (2581,000)\n",
      "83/100: temporal 1/1=1.000, features 83/100=(2154.4347, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1015, (25,50,75)pctl=(0.0403,0.0855,0.1468),(0.0<r>0.5): (2430,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0911, (25,50,75)pctl=(0.0338,0.0797,0.1340),(0.0<r>0.5): (2417,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1088, (25,50,75)pctl=(0.0437,0.0960,0.1633),(0.0<r>0.5): (2426,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1108, (25,50,75)pctl=(0.0545,0.0961,0.1499),(0.0<r>0.5): (2504,002)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0979, (25,50,75)pctl=(0.0360,0.0825,0.1432),(0.0<r>0.5): (2398,001)\n",
      "pop.cv.best:  1.000, mean=0.1020, (25,50,75)pctl=(0.0471,0.0858,0.1410),(0.0<r>0.5): (2582,000)\n",
      "84/100: temporal 1/1=1.000, features 84/100=(2154.4347, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1148, (25,50,75)pctl=(0.0465,0.0957,0.1634),(0.0<r>0.5): (2473,004)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1008, (25,50,75)pctl=(0.0400,0.0885,0.1474),(0.0<r>0.5): (2433,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1222, (25,50,75)pctl=(0.0504,0.1073,0.1810),(0.0<r>0.5): (2468,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1226, (25,50,75)pctl=(0.0609,0.1074,0.1658),(0.0<r>0.5): (2523,003)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1106, (25,50,75)pctl=(0.0437,0.0922,0.1617),(0.0<r>0.5): (2449,001)\n",
      "pop.cv.best:  1.000, mean=0.1142, (25,50,75)pctl=(0.0528,0.0966,0.1575),(0.0<r>0.5): (2589,001)\n",
      "85/100: temporal 1/1=1.000, features 85/100=(2154.4347, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1570, (25,50,75)pctl=(0.0668,0.1354,0.2275),(0.0<r>0.5): (2520,038)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1489, (25,50,75)pctl=(0.0662,0.1296,0.2187),(0.0<r>0.5): (2517,008)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1658, (25,50,75)pctl=(0.0709,0.1459,0.2486),(0.0<r>0.5): (2515,010)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1659, (25,50,75)pctl=(0.0832,0.1468,0.2309),(0.0<r>0.5): (2573,020)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1594, (25,50,75)pctl=(0.0693,0.1363,0.2356),(0.0<r>0.5): (2532,020)\n",
      "pop.cv.best:  1.000, mean=0.1594, (25,50,75)pctl=(0.0731,0.1364,0.2314),(0.0<r>0.5): (2608,015)\n",
      "86/100: temporal 1/1=1.000, features 86/100=(2154.4347, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2478, (25,50,75)pctl=(0.1140,0.2351,0.3683),(0.0<r>0.5): (2575,178)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2355, (25,50,75)pctl=(0.1113,0.2175,0.3506),(0.0<r>0.5): (2571,144)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2384, (25,50,75)pctl=(0.1123,0.2227,0.3606),(0.0<r>0.5): (2577,120)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2334, (25,50,75)pctl=(0.1154,0.2169,0.3383),(0.0<r>0.5): (2602,108)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2379, (25,50,75)pctl=(0.1112,0.2214,0.3525),(0.0<r>0.5): (2578,137)\n",
      "pop.cv.best:  1.000, mean=0.2386, (25,50,75)pctl=(0.1111,0.2219,0.3558),(0.0<r>0.5): (2624,117)\n",
      "87/100: temporal 1/1=1.000, features 87/100=(2154.4347, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.3057, (25,50,75)pctl=(0.1669,0.3053,0.4453),(0.0<r>0.5): (2604,409)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2660, (25,50,75)pctl=(0.1403,0.2584,0.3880),(0.0<r>0.5): (2599,198)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2705, (25,50,75)pctl=(0.1431,0.2624,0.3937),(0.0<r>0.5): (2599,218)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2657, (25,50,75)pctl=(0.1444,0.2605,0.3812),(0.0<r>0.5): (2598,187)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2784, (25,50,75)pctl=(0.1477,0.2739,0.4019),(0.0<r>0.5): (2608,255)\n",
      "pop.cv.best:  1.000, mean=0.2773, (25,50,75)pctl=(0.1490,0.2746,0.4013),(0.0<r>0.5): (2638,248)\n",
      "88/100: temporal 1/1=1.000, features 88/100=(2154.4347, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2807, (25,50,75)pctl=(0.1466,0.2749,0.4107),(0.0<r>0.5): (2587,288)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2317, (25,50,75)pctl=(0.1065,0.2080,0.3428),(0.0<r>0.5): (2575,155)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2410, (25,50,75)pctl=(0.1188,0.2256,0.3517),(0.0<r>0.5): (2572,163)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2450, (25,50,75)pctl=(0.1306,0.2398,0.3597),(0.0<r>0.5): (2575,123)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2462, (25,50,75)pctl=(0.1271,0.2366,0.3612),(0.0<r>0.5): (2589,141)\n",
      "pop.cv.best:  1.000, mean=0.2489, (25,50,75)pctl=(0.1281,0.2365,0.3600),(0.0<r>0.5): (2621,174)\n",
      "89/100: temporal 1/1=1.000, features 89/100=(2154.4347, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2731, (25,50,75)pctl=(0.1375,0.2654,0.3997),(0.0<r>0.5): (2575,283)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2244, (25,50,75)pctl=(0.0977,0.1964,0.3391),(0.0<r>0.5): (2560,151)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2342, (25,50,75)pctl=(0.1135,0.2175,0.3426),(0.0<r>0.5): (2565,161)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2382, (25,50,75)pctl=(0.1219,0.2308,0.3516),(0.0<r>0.5): (2570,123)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2381, (25,50,75)pctl=(0.1175,0.2280,0.3500),(0.0<r>0.5): (2574,136)\n",
      "pop.cv.best:  1.000, mean=0.2416, (25,50,75)pctl=(0.1217,0.2250,0.3490),(0.0<r>0.5): (2619,170)\n",
      "90/100: temporal 1/1=1.000, features 90/100=(2154.4347, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2727, (25,50,75)pctl=(0.1373,0.2649,0.3993),(0.0<r>0.5): (2575,283)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2240, (25,50,75)pctl=(0.0973,0.1960,0.3390),(0.0<r>0.5): (2560,151)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2339, (25,50,75)pctl=(0.1133,0.2169,0.3423),(0.0<r>0.5): (2564,161)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2378, (25,50,75)pctl=(0.1214,0.2300,0.3514),(0.0<r>0.5): (2571,122)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2377, (25,50,75)pctl=(0.1169,0.2274,0.3493),(0.0<r>0.5): (2570,134)\n",
      "pop.cv.best:  1.000, mean=0.2412, (25,50,75)pctl=(0.1211,0.2244,0.3485),(0.0<r>0.5): (2619,170)\n",
      "91/100: temporal 1/1=1.000, features 91/100=(10000.0000, 0.0100)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1000, (25,50,75)pctl=(0.0395,0.0841,0.1453),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0902, (25,50,75)pctl=(0.0333,0.0792,0.1327),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1073, (25,50,75)pctl=(0.0431,0.0945,0.1613),(0.0<r>0.5): (2422,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1096, (25,50,75)pctl=(0.0537,0.0954,0.1484),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0967, (25,50,75)pctl=(0.0356,0.0806,0.1413),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1007, (25,50,75)pctl=(0.0465,0.0845,0.1393),(0.0<r>0.5): (2581,000)\n",
      "92/100: temporal 1/1=1.000, features 92/100=(10000.0000, 0.0464)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1001, (25,50,75)pctl=(0.0396,0.0842,0.1453),(0.0<r>0.5): (2422,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0902, (25,50,75)pctl=(0.0334,0.0791,0.1327),(0.0<r>0.5): (2410,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1074, (25,50,75)pctl=(0.0431,0.0945,0.1614),(0.0<r>0.5): (2421,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1096, (25,50,75)pctl=(0.0538,0.0954,0.1484),(0.0<r>0.5): (2507,001)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0967, (25,50,75)pctl=(0.0356,0.0807,0.1414),(0.0<r>0.5): (2391,001)\n",
      "pop.cv.best:  1.000, mean=0.1008, (25,50,75)pctl=(0.0465,0.0846,0.1393),(0.0<r>0.5): (2581,000)\n",
      "93/100: temporal 1/1=1.000, features 93/100=(10000.0000, 0.2154)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1014, (25,50,75)pctl=(0.0403,0.0855,0.1467),(0.0<r>0.5): (2430,003)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0911, (25,50,75)pctl=(0.0337,0.0797,0.1340),(0.0<r>0.5): (2417,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1087, (25,50,75)pctl=(0.0436,0.0960,0.1632),(0.0<r>0.5): (2426,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1107, (25,50,75)pctl=(0.0545,0.0960,0.1499),(0.0<r>0.5): (2504,002)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0978, (25,50,75)pctl=(0.0360,0.0825,0.1432),(0.0<r>0.5): (2398,001)\n",
      "pop.cv.best:  1.000, mean=0.1020, (25,50,75)pctl=(0.0471,0.0857,0.1410),(0.0<r>0.5): (2582,000)\n",
      "94/100: temporal 1/1=1.000, features 94/100=(10000.0000, 1.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1135, (25,50,75)pctl=(0.0461,0.0949,0.1608),(0.0<r>0.5): (2471,004)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0993, (25,50,75)pctl=(0.0388,0.0869,0.1454),(0.0<r>0.5): (2429,000)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1207, (25,50,75)pctl=(0.0496,0.1065,0.1783),(0.0<r>0.5): (2466,000)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1214, (25,50,75)pctl=(0.0601,0.1056,0.1648),(0.0<r>0.5): (2524,003)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1094, (25,50,75)pctl=(0.0429,0.0914,0.1596),(0.0<r>0.5): (2446,001)\n",
      "pop.cv.best:  1.000, mean=0.1129, (25,50,75)pctl=(0.0522,0.0954,0.1553),(0.0<r>0.5): (2588,001)\n",
      "95/100: temporal 1/1=1.000, features 95/100=(10000.0000, 4.6416)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1395, (25,50,75)pctl=(0.0598,0.1176,0.1929),(0.0<r>0.5): (2498,036)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1320, (25,50,75)pctl=(0.0579,0.1169,0.1918),(0.0<r>0.5): (2503,008)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1494, (25,50,75)pctl=(0.0642,0.1318,0.2193),(0.0<r>0.5): (2499,010)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1513, (25,50,75)pctl=(0.0763,0.1344,0.2041),(0.0<r>0.5): (2560,018)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1463, (25,50,75)pctl=(0.0642,0.1237,0.2143),(0.0<r>0.5): (2517,020)\n",
      "pop.cv.best:  1.000, mean=0.1437, (25,50,75)pctl=(0.0671,0.1203,0.2022),(0.0<r>0.5): (2601,014)\n",
      "96/100: temporal 1/1=1.000, features 96/100=(10000.0000, 21.5443)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1839, (25,50,75)pctl=(0.0812,0.1590,0.2511),(0.0<r>0.5): (2542,113)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1857, (25,50,75)pctl=(0.0870,0.1649,0.2734),(0.0<r>0.5): (2537,042)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1852, (25,50,75)pctl=(0.0853,0.1697,0.2691),(0.0<r>0.5): (2539,033)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1837, (25,50,75)pctl=(0.0930,0.1641,0.2520),(0.0<r>0.5): (2575,061)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1898, (25,50,75)pctl=(0.0885,0.1685,0.2767),(0.0<r>0.5): (2555,072)\n",
      "pop.cv.best:  1.000, mean=0.1856, (25,50,75)pctl=(0.0879,0.1646,0.2652),(0.0<r>0.5): (2616,065)\n",
      "97/100: temporal 1/1=1.000, features 97/100=(10000.0000, 100.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2326, (25,50,75)pctl=(0.1175,0.2241,0.3369),(0.0<r>0.5): (2568,114)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2218, (25,50,75)pctl=(0.1058,0.2088,0.3283),(0.0<r>0.5): (2551,063)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2162, (25,50,75)pctl=(0.1075,0.2104,0.3171),(0.0<r>0.5): (2570,021)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2004, (25,50,75)pctl=(0.1025,0.1872,0.2860),(0.0<r>0.5): (2587,051)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2253, (25,50,75)pctl=(0.1139,0.2165,0.3237),(0.0<r>0.5): (2589,059)\n",
      "pop.cv.best:  1.000, mean=0.2193, (25,50,75)pctl=(0.1100,0.2093,0.3173),(0.0<r>0.5): (2620,045)\n",
      "98/100: temporal 1/1=1.000, features 98/100=(10000.0000, 464.1589)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2712, (25,50,75)pctl=(0.1458,0.2684,0.3996),(0.0<r>0.5): (2592,191)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2296, (25,50,75)pctl=(0.1166,0.2179,0.3318),(0.0<r>0.5): (2575,089)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2381, (25,50,75)pctl=(0.1223,0.2314,0.3504),(0.0<r>0.5): (2584,069)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2398, (25,50,75)pctl=(0.1303,0.2323,0.3432),(0.0<r>0.5): (2581,064)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2514, (25,50,75)pctl=(0.1337,0.2469,0.3630),(0.0<r>0.5): (2598,098)\n",
      "pop.cv.best:  1.000, mean=0.2460, (25,50,75)pctl=(0.1330,0.2422,0.3557),(0.0<r>0.5): (2628,064)\n",
      "99/100: temporal 1/1=1.000, features 99/100=(10000.0000, 2154.4347)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2368, (25,50,75)pctl=(0.1133,0.2249,0.3492),(0.0<r>0.5): (2566,128)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1872, (25,50,75)pctl=(0.0689,0.1581,0.2946),(0.0<r>0.5): (2492,066)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2051, (25,50,75)pctl=(0.0961,0.1854,0.3056),(0.0<r>0.5): (2527,054)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2099, (25,50,75)pctl=(0.1044,0.2025,0.3074),(0.0<r>0.5): (2556,030)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2134, (25,50,75)pctl=(0.1026,0.2036,0.3135),(0.0<r>0.5): (2552,046)\n",
      "pop.cv.best:  1.000, mean=0.2105, (25,50,75)pctl=(0.1029,0.1897,0.3007),(0.0<r>0.5): (2604,044)\n",
      "100/100: temporal 1/1=1.000, features 100/100=(10000.0000, 10000.0000)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2320, (25,50,75)pctl=(0.1093,0.2181,0.3436),(0.0<r>0.5): (2558,126)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1832, (25,50,75)pctl=(0.0635,0.1505,0.2941),(0.0<r>0.5): (2473,064)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2012, (25,50,75)pctl=(0.0901,0.1791,0.3026),(0.0<r>0.5): (2516,053)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2055, (25,50,75)pctl=(0.0998,0.1951,0.3014),(0.0<r>0.5): (2541,029)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2086, (25,50,75)pctl=(0.0980,0.1980,0.3057),(0.0<r>0.5): (2544,044)\n",
      "pop.cv.best:  1.000, mean=0.2061, (25,50,75)pctl=(0.0983,0.1822,0.2975),(0.0<r>0.5): (2598,042)\n",
      "Duration 43.7003[mins]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    1.000, mean=0.5405, (25,50,75)pctl=(0.4601,0.5866,0.6666),(0.0<r>0.5): (1144,774)\n",
      "1147 responses: ridge=    1.000, temporal=1.000, spatial=(464.159, 100.000) perf=0.5405\n",
      "lambda 01:    1.000, mean=-0.0022, (25,50,75)pctl=(-0.0374,-0.0069,0.0415),(0.0<r>0.5): (003,000)\n",
      "8 responses: ridge=    1.000, temporal=1.000, spatial=(21.544, 4.642) perf=-0.0022\n",
      "lambda 01:    1.000, mean=0.3032, (25,50,75)pctl=(0.2110,0.3191,0.3939),(0.0<r>0.5): (058,004)\n",
      "58 responses: ridge=    1.000, temporal=1.000, spatial=(2154.435, 464.159) perf=0.3032\n",
      "lambda 01:    1.000, mean=-0.0062, (25,50,75)pctl=(-0.0589,0.0456,0.0681),(0.0<r>0.5): (004,000)\n",
      "7 responses: ridge=    1.000, temporal=1.000, spatial=(464.159, 4.642) perf=-0.0062\n",
      "lambda 01:    1.000, mean=-0.0188, (25,50,75)pctl=(-0.0280,-0.0188,-0.0097),(0.0<r>0.5): (000,000)\n",
      "2 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 0.010) perf=-0.0188\n",
      "lambda 01:    1.000, mean=0.0361, (25,50,75)pctl=(0.0200,0.0653,0.0814),(0.0<r>0.5): (003,000)\n",
      "4 responses: ridge=    1.000, temporal=1.000, spatial=(0.215, 0.010) perf=0.0361\n",
      "lambda 01:    1.000, mean=0.4334, (25,50,75)pctl=(0.3095,0.4499,0.5722),(0.0<r>0.5): (099,045)\n",
      "101 responses: ridge=    1.000, temporal=1.000, spatial=(464.159, 464.159) perf=0.4334\n",
      "lambda 01:    1.000, mean=0.0302, (25,50,75)pctl=(-0.0005,0.0302,0.0609),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=    1.000, temporal=1.000, spatial=(1.000, 0.010) perf=0.0302\n",
      "lambda 01:    1.000, mean=0.3422, (25,50,75)pctl=(0.2530,0.3478,0.4519),(0.0<r>0.5): (333,056)\n",
      "339 responses: ridge=    1.000, temporal=1.000, spatial=(2154.435, 100.000) perf=0.3422\n",
      "lambda 01:    1.000, mean=0.0344, (25,50,75)pctl=(0.0344,0.0344,0.0344),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(4.642, 0.215) perf=0.0344\n",
      "lambda 01:    1.000, mean=0.0903, (25,50,75)pctl=(0.0770,0.1149,0.1282),(0.0<r>0.5): (003,000)\n",
      "4 responses: ridge=    1.000, temporal=1.000, spatial=(0.046, 0.010) perf=0.0903\n",
      "lambda 01:    1.000, mean=-0.0749, (25,50,75)pctl=(-0.0991,-0.0991,-0.0629),(0.0<r>0.5): (000,000)\n",
      "3 responses: ridge=    1.000, temporal=1.000, spatial=(0.215, 0.046) perf=-0.0749\n",
      "lambda 01:    1.000, mean=0.0054, (25,50,75)pctl=(-0.0325,-0.0099,0.0281),(0.0<r>0.5): (002,000)\n",
      "4 responses: ridge=    1.000, temporal=1.000, spatial=(21.544, 1.000) perf=0.0054\n",
      "lambda 01:    1.000, mean=0.1621, (25,50,75)pctl=(0.0105,0.0840,0.1968),(0.0<r>0.5): (014,003)\n",
      "17 responses: ridge=    1.000, temporal=1.000, spatial=(100.000, 21.544) perf=0.1621\n",
      "lambda 01:    1.000, mean=0.1685, (25,50,75)pctl=(0.1227,0.1816,0.2203),(0.0<r>0.5): (020,000)\n",
      "21 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 464.159) perf=0.1685\n",
      "lambda 01:    1.000, mean=0.0234, (25,50,75)pctl=(-0.0191,-0.0073,0.0505),(0.0<r>0.5): (001,000)\n",
      "3 responses: ridge=    1.000, temporal=1.000, spatial=(100.000, 464.159) perf=0.0234\n",
      "lambda 01:    1.000, mean=0.4955, (25,50,75)pctl=(0.4256,0.4572,0.5570),(0.0<r>0.5): (007,003)\n",
      "7 responses: ridge=    1.000, temporal=1.000, spatial=(464.159, 2154.435) perf=0.4955\n",
      "lambda 01:    1.000, mean=-0.0297, (25,50,75)pctl=(-0.1243,-0.0804,0.0396),(0.0<r>0.5): (001,000)\n",
      "3 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 2154.435) perf=-0.0297\n",
      "lambda 01:    1.000, mean=0.0150, (25,50,75)pctl=(-0.0210,0.0164,0.0737),(0.0<r>0.5): (008,000)\n",
      "14 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 4.642) perf=0.0150\n",
      "lambda 01:    1.000, mean=0.3179, (25,50,75)pctl=(0.1863,0.3502,0.4592),(0.0<r>0.5): (223,030)\n",
      "234 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 100.000) perf=0.3179\n",
      "lambda 01:    1.000, mean=0.4521, (25,50,75)pctl=(0.2817,0.5259,0.6286),(0.0<r>0.5): (282,155)\n",
      "288 responses: ridge=    1.000, temporal=1.000, spatial=(464.159, 21.544) perf=0.4521\n",
      "lambda 01:    1.000, mean=-0.1154, (25,50,75)pctl=(-0.1154,-0.1154,-0.1154),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(0.010, 0.215) perf=-0.1154\n",
      "lambda 01:    1.000, mean=-0.0056, (25,50,75)pctl=(-0.0056,-0.0056,-0.0056),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(0.010, 0.010) perf=-0.0056\n",
      "lambda 01:    1.000, mean=-0.0060, (25,50,75)pctl=(-0.0113,-0.0060,-0.0006),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 0.215) perf=-0.0060\n",
      "lambda 01:    1.000, mean=0.0852, (25,50,75)pctl=(0.0095,0.0560,0.1072),(0.0<r>0.5): (007,000)\n",
      "7 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 10000.000) perf=0.0852\n",
      "lambda 01:    1.000, mean=0.0568, (25,50,75)pctl=(0.0568,0.0568,0.0568),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(21.544, 0.215) perf=0.0568\n",
      "lambda 01:    1.000, mean=0.1156, (25,50,75)pctl=(0.0608,0.1265,0.1812),(0.0<r>0.5): (003,000)\n",
      "4 responses: ridge=    1.000, temporal=1.000, spatial=(0.010, 10000.000) perf=0.1156\n",
      "lambda 01:    1.000, mean=0.0334, (25,50,75)pctl=(-0.0314,-0.0254,0.0688),(0.0<r>0.5): (001,000)\n",
      "3 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 1.000) perf=0.0334\n",
      "lambda 01:    1.000, mean=0.3435, (25,50,75)pctl=(0.1823,0.3995,0.5429),(0.0<r>0.5): (040,013)\n",
      "44 responses: ridge=    1.000, temporal=1.000, spatial=(10000.000, 21.544) perf=0.3435\n",
      "lambda 01:    1.000, mean=-0.0785, (25,50,75)pctl=(-0.0785,-0.0785,-0.0785),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(21.544, 100.000) perf=-0.0785\n",
      "lambda 01:    1.000, mean=0.4908, (25,50,75)pctl=(0.4158,0.5552,0.6433),(0.0<r>0.5): (130,085)\n",
      "132 responses: ridge=    1.000, temporal=1.000, spatial=(2154.435, 21.544) perf=0.4908\n",
      "lambda 01:    1.000, mean=0.1300, (25,50,75)pctl=(0.0667,0.1074,0.2526),(0.0<r>0.5): (007,000)\n",
      "8 responses: ridge=    1.000, temporal=1.000, spatial=(100.000, 10000.000) perf=0.1300\n",
      "lambda 01:    1.000, mean=0.2561, (25,50,75)pctl=(0.1698,0.2602,0.3241),(0.0<r>0.5): (032,001)\n",
      "32 responses: ridge=    1.000, temporal=1.000, spatial=(2154.435, 10000.000) perf=0.2561\n",
      "lambda 01:    1.000, mean=0.0165, (25,50,75)pctl=(-0.0358,0.0105,0.0629),(0.0<r>0.5): (003,000)\n",
      "5 responses: ridge=    1.000, temporal=1.000, spatial=(100.000, 1.000) perf=0.0165\n",
      "lambda 01:    1.000, mean=0.2953, (25,50,75)pctl=(0.2288,0.3847,0.4066),(0.0<r>0.5): (003,000)\n",
      "3 responses: ridge=    1.000, temporal=1.000, spatial=(2154.435, 2154.435) perf=0.2953\n",
      "lambda 01:    1.000, mean=-0.0089, (25,50,75)pctl=(-0.0089,-0.0089,-0.0089),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(100.000, 0.215) perf=-0.0089\n",
      "lambda 01:    1.000, mean=0.0125, (25,50,75)pctl=(-0.0272,-0.0260,0.0137),(0.0<r>0.5): (001,000)\n",
      "4 responses: ridge=    1.000, temporal=1.000, spatial=(0.010, 0.046) perf=0.0125\n",
      "lambda 01:    1.000, mean=0.0229, (25,50,75)pctl=(0.0229,0.0229,0.0229),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(2154.435, 4.642) perf=0.0229\n",
      "lambda 01:    1.000, mean=0.4030, (25,50,75)pctl=(0.2470,0.4260,0.5694),(0.0<r>0.5): (114,043)\n",
      "115 responses: ridge=    1.000, temporal=1.000, spatial=(464.159, 10000.000) perf=0.4030\n",
      "lambda 01:    1.000, mean=0.0537, (25,50,75)pctl=(-0.0020,0.0404,0.1438),(0.0<r>0.5): (013,000)\n",
      "18 responses: ridge=    1.000, temporal=1.000, spatial=(100.000, 4.642) perf=0.0537\n",
      "Total duration 56.9378[mins]\n"
     ]
    }
   ],
   "source": [
    "ridges = np.logspace(-2,4,10)\n",
    "moten_prior = spatial_priors.SphericalPrior(nfeaturesm, hyparams=ridges)\n",
    "obcat_prior = spatial_priors.SphericalPrior(nfeatureso, hyparams=ridges)\n",
    "temporal_prior = temporal_priors.SphericalPrior(delays=delays)\n",
    "\n",
    "fit_banded_grid = models.estimate_stem_wmvnp([Mtrain, Otrain], Ytrain, \n",
    "                                        [Mtest, Otest],Ytest,\n",
    "                                        feature_priors=[moten_prior, obcat_prior],\n",
    "                                        temporal_prior=temporal_prior,\n",
    "                                        ridges=[1.0],\n",
    "                                        folds=(1,5),\n",
    "                                        performance=True,\n",
    "                                        verbosity=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,u'hyper-parameter search\\n(polar vs grid)')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3be680790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plot_model_2dhist_comparison(np.nan_to_num(fit_banded_polar['performance'].squeeze()),\n",
    "                                  np.nan_to_num(fit_banded_grid['performance'].squeeze()),\n",
    "                                  'banded ridge\\n(polar search)', \n",
    "                                  'banded ridge\\n(grid search)')\n",
    "ax.set_title('hyper-parameter search\\n(polar vs grid)', fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0175  0.1727  0.3365  0.5184  0.7319  1.      1.3663  1.9292  2.9714\n",
      "  5.7894 57.29  ]\n",
      "[0.0175 0.1702 0.319  0.4602 0.5906 0.7071 0.807  0.8878 0.9478 0.9854\n",
      " 0.9998]\n",
      "[0.9998 0.9854 0.9478 0.8878 0.807  0.7071 0.5906 0.4602 0.319  0.1702\n",
      " 0.0175]\n"
     ]
    }
   ],
   "source": [
    "offset = 1\n",
    "angle = np.linspace(0+offset, 90 - offset, 11)\n",
    "angle = np.deg2rad(angle)\n",
    "alpha1 = np.sin(angle)\n",
    "alpha2 = np.cos(angle)\n",
    "alphas = zip(alpha1, alpha2)\n",
    "ratios = alpha1/alpha2\n",
    "print(ratios)\n",
    "print(alpha1)\n",
    "print(alpha2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridges = ratios\n",
    "moten_prior = spatial_priors.SphericalPrior(nfeaturesm, hyparams=[1.0])\n",
    "obcat_prior = spatial_priors.SphericalPrior(nfeatureso, hyparams=ridges)\n",
    "temporal_prior = temporal_priors.HRFPrior(delays=delays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/11: temporal 1/1=1.000, features 1/11=(0.9998, 0.0175)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0709, (25,50,75)pctl=(0.0155,0.0597,0.1150),(0.0<r>0.5): (2178,000)\n",
      "lambda 02:    2.783, mean=0.0748, (25,50,75)pctl=(0.0173,0.0625,0.1201),(0.0<r>0.5): (2197,000)\n",
      "lambda 03:    7.743, mean=0.0936, (25,50,75)pctl=(0.0279,0.0775,0.1463),(0.0<r>0.5): (2301,000)\n",
      "lambda 04:   21.544, mean=0.1420, (25,50,75)pctl=(0.0523,0.1198,0.2192),(0.0<r>0.5): (2443,003)\n",
      "lambda 05:   59.948, mean=0.2056, (25,50,75)pctl=(0.0891,0.1790,0.3126),(0.0<r>0.5): (2550,057)\n",
      "lambda 06:  166.810, mean=0.2520, (25,50,75)pctl=(0.1214,0.2331,0.3767),(0.0<r>0.5): (2603,199)\n",
      "lambda 07:  464.159, mean=0.2778, (25,50,75)pctl=(0.1401,0.2598,0.4142),(0.0<r>0.5): (2611,317)\n",
      "lambda 08: 1291.550, mean=0.2853, (25,50,75)pctl=(0.1453,0.2710,0.4202),(0.0<r>0.5): (2608,330)\n",
      "lambda 09: 3593.814, mean=0.2686, (25,50,75)pctl=(0.1406,0.2591,0.3914),(0.0<r>0.5): (2601,215)\n",
      "lambda 10: 10000.000, mean=0.2456, (25,50,75)pctl=(0.1316,0.2332,0.3587),(0.0<r>0.5): (2591,094)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0743, (25,50,75)pctl=(0.0177,0.0660,0.1233),(0.0<r>0.5): (2207,000)\n",
      "lambda 02:    2.783, mean=0.0791, (25,50,75)pctl=(0.0201,0.0687,0.1307),(0.0<r>0.5): (2242,000)\n",
      "lambda 03:    7.743, mean=0.1004, (25,50,75)pctl=(0.0301,0.0827,0.1585),(0.0<r>0.5): (2347,000)\n",
      "lambda 04:   21.544, mean=0.1546, (25,50,75)pctl=(0.0598,0.1337,0.2385),(0.0<r>0.5): (2492,013)\n",
      "lambda 05:   59.948, mean=0.2205, (25,50,75)pctl=(0.1018,0.1979,0.3299),(0.0<r>0.5): (2562,114)\n",
      "lambda 06:  166.810, mean=0.2634, (25,50,75)pctl=(0.1321,0.2441,0.3906),(0.0<r>0.5): (2598,248)\n",
      "lambda 07:  464.159, mean=0.2874, (25,50,75)pctl=(0.1448,0.2746,0.4253),(0.0<r>0.5): (2603,361)\n",
      "lambda 08: 1291.550, mean=0.2958, (25,50,75)pctl=(0.1477,0.2878,0.4382),(0.0<r>0.5): (2596,394)\n",
      "lambda 09: 3593.814, mean=0.2823, (25,50,75)pctl=(0.1434,0.2810,0.4123),(0.0<r>0.5): (2590,315)\n",
      "lambda 10: 10000.000, mean=0.2617, (25,50,75)pctl=(0.1330,0.2601,0.3826),(0.0<r>0.5): (2564,202)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0713, (25,50,75)pctl=(0.0141,0.0550,0.1201),(0.0<r>0.5): (2151,000)\n",
      "lambda 02:    2.783, mean=0.0759, (25,50,75)pctl=(0.0155,0.0589,0.1268),(0.0<r>0.5): (2184,000)\n",
      "lambda 03:    7.743, mean=0.0959, (25,50,75)pctl=(0.0262,0.0759,0.1511),(0.0<r>0.5): (2303,001)\n",
      "lambda 04:   21.544, mean=0.1434, (25,50,75)pctl=(0.0505,0.1180,0.2184),(0.0<r>0.5): (2431,011)\n",
      "lambda 05:   59.948, mean=0.2010, (25,50,75)pctl=(0.0873,0.1768,0.2985),(0.0<r>0.5): (2548,082)\n",
      "lambda 06:  166.810, mean=0.2410, (25,50,75)pctl=(0.1147,0.2194,0.3512),(0.0<r>0.5): (2599,184)\n",
      "lambda 07:  464.159, mean=0.2648, (25,50,75)pctl=(0.1290,0.2474,0.3901),(0.0<r>0.5): (2605,273)\n",
      "lambda 08: 1291.550, mean=0.2701, (25,50,75)pctl=(0.1389,0.2612,0.3939),(0.0<r>0.5): (2600,256)\n",
      "lambda 09: 3593.814, mean=0.2552, (25,50,75)pctl=(0.1342,0.2493,0.3681),(0.0<r>0.5): (2593,155)\n",
      "lambda 10: 10000.000, mean=0.2340, (25,50,75)pctl=(0.1208,0.2268,0.3398),(0.0<r>0.5): (2571,081)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0823, (25,50,75)pctl=(0.0258,0.0750,0.1275),(0.0<r>0.5): (2276,000)\n",
      "lambda 02:    2.783, mean=0.0864, (25,50,75)pctl=(0.0274,0.0778,0.1322),(0.0<r>0.5): (2303,000)\n",
      "lambda 03:    7.743, mean=0.1056, (25,50,75)pctl=(0.0386,0.0943,0.1610),(0.0<r>0.5): (2383,001)\n",
      "lambda 04:   21.544, mean=0.1537, (25,50,75)pctl=(0.0634,0.1327,0.2301),(0.0<r>0.5): (2484,006)\n",
      "lambda 05:   59.948, mean=0.2133, (25,50,75)pctl=(0.1011,0.1913,0.3153),(0.0<r>0.5): (2577,066)\n",
      "lambda 06:  166.810, mean=0.2556, (25,50,75)pctl=(0.1325,0.2365,0.3733),(0.0<r>0.5): (2609,189)\n",
      "lambda 07:  464.159, mean=0.2793, (25,50,75)pctl=(0.1460,0.2642,0.4081),(0.0<r>0.5): (2615,309)\n",
      "lambda 08: 1291.550, mean=0.2755, (25,50,75)pctl=(0.1441,0.2642,0.4024),(0.0<r>0.5): (2608,261)\n",
      "lambda 09: 3593.814, mean=0.2472, (25,50,75)pctl=(0.1233,0.2409,0.3611),(0.0<r>0.5): (2582,115)\n",
      "lambda 10: 10000.000, mean=0.2225, (25,50,75)pctl=(0.1069,0.2167,0.3272),(0.0<r>0.5): (2570,050)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.0805, (25,50,75)pctl=(0.0191,0.0660,0.1301),(0.0<r>0.5): (2207,000)\n",
      "lambda 02:    2.783, mean=0.0845, (25,50,75)pctl=(0.0209,0.0710,0.1350),(0.0<r>0.5): (2224,000)\n",
      "lambda 03:    7.743, mean=0.1040, (25,50,75)pctl=(0.0299,0.0875,0.1607),(0.0<r>0.5): (2318,000)\n",
      "lambda 04:   21.544, mean=0.1541, (25,50,75)pctl=(0.0571,0.1278,0.2324),(0.0<r>0.5): (2466,020)\n",
      "lambda 05:   59.948, mean=0.2181, (25,50,75)pctl=(0.1010,0.1910,0.3286),(0.0<r>0.5): (2571,124)\n",
      "lambda 06:  166.810, mean=0.2597, (25,50,75)pctl=(0.1279,0.2396,0.3823),(0.0<r>0.5): (2614,236)\n",
      "lambda 07:  464.159, mean=0.2854, (25,50,75)pctl=(0.1457,0.2676,0.4190),(0.0<r>0.5): (2610,359)\n",
      "lambda 08: 1291.550, mean=0.2964, (25,50,75)pctl=(0.1559,0.2861,0.4326),(0.0<r>0.5): (2603,387)\n",
      "lambda 09: 3593.814, mean=0.2836, (25,50,75)pctl=(0.1536,0.2790,0.4127),(0.0<r>0.5): (2589,293)\n",
      "lambda 10: 10000.000, mean=0.2652, (25,50,75)pctl=(0.1447,0.2603,0.3840),(0.0<r>0.5): (2578,186)\n",
      "pop.cv.best: 1291.550, mean=0.1948, (25,50,75)pctl=(0.1550,0.2786,0.4160),(0.0<r>0.5): (2647,323)\n",
      "2/11: temporal 1/1=1.000, features 2/11=(0.9854, 0.1702)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1148, (25,50,75)pctl=(0.0387,0.0921,0.1767),(0.0<r>0.5): (2370,000)\n",
      "lambda 02:    2.783, mean=0.1251, (25,50,75)pctl=(0.0427,0.1006,0.1907),(0.0<r>0.5): (2406,003)\n",
      "lambda 03:    7.743, mean=0.1637, (25,50,75)pctl=(0.0594,0.1361,0.2505),(0.0<r>0.5): (2494,031)\n",
      "lambda 04:   21.544, mean=0.2288, (25,50,75)pctl=(0.0979,0.2003,0.3497),(0.0<r>0.5): (2569,172)\n",
      "lambda 05:   59.948, mean=0.2764, (25,50,75)pctl=(0.1294,0.2581,0.4197),(0.0<r>0.5): (2609,352)\n",
      "lambda 06:  166.810, mean=0.3021, (25,50,75)pctl=(0.1470,0.2902,0.4511),(0.0<r>0.5): (2612,467)\n",
      "lambda 07:  464.159, mean=0.3144, (25,50,75)pctl=(0.1674,0.3093,0.4641),(0.0<r>0.5): (2622,483)\n",
      "lambda 08: 1291.550, mean=0.2987, (25,50,75)pctl=(0.1603,0.2911,0.4336),(0.0<r>0.5): (2623,359)\n",
      "lambda 09: 3593.814, mean=0.2594, (25,50,75)pctl=(0.1364,0.2484,0.3772),(0.0<r>0.5): (2598,180)\n",
      "lambda 10: 10000.000, mean=0.2346, (25,50,75)pctl=(0.1174,0.2224,0.3421),(0.0<r>0.5): (2569,102)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1119, (25,50,75)pctl=(0.0365,0.0948,0.1725),(0.0<r>0.5): (2352,000)\n",
      "lambda 02:    2.783, mean=0.1225, (25,50,75)pctl=(0.0430,0.1040,0.1885),(0.0<r>0.5): (2387,002)\n",
      "lambda 03:    7.743, mean=0.1623, (25,50,75)pctl=(0.0659,0.1391,0.2468),(0.0<r>0.5): (2457,027)\n",
      "lambda 04:   21.544, mean=0.2299, (25,50,75)pctl=(0.1046,0.2065,0.3464),(0.0<r>0.5): (2554,148)\n",
      "lambda 05:   59.948, mean=0.2813, (25,50,75)pctl=(0.1415,0.2639,0.4163),(0.0<r>0.5): (2602,334)\n",
      "lambda 06:  166.810, mean=0.3129, (25,50,75)pctl=(0.1620,0.3030,0.4553),(0.0<r>0.5): (2614,496)\n",
      "lambda 07:  464.159, mean=0.3255, (25,50,75)pctl=(0.1758,0.3228,0.4691),(0.0<r>0.5): (2622,535)\n",
      "lambda 08: 1291.550, mean=0.3054, (25,50,75)pctl=(0.1652,0.3029,0.4376),(0.0<r>0.5): (2613,403)\n",
      "lambda 09: 3593.814, mean=0.2591, (25,50,75)pctl=(0.1369,0.2482,0.3685),(0.0<r>0.5): (2589,210)\n",
      "lambda 10: 10000.000, mean=0.2267, (25,50,75)pctl=(0.1112,0.2049,0.3250),(0.0<r>0.5): (2553,149)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1100, (25,50,75)pctl=(0.0312,0.0890,0.1709),(0.0<r>0.5): (2323,002)\n",
      "lambda 02:    2.783, mean=0.1208, (25,50,75)pctl=(0.0368,0.0982,0.1863),(0.0<r>0.5): (2362,004)\n",
      "lambda 03:    7.743, mean=0.1598, (25,50,75)pctl=(0.0563,0.1320,0.2453),(0.0<r>0.5): (2453,044)\n",
      "lambda 04:   21.544, mean=0.2230, (25,50,75)pctl=(0.0943,0.1934,0.3393),(0.0<r>0.5): (2553,165)\n",
      "lambda 05:   59.948, mean=0.2695, (25,50,75)pctl=(0.1271,0.2436,0.4006),(0.0<r>0.5): (2609,319)\n",
      "lambda 06:  166.810, mean=0.2951, (25,50,75)pctl=(0.1462,0.2791,0.4362),(0.0<r>0.5): (2611,420)\n",
      "lambda 07:  464.159, mean=0.3052, (25,50,75)pctl=(0.1662,0.2975,0.4423),(0.0<r>0.5): (2612,422)\n",
      "lambda 08: 1291.550, mean=0.2814, (25,50,75)pctl=(0.1580,0.2744,0.4018),(0.0<r>0.5): (2605,255)\n",
      "lambda 09: 3593.814, mean=0.2313, (25,50,75)pctl=(0.1229,0.2199,0.3284),(0.0<r>0.5): (2583,105)\n",
      "lambda 10: 10000.000, mean=0.1986, (25,50,75)pctl=(0.0953,0.1807,0.2857),(0.0<r>0.5): (2539,048)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    1.000, mean=0.1264, (25,50,75)pctl=(0.0518,0.1063,0.1918),(0.0<r>0.5): (2457,001)\n",
      "lambda 02:    2.783, mean=0.1366, (25,50,75)pctl=(0.0566,0.1160,0.2066),(0.0<r>0.5): (2475,004)\n",
      "lambda 03:    7.743, mean=0.1735, (25,50,75)pctl=(0.0744,0.1492,0.2569),(0.0<r>0.5): (2534,026)\n",
      "lambda 04:   21.544, mean=0.2341, (25,50,75)pctl=(0.1130,0.2133,0.3435),(0.0<r>0.5): (2582,149)\n",
      "lambda 05:   59.948, mean=0.2777, (25,50,75)pctl=(0.1468,0.2591,0.4036),(0.0<r>0.5): (2612,313)\n",
      "lambda 06:  166.810, mean=0.3003, (25,50,75)pctl=(0.1647,0.2844,0.4363),(0.0<r>0.5): (2623,407)\n",
      "lambda 07:  464.159, mean=0.3039, (25,50,75)pctl=(0.1719,0.2941,0.4366),(0.0<r>0.5): (2619,416)\n",
      "lambda 08: 1291.550, mean=0.2733, (25,50,75)pctl=(0.1529,0.2647,0.3945),(0.0<r>0.5): (2599,221)\n",
      "lambda 09: 3593.814, mean=0.2200, (25,50,75)pctl=(0.1172,0.2099,0.3202),(0.0<r>0.5): (2561,047)\n",
      "lambda 10: 10000.000, mean=0.1893, (25,50,75)pctl=(0.0917,0.1761,0.2763),(0.0<r>0.5): (2519,014)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1247, (25,50,75)pctl=(0.0398,0.1008,0.1927),(0.0<r>0.5): (2395,008)\n",
      "lambda 02:    2.783, mean=0.1350, (25,50,75)pctl=(0.0445,0.1090,0.2095),(0.0<r>0.5): (2421,012)\n",
      "lambda 03:    7.743, mean=0.1735, (25,50,75)pctl=(0.0632,0.1426,0.2662),(0.0<r>0.5): (2484,048)\n",
      "lambda 04:   21.544, mean=0.2380, (25,50,75)pctl=(0.1046,0.2117,0.3600),(0.0<r>0.5): (2563,206)\n",
      "lambda 05:   59.948, mean=0.2855, (25,50,75)pctl=(0.1396,0.2678,0.4190),(0.0<r>0.5): (2614,364)\n",
      "lambda 06:  166.810, mean=0.3122, (25,50,75)pctl=(0.1610,0.3025,0.4531),(0.0<r>0.5): (2615,485)\n",
      "lambda 07:  464.159, mean=0.3224, (25,50,75)pctl=(0.1759,0.3221,0.4616),(0.0<r>0.5): (2613,505)\n",
      "lambda 08: 1291.550, mean=0.3056, (25,50,75)pctl=(0.1741,0.3025,0.4350),(0.0<r>0.5): (2606,375)\n",
      "lambda 09: 3593.814, mean=0.2673, (25,50,75)pctl=(0.1521,0.2643,0.3837),(0.0<r>0.5): (2581,162)\n",
      "lambda 10: 10000.000, mean=0.2399, (25,50,75)pctl=(0.1309,0.2343,0.3480),(0.0<r>0.5): (2562,096)\n",
      "pop.cv.best: 464.159, mean=0.2298, (25,50,75)pctl=(0.1765,0.3076,0.4568),(0.0<r>0.5): (2648,466)\n",
      "3/11: temporal 1/1=1.000, features 3/11=(0.9478, 0.3190)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1137, (25,50,75)pctl=(0.0373,0.0907,0.1732),(0.0<r>0.5): (2376,002)\n",
      "lambda 02:    2.783, mean=0.1241, (25,50,75)pctl=(0.0411,0.0999,0.1875),(0.0<r>0.5): (2407,003)\n",
      "lambda 03:    7.743, mean=0.1626, (25,50,75)pctl=(0.0592,0.1342,0.2474),(0.0<r>0.5): (2481,035)\n",
      "lambda 04:   21.544, mean=0.2264, (25,50,75)pctl=(0.0964,0.1961,0.3485),(0.0<r>0.5): (2560,179)\n",
      "lambda 05:   59.948, mean=0.2728, (25,50,75)pctl=(0.1250,0.2535,0.4137),(0.0<r>0.5): (2603,347)\n",
      "lambda 06:  166.810, mean=0.2987, (25,50,75)pctl=(0.1423,0.2882,0.4475),(0.0<r>0.5): (2609,463)\n",
      "lambda 07:  464.159, mean=0.3100, (25,50,75)pctl=(0.1629,0.3062,0.4543),(0.0<r>0.5): (2623,462)\n",
      "lambda 08: 1291.550, mean=0.2930, (25,50,75)pctl=(0.1561,0.2852,0.4248),(0.0<r>0.5): (2619,335)\n",
      "lambda 09: 3593.814, mean=0.2544, (25,50,75)pctl=(0.1323,0.2454,0.3691),(0.0<r>0.5): (2591,180)\n",
      "lambda 10: 10000.000, mean=0.2300, (25,50,75)pctl=(0.1150,0.2160,0.3344),(0.0<r>0.5): (2566,102)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1117, (25,50,75)pctl=(0.0370,0.0948,0.1719),(0.0<r>0.5): (2366,000)\n",
      "lambda 02:    2.783, mean=0.1224, (25,50,75)pctl=(0.0422,0.1047,0.1866),(0.0<r>0.5): (2387,003)\n",
      "lambda 03:    7.743, mean=0.1616, (25,50,75)pctl=(0.0640,0.1407,0.2466),(0.0<r>0.5): (2457,028)\n",
      "lambda 04:   21.544, mean=0.2273, (25,50,75)pctl=(0.1035,0.2051,0.3439),(0.0<r>0.5): (2550,143)\n",
      "lambda 05:   59.948, mean=0.2772, (25,50,75)pctl=(0.1386,0.2603,0.4069),(0.0<r>0.5): (2599,312)\n",
      "lambda 06:  166.810, mean=0.3087, (25,50,75)pctl=(0.1577,0.2982,0.4495),(0.0<r>0.5): (2614,482)\n",
      "lambda 07:  464.159, mean=0.3187, (25,50,75)pctl=(0.1706,0.3136,0.4580),(0.0<r>0.5): (2619,490)\n",
      "lambda 08: 1291.550, mean=0.2966, (25,50,75)pctl=(0.1566,0.2864,0.4248),(0.0<r>0.5): (2604,368)\n",
      "lambda 09: 3593.814, mean=0.2523, (25,50,75)pctl=(0.1296,0.2359,0.3600),(0.0<r>0.5): (2587,210)\n",
      "lambda 10: 10000.000, mean=0.2211, (25,50,75)pctl=(0.1028,0.1963,0.3237),(0.0<r>0.5): (2548,149)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1107, (25,50,75)pctl=(0.0309,0.0895,0.1716),(0.0<r>0.5): (2324,002)\n",
      "lambda 02:    2.783, mean=0.1215, (25,50,75)pctl=(0.0377,0.0981,0.1887),(0.0<r>0.5): (2365,003)\n",
      "lambda 03:    7.743, mean=0.1606, (25,50,75)pctl=(0.0558,0.1304,0.2492),(0.0<r>0.5): (2459,051)\n",
      "lambda 04:   21.544, mean=0.2228, (25,50,75)pctl=(0.0924,0.1923,0.3394),(0.0<r>0.5): (2548,174)\n",
      "lambda 05:   59.948, mean=0.2678, (25,50,75)pctl=(0.1252,0.2407,0.4008),(0.0<r>0.5): (2596,312)\n",
      "lambda 06:  166.810, mean=0.2927, (25,50,75)pctl=(0.1428,0.2760,0.4343),(0.0<r>0.5): (2607,412)\n",
      "lambda 07:  464.159, mean=0.2997, (25,50,75)pctl=(0.1585,0.2915,0.4376),(0.0<r>0.5): (2608,390)\n",
      "lambda 08: 1291.550, mean=0.2739, (25,50,75)pctl=(0.1493,0.2656,0.3919),(0.0<r>0.5): (2598,242)\n",
      "lambda 09: 3593.814, mean=0.2257, (25,50,75)pctl=(0.1174,0.2128,0.3189),(0.0<r>0.5): (2572,106)\n",
      "lambda 10: 10000.000, mean=0.1939, (25,50,75)pctl=(0.0889,0.1740,0.2833),(0.0<r>0.5): (2518,048)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1245, (25,50,75)pctl=(0.0498,0.1036,0.1887),(0.0<r>0.5): (2465,000)\n",
      "lambda 02:    2.783, mean=0.1348, (25,50,75)pctl=(0.0544,0.1132,0.2031),(0.0<r>0.5): (2476,002)\n",
      "lambda 03:    7.743, mean=0.1715, (25,50,75)pctl=(0.0720,0.1481,0.2542),(0.0<r>0.5): (2527,019)\n",
      "lambda 04:   21.544, mean=0.2309, (25,50,75)pctl=(0.1109,0.2099,0.3392),(0.0<r>0.5): (2573,139)\n",
      "lambda 05:   59.948, mean=0.2727, (25,50,75)pctl=(0.1429,0.2529,0.3959),(0.0<r>0.5): (2608,294)\n",
      "lambda 06:  166.810, mean=0.2948, (25,50,75)pctl=(0.1605,0.2768,0.4303),(0.0<r>0.5): (2622,386)\n",
      "lambda 07:  464.159, mean=0.2984, (25,50,75)pctl=(0.1676,0.2867,0.4298),(0.0<r>0.5): (2618,394)\n",
      "lambda 08: 1291.550, mean=0.2667, (25,50,75)pctl=(0.1480,0.2560,0.3867),(0.0<r>0.5): (2593,202)\n",
      "lambda 09: 3593.814, mean=0.2147, (25,50,75)pctl=(0.1118,0.2019,0.3117),(0.0<r>0.5): (2551,047)\n",
      "lambda 10: 10000.000, mean=0.1846, (25,50,75)pctl=(0.0871,0.1689,0.2715),(0.0<r>0.5): (2508,014)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1280, (25,50,75)pctl=(0.0420,0.1037,0.1975),(0.0<r>0.5): (2410,010)\n",
      "lambda 02:    2.783, mean=0.1383, (25,50,75)pctl=(0.0464,0.1115,0.2131),(0.0<r>0.5): (2431,017)\n",
      "lambda 03:    7.743, mean=0.1765, (25,50,75)pctl=(0.0632,0.1467,0.2709),(0.0<r>0.5): (2489,057)\n",
      "lambda 04:   21.544, mean=0.2388, (25,50,75)pctl=(0.1047,0.2106,0.3631),(0.0<r>0.5): (2560,211)\n",
      "lambda 05:   59.948, mean=0.2835, (25,50,75)pctl=(0.1367,0.2654,0.4159),(0.0<r>0.5): (2609,358)\n",
      "lambda 06:  166.810, mean=0.3086, (25,50,75)pctl=(0.1555,0.3014,0.4486),(0.0<r>0.5): (2612,473)\n",
      "lambda 07:  464.159, mean=0.3161, (25,50,75)pctl=(0.1733,0.3145,0.4533),(0.0<r>0.5): (2614,473)\n",
      "lambda 08: 1291.550, mean=0.2985, (25,50,75)pctl=(0.1680,0.2964,0.4257),(0.0<r>0.5): (2599,332)\n",
      "lambda 09: 3593.814, mean=0.2621, (25,50,75)pctl=(0.1473,0.2587,0.3762),(0.0<r>0.5): (2579,153)\n",
      "lambda 10: 10000.000, mean=0.2351, (25,50,75)pctl=(0.1265,0.2295,0.3411),(0.0<r>0.5): (2562,094)\n",
      "pop.cv.best: 464.159, mean=0.2266, (25,50,75)pctl=(0.1710,0.3031,0.4491),(0.0<r>0.5): (2649,441)\n",
      "4/11: temporal 1/1=1.000, features 4/11=(0.8878, 0.4602)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1119, (25,50,75)pctl=(0.0362,0.0896,0.1686),(0.0<r>0.5): (2373,001)\n",
      "lambda 02:    2.783, mean=0.1214, (25,50,75)pctl=(0.0396,0.0974,0.1837),(0.0<r>0.5): (2399,003)\n",
      "lambda 03:    7.743, mean=0.1577, (25,50,75)pctl=(0.0562,0.1293,0.2403),(0.0<r>0.5): (2476,031)\n",
      "lambda 04:   21.544, mean=0.2202, (25,50,75)pctl=(0.0928,0.1886,0.3383),(0.0<r>0.5): (2558,164)\n",
      "lambda 05:   59.948, mean=0.2674, (25,50,75)pctl=(0.1208,0.2465,0.4050),(0.0<r>0.5): (2601,330)\n",
      "lambda 06:  166.810, mean=0.2942, (25,50,75)pctl=(0.1388,0.2812,0.4412),(0.0<r>0.5): (2605,436)\n",
      "lambda 07:  464.159, mean=0.3065, (25,50,75)pctl=(0.1595,0.3010,0.4513),(0.0<r>0.5): (2619,452)\n",
      "lambda 08: 1291.550, mean=0.2921, (25,50,75)pctl=(0.1547,0.2840,0.4243),(0.0<r>0.5): (2617,332)\n",
      "lambda 09: 3593.814, mean=0.2550, (25,50,75)pctl=(0.1318,0.2469,0.3694),(0.0<r>0.5): (2592,187)\n",
      "lambda 10: 10000.000, mean=0.2296, (25,50,75)pctl=(0.1145,0.2163,0.3336),(0.0<r>0.5): (2566,103)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    1.000, mean=0.1105, (25,50,75)pctl=(0.0360,0.0954,0.1692),(0.0<r>0.5): (2365,000)\n",
      "lambda 02:    2.783, mean=0.1203, (25,50,75)pctl=(0.0412,0.1037,0.1824),(0.0<r>0.5): (2377,002)\n",
      "lambda 03:    7.743, mean=0.1572, (25,50,75)pctl=(0.0609,0.1369,0.2375),(0.0<r>0.5): (2450,022)\n",
      "lambda 04:   21.544, mean=0.2213, (25,50,75)pctl=(0.1016,0.1993,0.3348),(0.0<r>0.5): (2544,128)\n",
      "lambda 05:   59.948, mean=0.2716, (25,50,75)pctl=(0.1344,0.2530,0.3987),(0.0<r>0.5): (2594,291)\n",
      "lambda 06:  166.810, mean=0.3033, (25,50,75)pctl=(0.1541,0.2905,0.4407),(0.0<r>0.5): (2612,441)\n",
      "lambda 07:  464.159, mean=0.3139, (25,50,75)pctl=(0.1641,0.3067,0.4516),(0.0<r>0.5): (2616,470)\n",
      "lambda 08: 1291.550, mean=0.2949, (25,50,75)pctl=(0.1560,0.2829,0.4228),(0.0<r>0.5): (2602,371)\n",
      "lambda 09: 3593.814, mean=0.2529, (25,50,75)pctl=(0.1284,0.2361,0.3618),(0.0<r>0.5): (2588,216)\n",
      "lambda 10: 10000.000, mean=0.2208, (25,50,75)pctl=(0.1021,0.1954,0.3241),(0.0<r>0.5): (2547,152)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1099, (25,50,75)pctl=(0.0304,0.0883,0.1698),(0.0<r>0.5): (2321,001)\n",
      "lambda 02:    2.783, mean=0.1198, (25,50,75)pctl=(0.0358,0.0959,0.1850),(0.0<r>0.5): (2350,004)\n",
      "lambda 03:    7.743, mean=0.1569, (25,50,75)pctl=(0.0536,0.1257,0.2428),(0.0<r>0.5): (2453,048)\n",
      "lambda 04:   21.544, mean=0.2181, (25,50,75)pctl=(0.0891,0.1867,0.3315),(0.0<r>0.5): (2540,161)\n",
      "lambda 05:   59.948, mean=0.2634, (25,50,75)pctl=(0.1211,0.2360,0.3934),(0.0<r>0.5): (2595,299)\n",
      "lambda 06:  166.810, mean=0.2883, (25,50,75)pctl=(0.1382,0.2704,0.4315),(0.0<r>0.5): (2602,395)\n",
      "lambda 07:  464.159, mean=0.2957, (25,50,75)pctl=(0.1540,0.2870,0.4287),(0.0<r>0.5): (2606,381)\n",
      "lambda 08: 1291.550, mean=0.2731, (25,50,75)pctl=(0.1467,0.2636,0.3916),(0.0<r>0.5): (2597,251)\n",
      "lambda 09: 3593.814, mean=0.2268, (25,50,75)pctl=(0.1163,0.2129,0.3214),(0.0<r>0.5): (2574,111)\n",
      "lambda 10: 10000.000, mean=0.1937, (25,50,75)pctl=(0.0885,0.1734,0.2840),(0.0<r>0.5): (2516,049)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1217, (25,50,75)pctl=(0.0476,0.1018,0.1840),(0.0<r>0.5): (2457,000)\n",
      "lambda 02:    2.783, mean=0.1312, (25,50,75)pctl=(0.0523,0.1102,0.1978),(0.0<r>0.5): (2469,002)\n",
      "lambda 03:    7.743, mean=0.1660, (25,50,75)pctl=(0.0692,0.1431,0.2465),(0.0<r>0.5): (2519,017)\n",
      "lambda 04:   21.544, mean=0.2244, (25,50,75)pctl=(0.1070,0.2025,0.3287),(0.0<r>0.5): (2562,124)\n",
      "lambda 05:   59.948, mean=0.2669, (25,50,75)pctl=(0.1395,0.2465,0.3884),(0.0<r>0.5): (2604,270)\n",
      "lambda 06:  166.810, mean=0.2899, (25,50,75)pctl=(0.1568,0.2700,0.4234),(0.0<r>0.5): (2618,374)\n",
      "lambda 07:  464.159, mean=0.2950, (25,50,75)pctl=(0.1635,0.2818,0.4256),(0.0<r>0.5): (2617,385)\n",
      "lambda 08: 1291.550, mean=0.2666, (25,50,75)pctl=(0.1476,0.2548,0.3856),(0.0<r>0.5): (2592,208)\n",
      "lambda 09: 3593.814, mean=0.2158, (25,50,75)pctl=(0.1123,0.2024,0.3129),(0.0<r>0.5): (2548,052)\n",
      "lambda 10: 10000.000, mean=0.1844, (25,50,75)pctl=(0.0868,0.1684,0.2717),(0.0<r>0.5): (2508,014)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1281, (25,50,75)pctl=(0.0420,0.1031,0.1972),(0.0<r>0.5): (2397,009)\n",
      "lambda 02:    2.783, mean=0.1375, (25,50,75)pctl=(0.0458,0.1115,0.2124),(0.0<r>0.5): (2422,016)\n",
      "lambda 03:    7.743, mean=0.1735, (25,50,75)pctl=(0.0619,0.1446,0.2674),(0.0<r>0.5): (2481,054)\n",
      "lambda 04:   21.544, mean=0.2342, (25,50,75)pctl=(0.1007,0.2054,0.3566),(0.0<r>0.5): (2555,202)\n",
      "lambda 05:   59.948, mean=0.2789, (25,50,75)pctl=(0.1333,0.2589,0.4103),(0.0<r>0.5): (2604,345)\n",
      "lambda 06:  166.810, mean=0.3038, (25,50,75)pctl=(0.1515,0.2936,0.4437),(0.0<r>0.5): (2615,448)\n",
      "lambda 07:  464.159, mean=0.3118, (25,50,75)pctl=(0.1689,0.3087,0.4483),(0.0<r>0.5): (2613,455)\n",
      "lambda 08: 1291.550, mean=0.2971, (25,50,75)pctl=(0.1657,0.2952,0.4246),(0.0<r>0.5): (2595,329)\n",
      "lambda 09: 3593.814, mean=0.2628, (25,50,75)pctl=(0.1462,0.2600,0.3783),(0.0<r>0.5): (2579,157)\n",
      "lambda 10: 10000.000, mean=0.2348, (25,50,75)pctl=(0.1259,0.2292,0.3409),(0.0<r>0.5): (2561,096)\n",
      "pop.cv.best: 464.159, mean=0.2239, (25,50,75)pctl=(0.1663,0.2978,0.4440),(0.0<r>0.5): (2649,424)\n",
      "5/11: temporal 1/1=1.000, features 5/11=(0.8070, 0.5906)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1102, (25,50,75)pctl=(0.0357,0.0884,0.1678),(0.0<r>0.5): (2366,001)\n",
      "lambda 02:    2.783, mean=0.1183, (25,50,75)pctl=(0.0396,0.0946,0.1797),(0.0<r>0.5): (2389,003)\n",
      "lambda 03:    7.743, mean=0.1513, (25,50,75)pctl=(0.0526,0.1233,0.2294),(0.0<r>0.5): (2462,023)\n",
      "lambda 04:   21.544, mean=0.2122, (25,50,75)pctl=(0.0879,0.1803,0.3265),(0.0<r>0.5): (2547,139)\n",
      "lambda 05:   59.948, mean=0.2612, (25,50,75)pctl=(0.1167,0.2378,0.3978),(0.0<r>0.5): (2598,304)\n",
      "lambda 06:  166.810, mean=0.2894, (25,50,75)pctl=(0.1345,0.2740,0.4343),(0.0<r>0.5): (2604,420)\n",
      "lambda 07:  464.159, mean=0.3040, (25,50,75)pctl=(0.1552,0.2983,0.4497),(0.0<r>0.5): (2617,446)\n",
      "lambda 08: 1291.550, mean=0.2933, (25,50,75)pctl=(0.1561,0.2868,0.4274),(0.0<r>0.5): (2618,347)\n",
      "lambda 09: 3593.814, mean=0.2577, (25,50,75)pctl=(0.1338,0.2495,0.3735),(0.0<r>0.5): (2596,195)\n",
      "lambda 10: 10000.000, mean=0.2305, (25,50,75)pctl=(0.1157,0.2169,0.3351),(0.0<r>0.5): (2566,108)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1092, (25,50,75)pctl=(0.0346,0.0937,0.1668),(0.0<r>0.5): (2357,000)\n",
      "lambda 02:    2.783, mean=0.1176, (25,50,75)pctl=(0.0386,0.1009,0.1783),(0.0<r>0.5): (2373,001)\n",
      "lambda 03:    7.743, mean=0.1511, (25,50,75)pctl=(0.0568,0.1311,0.2299),(0.0<r>0.5): (2442,017)\n",
      "lambda 04:   21.544, mean=0.2134, (25,50,75)pctl=(0.0967,0.1908,0.3235),(0.0<r>0.5): (2537,110)\n",
      "lambda 05:   59.948, mean=0.2652, (25,50,75)pctl=(0.1298,0.2472,0.3907),(0.0<r>0.5): (2588,277)\n",
      "lambda 06:  166.810, mean=0.2976, (25,50,75)pctl=(0.1506,0.2824,0.4319),(0.0<r>0.5): (2609,415)\n",
      "lambda 07:  464.159, mean=0.3111, (25,50,75)pctl=(0.1619,0.3018,0.4476),(0.0<r>0.5): (2616,465)\n",
      "lambda 08: 1291.550, mean=0.2960, (25,50,75)pctl=(0.1559,0.2855,0.4246),(0.0<r>0.5): (2605,377)\n",
      "lambda 09: 3593.814, mean=0.2563, (25,50,75)pctl=(0.1299,0.2394,0.3657),(0.0<r>0.5): (2589,229)\n",
      "lambda 10: 10000.000, mean=0.2219, (25,50,75)pctl=(0.1029,0.1973,0.3250),(0.0<r>0.5): (2552,153)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1087, (25,50,75)pctl=(0.0305,0.0873,0.1664),(0.0<r>0.5): (2318,001)\n",
      "lambda 02:    2.783, mean=0.1172, (25,50,75)pctl=(0.0348,0.0943,0.1803),(0.0<r>0.5): (2350,004)\n",
      "lambda 03:    7.743, mean=0.1511, (25,50,75)pctl=(0.0511,0.1196,0.2325),(0.0<r>0.5): (2434,039)\n",
      "lambda 04:   21.544, mean=0.2110, (25,50,75)pctl=(0.0845,0.1780,0.3228),(0.0<r>0.5): (2538,142)\n",
      "lambda 05:   59.948, mean=0.2577, (25,50,75)pctl=(0.1169,0.2288,0.3860),(0.0<r>0.5): (2590,280)\n",
      "lambda 06:  166.810, mean=0.2835, (25,50,75)pctl=(0.1342,0.2631,0.4237),(0.0<r>0.5): (2600,379)\n",
      "lambda 07:  464.159, mean=0.2933, (25,50,75)pctl=(0.1483,0.2836,0.4265),(0.0<r>0.5): (2606,376)\n",
      "lambda 08: 1291.550, mean=0.2750, (25,50,75)pctl=(0.1470,0.2653,0.3944),(0.0<r>0.5): (2596,264)\n",
      "lambda 09: 3593.814, mean=0.2305, (25,50,75)pctl=(0.1187,0.2168,0.3271),(0.0<r>0.5): (2581,118)\n",
      "lambda 10: 10000.000, mean=0.1950, (25,50,75)pctl=(0.0897,0.1748,0.2851),(0.0<r>0.5): (2516,050)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1195, (25,50,75)pctl=(0.0463,0.0996,0.1796),(0.0<r>0.5): (2438,000)\n",
      "lambda 02:    2.783, mean=0.1276, (25,50,75)pctl=(0.0505,0.1064,0.1925),(0.0<r>0.5): (2464,000)\n",
      "lambda 03:    7.743, mean=0.1594, (25,50,75)pctl=(0.0662,0.1367,0.2377),(0.0<r>0.5): (2512,014)\n",
      "lambda 04:   21.544, mean=0.2167, (25,50,75)pctl=(0.1013,0.1939,0.3163),(0.0<r>0.5): (2564,102)\n",
      "lambda 05:   59.948, mean=0.2610, (25,50,75)pctl=(0.1345,0.2395,0.3796),(0.0<r>0.5): (2601,255)\n",
      "lambda 06:  166.810, mean=0.2853, (25,50,75)pctl=(0.1530,0.2638,0.4183),(0.0<r>0.5): (2615,351)\n",
      "lambda 07:  464.159, mean=0.2932, (25,50,75)pctl=(0.1604,0.2784,0.4236),(0.0<r>0.5): (2619,371)\n",
      "lambda 08: 1291.550, mean=0.2693, (25,50,75)pctl=(0.1484,0.2564,0.3883),(0.0<r>0.5): (2596,220)\n",
      "lambda 09: 3593.814, mean=0.2195, (25,50,75)pctl=(0.1147,0.2056,0.3183),(0.0<r>0.5): (2551,061)\n",
      "lambda 10: 10000.000, mean=0.1855, (25,50,75)pctl=(0.0880,0.1697,0.2730),(0.0<r>0.5): (2513,015)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1273, (25,50,75)pctl=(0.0412,0.1022,0.1971),(0.0<r>0.5): (2397,009)\n",
      "lambda 02:    2.783, mean=0.1354, (25,50,75)pctl=(0.0447,0.1090,0.2099),(0.0<r>0.5): (2420,015)\n",
      "lambda 03:    7.743, mean=0.1680, (25,50,75)pctl=(0.0599,0.1384,0.2603),(0.0<r>0.5): (2477,049)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 04:   21.544, mean=0.2272, (25,50,75)pctl=(0.0966,0.1973,0.3467),(0.0<r>0.5): (2552,185)\n",
      "lambda 05:   59.948, mean=0.2733, (25,50,75)pctl=(0.1308,0.2517,0.4046),(0.0<r>0.5): (2602,328)\n",
      "lambda 06:  166.810, mean=0.2990, (25,50,75)pctl=(0.1479,0.2878,0.4390),(0.0<r>0.5): (2613,425)\n",
      "lambda 07:  464.159, mean=0.3091, (25,50,75)pctl=(0.1659,0.3035,0.4459),(0.0<r>0.5): (2613,444)\n",
      "lambda 08: 1291.550, mean=0.2979, (25,50,75)pctl=(0.1654,0.2963,0.4260),(0.0<r>0.5): (2595,341)\n",
      "lambda 09: 3593.814, mean=0.2656, (25,50,75)pctl=(0.1476,0.2634,0.3821),(0.0<r>0.5): (2583,169)\n",
      "lambda 10: 10000.000, mean=0.2359, (25,50,75)pctl=(0.1270,0.2300,0.3425),(0.0<r>0.5): (2558,096)\n",
      "pop.cv.best: 464.159, mean=0.2213, (25,50,75)pctl=(0.1643,0.2936,0.4411),(0.0<r>0.5): (2649,408)\n",
      "6/11: temporal 1/1=1.000, features 6/11=(0.7071, 0.7071)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1086, (25,50,75)pctl=(0.0346,0.0871,0.1656),(0.0<r>0.5): (2356,001)\n",
      "lambda 02:    2.783, mean=0.1151, (25,50,75)pctl=(0.0381,0.0921,0.1736),(0.0<r>0.5): (2375,003)\n",
      "lambda 03:    7.743, mean=0.1437, (25,50,75)pctl=(0.0498,0.1166,0.2171),(0.0<r>0.5): (2445,010)\n",
      "lambda 04:   21.544, mean=0.2021, (25,50,75)pctl=(0.0812,0.1703,0.3114),(0.0<r>0.5): (2539,108)\n",
      "lambda 05:   59.948, mean=0.2539, (25,50,75)pctl=(0.1119,0.2267,0.3859),(0.0<r>0.5): (2591,276)\n",
      "lambda 06:  166.810, mean=0.2840, (25,50,75)pctl=(0.1296,0.2663,0.4284),(0.0<r>0.5): (2602,397)\n",
      "lambda 07:  464.159, mean=0.3017, (25,50,75)pctl=(0.1520,0.2941,0.4469),(0.0<r>0.5): (2614,445)\n",
      "lambda 08: 1291.550, mean=0.2956, (25,50,75)pctl=(0.1556,0.2878,0.4305),(0.0<r>0.5): (2619,371)\n",
      "lambda 09: 3593.814, mean=0.2623, (25,50,75)pctl=(0.1357,0.2540,0.3789),(0.0<r>0.5): (2601,210)\n",
      "lambda 10: 10000.000, mean=0.2324, (25,50,75)pctl=(0.1171,0.2192,0.3380),(0.0<r>0.5): (2570,109)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1079, (25,50,75)pctl=(0.0339,0.0928,0.1641),(0.0<r>0.5): (2350,000)\n",
      "lambda 02:    2.783, mean=0.1146, (25,50,75)pctl=(0.0369,0.0981,0.1736),(0.0<r>0.5): (2364,000)\n",
      "lambda 03:    7.743, mean=0.1437, (25,50,75)pctl=(0.0521,0.1241,0.2174),(0.0<r>0.5): (2429,013)\n",
      "lambda 04:   21.544, mean=0.2033, (25,50,75)pctl=(0.0904,0.1794,0.3058),(0.0<r>0.5): (2524,093)\n",
      "lambda 05:   59.948, mean=0.2574, (25,50,75)pctl=(0.1241,0.2393,0.3789),(0.0<r>0.5): (2584,253)\n",
      "lambda 06:  166.810, mean=0.2913, (25,50,75)pctl=(0.1457,0.2742,0.4214),(0.0<r>0.5): (2604,394)\n",
      "lambda 07:  464.159, mean=0.3088, (25,50,75)pctl=(0.1593,0.2971,0.4440),(0.0<r>0.5): (2616,462)\n",
      "lambda 08: 1291.550, mean=0.2984, (25,50,75)pctl=(0.1568,0.2887,0.4267),(0.0<r>0.5): (2605,385)\n",
      "lambda 09: 3593.814, mean=0.2617, (25,50,75)pctl=(0.1325,0.2469,0.3732),(0.0<r>0.5): (2590,241)\n",
      "lambda 10: 10000.000, mean=0.2244, (25,50,75)pctl=(0.1050,0.2003,0.3272),(0.0<r>0.5): (2555,156)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1074, (25,50,75)pctl=(0.0301,0.0861,0.1646),(0.0<r>0.5): (2306,000)\n",
      "lambda 02:    2.783, mean=0.1142, (25,50,75)pctl=(0.0332,0.0911,0.1760),(0.0<r>0.5): (2339,004)\n",
      "lambda 03:    7.743, mean=0.1436, (25,50,75)pctl=(0.0480,0.1133,0.2210),(0.0<r>0.5): (2425,026)\n",
      "lambda 04:   21.544, mean=0.2015, (25,50,75)pctl=(0.0789,0.1693,0.3100),(0.0<r>0.5): (2528,121)\n",
      "lambda 05:   59.948, mean=0.2507, (25,50,75)pctl=(0.1128,0.2212,0.3739),(0.0<r>0.5): (2585,260)\n",
      "lambda 06:  166.810, mean=0.2783, (25,50,75)pctl=(0.1302,0.2567,0.4177),(0.0<r>0.5): (2596,363)\n",
      "lambda 07:  464.159, mean=0.2915, (25,50,75)pctl=(0.1451,0.2789,0.4267),(0.0<r>0.5): (2603,381)\n",
      "lambda 08: 1291.550, mean=0.2783, (25,50,75)pctl=(0.1472,0.2700,0.3993),(0.0<r>0.5): (2597,281)\n",
      "lambda 09: 3593.814, mean=0.2363, (25,50,75)pctl=(0.1233,0.2229,0.3356),(0.0<r>0.5): (2580,130)\n",
      "lambda 10: 10000.000, mean=0.1976, (25,50,75)pctl=(0.0925,0.1771,0.2879),(0.0<r>0.5): (2524,062)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1176, (25,50,75)pctl=(0.0455,0.0981,0.1761),(0.0<r>0.5): (2430,000)\n",
      "lambda 02:    2.783, mean=0.1241, (25,50,75)pctl=(0.0480,0.1040,0.1869),(0.0<r>0.5): (2451,000)\n",
      "lambda 03:    7.743, mean=0.1518, (25,50,75)pctl=(0.0618,0.1293,0.2281),(0.0<r>0.5): (2501,011)\n",
      "lambda 04:   21.544, mean=0.2071, (25,50,75)pctl=(0.0955,0.1849,0.3024),(0.0<r>0.5): (2558,076)\n",
      "lambda 05:   59.948, mean=0.2543, (25,50,75)pctl=(0.1292,0.2337,0.3709),(0.0<r>0.5): (2598,230)\n",
      "lambda 06:  166.810, mean=0.2804, (25,50,75)pctl=(0.1482,0.2561,0.4125),(0.0<r>0.5): (2611,336)\n",
      "lambda 07:  464.159, mean=0.2920, (25,50,75)pctl=(0.1583,0.2760,0.4227),(0.0<r>0.5): (2619,368)\n",
      "lambda 08: 1291.550, mean=0.2735, (25,50,75)pctl=(0.1508,0.2610,0.3938),(0.0<r>0.5): (2598,237)\n",
      "lambda 09: 3593.814, mean=0.2256, (25,50,75)pctl=(0.1186,0.2117,0.3267),(0.0<r>0.5): (2559,085)\n",
      "lambda 10: 10000.000, mean=0.1879, (25,50,75)pctl=(0.0903,0.1726,0.2756),(0.0<r>0.5): (2517,016)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1263, (25,50,75)pctl=(0.0405,0.1008,0.1959),(0.0<r>0.5): (2394,007)\n",
      "lambda 02:    2.783, mean=0.1327, (25,50,75)pctl=(0.0436,0.1064,0.2059),(0.0<r>0.5): (2413,015)\n",
      "lambda 03:    7.743, mean=0.1610, (25,50,75)pctl=(0.0572,0.1319,0.2494),(0.0<r>0.5): (2460,037)\n",
      "lambda 04:   21.544, mean=0.2179, (25,50,75)pctl=(0.0903,0.1882,0.3331),(0.0<r>0.5): (2537,156)\n",
      "lambda 05:   59.948, mean=0.2666, (25,50,75)pctl=(0.1253,0.2425,0.3956),(0.0<r>0.5): (2595,303)\n",
      "lambda 06:  166.810, mean=0.2938, (25,50,75)pctl=(0.1440,0.2794,0.4316),(0.0<r>0.5): (2612,403)\n",
      "lambda 07:  464.159, mean=0.3070, (25,50,75)pctl=(0.1631,0.3009,0.4438),(0.0<r>0.5): (2616,439)\n",
      "lambda 08: 1291.550, mean=0.2998, (25,50,75)pctl=(0.1663,0.2987,0.4296),(0.0<r>0.5): (2598,353)\n",
      "lambda 09: 3593.814, mean=0.2701, (25,50,75)pctl=(0.1504,0.2678,0.3871),(0.0<r>0.5): (2584,188)\n",
      "lambda 10: 10000.000, mean=0.2382, (25,50,75)pctl=(0.1285,0.2318,0.3453),(0.0<r>0.5): (2559,100)\n",
      "pop.cv.best: 464.159, mean=0.2188, (25,50,75)pctl=(0.1633,0.2907,0.4390),(0.0<r>0.5): (2649,401)\n",
      "7/11: temporal 1/1=1.000, features 7/11=(0.5906, 0.8070)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1072, (25,50,75)pctl=(0.0337,0.0850,0.1635),(0.0<r>0.5): (2347,001)\n",
      "lambda 02:    2.783, mean=0.1119, (25,50,75)pctl=(0.0371,0.0892,0.1697),(0.0<r>0.5): (2363,003)\n",
      "lambda 03:    7.743, mean=0.1350, (25,50,75)pctl=(0.0461,0.1084,0.2028),(0.0<r>0.5): (2420,005)\n",
      "lambda 04:   21.544, mean=0.1893, (25,50,75)pctl=(0.0732,0.1575,0.2900),(0.0<r>0.5): (2526,084)\n",
      "lambda 05:   59.948, mean=0.2445, (25,50,75)pctl=(0.1060,0.2139,0.3711),(0.0<r>0.5): (2584,248)\n",
      "lambda 06:  166.810, mean=0.2775, (25,50,75)pctl=(0.1259,0.2565,0.4184),(0.0<r>0.5): (2598,373)\n",
      "lambda 07:  464.159, mean=0.2988, (25,50,75)pctl=(0.1477,0.2910,0.4452),(0.0<r>0.5): (2611,442)\n",
      "lambda 08: 1291.550, mean=0.2983, (25,50,75)pctl=(0.1579,0.2902,0.4344),(0.0<r>0.5): (2617,387)\n",
      "lambda 09: 3593.814, mean=0.2689, (25,50,75)pctl=(0.1395,0.2594,0.3891),(0.0<r>0.5): (2602,232)\n",
      "lambda 10: 10000.000, mean=0.2359, (25,50,75)pctl=(0.1196,0.2226,0.3427),(0.0<r>0.5): (2578,125)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1067, (25,50,75)pctl=(0.0336,0.0913,0.1620),(0.0<r>0.5): (2343,000)\n",
      "lambda 02:    2.783, mean=0.1116, (25,50,75)pctl=(0.0357,0.0950,0.1693),(0.0<r>0.5): (2360,000)\n",
      "lambda 03:    7.743, mean=0.1350, (25,50,75)pctl=(0.0474,0.1161,0.2038),(0.0<r>0.5): (2416,010)\n",
      "lambda 04:   21.544, mean=0.1903, (25,50,75)pctl=(0.0813,0.1668,0.2855),(0.0<r>0.5): (2503,066)\n",
      "lambda 05:   59.948, mean=0.2476, (25,50,75)pctl=(0.1181,0.2277,0.3653),(0.0<r>0.5): (2578,209)\n",
      "lambda 06:  166.810, mean=0.2839, (25,50,75)pctl=(0.1407,0.2658,0.4109),(0.0<r>0.5): (2602,350)\n",
      "lambda 07:  464.159, mean=0.3061, (25,50,75)pctl=(0.1568,0.2943,0.4413),(0.0<r>0.5): (2616,449)\n",
      "lambda 08: 1291.550, mean=0.3015, (25,50,75)pctl=(0.1578,0.2923,0.4313),(0.0<r>0.5): (2605,407)\n",
      "lambda 09: 3593.814, mean=0.2693, (25,50,75)pctl=(0.1379,0.2561,0.3858),(0.0<r>0.5): (2593,272)\n",
      "lambda 10: 10000.000, mean=0.2288, (25,50,75)pctl=(0.1090,0.2064,0.3324),(0.0<r>0.5): (2561,166)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1060, (25,50,75)pctl=(0.0301,0.0849,0.1637),(0.0<r>0.5): (2299,000)\n",
      "lambda 02:    2.783, mean=0.1110, (25,50,75)pctl=(0.0319,0.0888,0.1709),(0.0<r>0.5): (2326,001)\n",
      "lambda 03:    7.743, mean=0.1347, (25,50,75)pctl=(0.0439,0.1064,0.2078),(0.0<r>0.5): (2400,015)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 04:   21.544, mean=0.1890, (25,50,75)pctl=(0.0712,0.1566,0.2913),(0.0<r>0.5): (2507,102)\n",
      "lambda 05:   59.948, mean=0.2417, (25,50,75)pctl=(0.1061,0.2118,0.3594),(0.0<r>0.5): (2572,240)\n",
      "lambda 06:  166.810, mean=0.2722, (25,50,75)pctl=(0.1252,0.2466,0.4094),(0.0<r>0.5): (2597,349)\n",
      "lambda 07:  464.159, mean=0.2892, (25,50,75)pctl=(0.1401,0.2759,0.4259),(0.0<r>0.5): (2600,381)\n",
      "lambda 08: 1291.550, mean=0.2825, (25,50,75)pctl=(0.1483,0.2748,0.4060),(0.0<r>0.5): (2600,297)\n",
      "lambda 09: 3593.814, mean=0.2445, (25,50,75)pctl=(0.1283,0.2306,0.3489),(0.0<r>0.5): (2586,152)\n",
      "lambda 10: 10000.000, mean=0.2022, (25,50,75)pctl=(0.0977,0.1834,0.2920),(0.0<r>0.5): (2541,067)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1160, (25,50,75)pctl=(0.0438,0.0970,0.1731),(0.0<r>0.5): (2422,000)\n",
      "lambda 02:    2.783, mean=0.1207, (25,50,75)pctl=(0.0467,0.1010,0.1807),(0.0<r>0.5): (2437,000)\n",
      "lambda 03:    7.743, mean=0.1432, (25,50,75)pctl=(0.0575,0.1214,0.2155),(0.0<r>0.5): (2487,007)\n",
      "lambda 04:   21.544, mean=0.1949, (25,50,75)pctl=(0.0881,0.1719,0.2870),(0.0<r>0.5): (2547,054)\n",
      "lambda 05:   59.948, mean=0.2458, (25,50,75)pctl=(0.1228,0.2227,0.3607),(0.0<r>0.5): (2587,196)\n",
      "lambda 06:  166.810, mean=0.2746, (25,50,75)pctl=(0.1435,0.2503,0.4039),(0.0<r>0.5): (2609,307)\n",
      "lambda 07:  464.159, mean=0.2903, (25,50,75)pctl=(0.1551,0.2734,0.4230),(0.0<r>0.5): (2616,367)\n",
      "lambda 08: 1291.550, mean=0.2788, (25,50,75)pctl=(0.1538,0.2654,0.4014),(0.0<r>0.5): (2601,273)\n",
      "lambda 09: 3593.814, mean=0.2344, (25,50,75)pctl=(0.1256,0.2207,0.3389),(0.0<r>0.5): (2570,106)\n",
      "lambda 10: 10000.000, mean=0.1921, (25,50,75)pctl=(0.0942,0.1769,0.2809),(0.0<r>0.5): (2522,020)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1252, (25,50,75)pctl=(0.0400,0.1005,0.1939),(0.0<r>0.5): (2395,007)\n",
      "lambda 02:    2.783, mean=0.1298, (25,50,75)pctl=(0.0422,0.1041,0.2015),(0.0<r>0.5): (2404,013)\n",
      "lambda 03:    7.743, mean=0.1526, (25,50,75)pctl=(0.0528,0.1240,0.2371),(0.0<r>0.5): (2448,026)\n",
      "lambda 04:   21.544, mean=0.2057, (25,50,75)pctl=(0.0831,0.1740,0.3145),(0.0<r>0.5): (2521,134)\n",
      "lambda 05:   59.948, mean=0.2580, (25,50,75)pctl=(0.1187,0.2339,0.3841),(0.0<r>0.5): (2589,282)\n",
      "lambda 06:  166.810, mean=0.2878, (25,50,75)pctl=(0.1415,0.2694,0.4240),(0.0<r>0.5): (2609,376)\n",
      "lambda 07:  464.159, mean=0.3045, (25,50,75)pctl=(0.1592,0.2962,0.4423),(0.0<r>0.5): (2613,435)\n",
      "lambda 08: 1291.550, mean=0.3021, (25,50,75)pctl=(0.1674,0.3003,0.4342),(0.0<r>0.5): (2602,377)\n",
      "lambda 09: 3593.814, mean=0.2764, (25,50,75)pctl=(0.1540,0.2752,0.3962),(0.0<r>0.5): (2588,222)\n",
      "lambda 10: 10000.000, mean=0.2421, (25,50,75)pctl=(0.1315,0.2364,0.3523),(0.0<r>0.5): (2561,106)\n",
      "pop.cv.best: 464.159, mean=0.2159, (25,50,75)pctl=(0.1638,0.2896,0.4358),(0.0<r>0.5): (2648,395)\n",
      "8/11: temporal 1/1=1.000, features 8/11=(0.4602, 0.8878)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1060, (25,50,75)pctl=(0.0332,0.0834,0.1612),(0.0<r>0.5): (2343,000)\n",
      "lambda 02:    2.783, mean=0.1090, (25,50,75)pctl=(0.0352,0.0856,0.1647),(0.0<r>0.5): (2352,002)\n",
      "lambda 03:    7.743, mean=0.1254, (25,50,75)pctl=(0.0427,0.0998,0.1888),(0.0<r>0.5): (2392,004)\n",
      "lambda 04:   21.544, mean=0.1727, (25,50,75)pctl=(0.0628,0.1408,0.2633),(0.0<r>0.5): (2499,053)\n",
      "lambda 05:   59.948, mean=0.2315, (25,50,75)pctl=(0.0973,0.1984,0.3493),(0.0<r>0.5): (2571,212)\n",
      "lambda 06:  166.810, mean=0.2690, (25,50,75)pctl=(0.1194,0.2438,0.4059),(0.0<r>0.5): (2599,347)\n",
      "lambda 07:  464.159, mean=0.2940, (25,50,75)pctl=(0.1413,0.2834,0.4366),(0.0<r>0.5): (2608,435)\n",
      "lambda 08: 1291.550, mean=0.3007, (25,50,75)pctl=(0.1586,0.2939,0.4389),(0.0<r>0.5): (2619,413)\n",
      "lambda 09: 3593.814, mean=0.2780, (25,50,75)pctl=(0.1458,0.2695,0.4025),(0.0<r>0.5): (2610,263)\n",
      "lambda 10: 10000.000, mean=0.2420, (25,50,75)pctl=(0.1240,0.2307,0.3500),(0.0<r>0.5): (2583,152)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1056, (25,50,75)pctl=(0.0337,0.0893,0.1608),(0.0<r>0.5): (2341,000)\n",
      "lambda 02:    2.783, mean=0.1086, (25,50,75)pctl=(0.0350,0.0925,0.1654),(0.0<r>0.5): (2347,000)\n",
      "lambda 03:    7.743, mean=0.1255, (25,50,75)pctl=(0.0431,0.1075,0.1904),(0.0<r>0.5): (2390,005)\n",
      "lambda 04:   21.544, mean=0.1734, (25,50,75)pctl=(0.0704,0.1506,0.2598),(0.0<r>0.5): (2480,037)\n",
      "lambda 05:   59.948, mean=0.2339, (25,50,75)pctl=(0.1090,0.2118,0.3458),(0.0<r>0.5): (2565,170)\n",
      "lambda 06:  166.810, mean=0.2742, (25,50,75)pctl=(0.1345,0.2558,0.3977),(0.0<r>0.5): (2597,309)\n",
      "lambda 07:  464.159, mean=0.3015, (25,50,75)pctl=(0.1519,0.2896,0.4355),(0.0<r>0.5): (2610,436)\n",
      "lambda 08: 1291.550, mean=0.3047, (25,50,75)pctl=(0.1579,0.2954,0.4360),(0.0<r>0.5): (2609,418)\n",
      "lambda 09: 3593.814, mean=0.2793, (25,50,75)pctl=(0.1457,0.2686,0.4011),(0.0<r>0.5): (2596,311)\n",
      "lambda 10: 10000.000, mean=0.2367, (25,50,75)pctl=(0.1164,0.2148,0.3423),(0.0<r>0.5): (2572,180)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1047, (25,50,75)pctl=(0.0295,0.0838,0.1626),(0.0<r>0.5): (2296,000)\n",
      "lambda 02:    2.783, mean=0.1079, (25,50,75)pctl=(0.0307,0.0865,0.1665),(0.0<r>0.5): (2309,000)\n",
      "lambda 03:    7.743, mean=0.1249, (25,50,75)pctl=(0.0395,0.0993,0.1931),(0.0<r>0.5): (2375,006)\n",
      "lambda 04:   21.544, mean=0.1726, (25,50,75)pctl=(0.0627,0.1402,0.2671),(0.0<r>0.5): (2477,063)\n",
      "lambda 05:   59.948, mean=0.2292, (25,50,75)pctl=(0.0982,0.1971,0.3420),(0.0<r>0.5): (2558,213)\n",
      "lambda 06:  166.810, mean=0.2642, (25,50,75)pctl=(0.1201,0.2355,0.3979),(0.0<r>0.5): (2590,315)\n",
      "lambda 07:  464.159, mean=0.2854, (25,50,75)pctl=(0.1358,0.2690,0.4215),(0.0<r>0.5): (2600,378)\n",
      "lambda 08: 1291.550, mean=0.2868, (25,50,75)pctl=(0.1486,0.2783,0.4141),(0.0<r>0.5): (2604,323)\n",
      "lambda 09: 3593.814, mean=0.2557, (25,50,75)pctl=(0.1366,0.2437,0.3662),(0.0<r>0.5): (2592,187)\n",
      "lambda 10: 10000.000, mean=0.2103, (25,50,75)pctl=(0.1043,0.1932,0.3006),(0.0<r>0.5): (2545,078)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1147, (25,50,75)pctl=(0.0430,0.0963,0.1709),(0.0<r>0.5): (2416,000)\n",
      "lambda 02:    2.783, mean=0.1177, (25,50,75)pctl=(0.0442,0.0985,0.1766),(0.0<r>0.5): (2430,000)\n",
      "lambda 03:    7.743, mean=0.1339, (25,50,75)pctl=(0.0528,0.1121,0.2019),(0.0<r>0.5): (2472,003)\n",
      "lambda 04:   21.544, mean=0.1791, (25,50,75)pctl=(0.0777,0.1560,0.2648),(0.0<r>0.5): (2530,032)\n",
      "lambda 05:   59.948, mean=0.2340, (25,50,75)pctl=(0.1139,0.2116,0.3445),(0.0<r>0.5): (2576,155)\n",
      "lambda 06:  166.810, mean=0.2671, (25,50,75)pctl=(0.1372,0.2431,0.3940),(0.0<r>0.5): (2603,279)\n",
      "lambda 07:  464.159, mean=0.2871, (25,50,75)pctl=(0.1523,0.2693,0.4195),(0.0<r>0.5): (2619,352)\n",
      "lambda 08: 1291.550, mean=0.2846, (25,50,75)pctl=(0.1571,0.2715,0.4109),(0.0<r>0.5): (2604,307)\n",
      "lambda 09: 3593.814, mean=0.2469, (25,50,75)pctl=(0.1336,0.2340,0.3578),(0.0<r>0.5): (2581,142)\n",
      "lambda 10: 10000.000, mean=0.1997, (25,50,75)pctl=(0.1007,0.1859,0.2914),(0.0<r>0.5): (2534,026)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1241, (25,50,75)pctl=(0.0393,0.0999,0.1914),(0.0<r>0.5): (2386,007)\n",
      "lambda 02:    2.783, mean=0.1270, (25,50,75)pctl=(0.0407,0.1021,0.1961),(0.0<r>0.5): (2391,009)\n",
      "lambda 03:    7.743, mean=0.1433, (25,50,75)pctl=(0.0475,0.1158,0.2218),(0.0<r>0.5): (2425,022)\n",
      "lambda 04:   21.544, mean=0.1897, (25,50,75)pctl=(0.0732,0.1569,0.2919),(0.0<r>0.5): (2496,090)\n",
      "lambda 05:   59.948, mean=0.2458, (25,50,75)pctl=(0.1102,0.2196,0.3702),(0.0<r>0.5): (2577,240)\n",
      "lambda 06:  166.810, mean=0.2799, (25,50,75)pctl=(0.1348,0.2574,0.4121),(0.0<r>0.5): (2607,344)\n",
      "lambda 07:  464.159, mean=0.3007, (25,50,75)pctl=(0.1531,0.2901,0.4390),(0.0<r>0.5): (2614,421)\n",
      "lambda 08: 1291.550, mean=0.3044, (25,50,75)pctl=(0.1687,0.3014,0.4370),(0.0<r>0.5): (2608,397)\n",
      "lambda 09: 3593.814, mean=0.2846, (25,50,75)pctl=(0.1585,0.2833,0.4081),(0.0<r>0.5): (2589,266)\n",
      "lambda 10: 10000.000, mean=0.2490, (25,50,75)pctl=(0.1367,0.2453,0.3608),(0.0<r>0.5): (2563,115)\n",
      "pop.cv.best: 1291.550, mean=0.2125, (25,50,75)pctl=(0.1630,0.2873,0.4339),(0.0<r>0.5): (2648,390)\n",
      "9/11: temporal 1/1=1.000, features 9/11=(0.3190, 0.9478)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1051, (25,50,75)pctl=(0.0330,0.0826,0.1598),(0.0<r>0.5): (2336,000)\n",
      "lambda 02:    2.783, mean=0.1066, (25,50,75)pctl=(0.0338,0.0837,0.1614),(0.0<r>0.5): (2345,001)\n",
      "lambda 03:    7.743, mean=0.1159, (25,50,75)pctl=(0.0386,0.0916,0.1747),(0.0<r>0.5): (2370,003)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 04:   21.544, mean=0.1513, (25,50,75)pctl=(0.0532,0.1219,0.2266),(0.0<r>0.5): (2466,032)\n",
      "lambda 05:   59.948, mean=0.2108, (25,50,75)pctl=(0.0859,0.1773,0.3193),(0.0<r>0.5): (2554,156)\n",
      "lambda 06:  166.810, mean=0.2563, (25,50,75)pctl=(0.1130,0.2289,0.3875),(0.0<r>0.5): (2594,301)\n",
      "lambda 07:  464.159, mean=0.2855, (25,50,75)pctl=(0.1321,0.2677,0.4279),(0.0<r>0.5): (2607,405)\n",
      "lambda 08: 1291.550, mean=0.3011, (25,50,75)pctl=(0.1517,0.2942,0.4425),(0.0<r>0.5): (2617,427)\n",
      "lambda 09: 3593.814, mean=0.2894, (25,50,75)pctl=(0.1527,0.2810,0.4217),(0.0<r>0.5): (2614,326)\n",
      "lambda 10: 10000.000, mean=0.2536, (25,50,75)pctl=(0.1305,0.2449,0.3667),(0.0<r>0.5): (2591,186)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1047, (25,50,75)pctl=(0.0329,0.0886,0.1598),(0.0<r>0.5): (2340,000)\n",
      "lambda 02:    2.783, mean=0.1062, (25,50,75)pctl=(0.0336,0.0898,0.1616),(0.0<r>0.5): (2344,000)\n",
      "lambda 03:    7.743, mean=0.1159, (25,50,75)pctl=(0.0379,0.0976,0.1775),(0.0<r>0.5): (2367,002)\n",
      "lambda 04:   21.544, mean=0.1518, (25,50,75)pctl=(0.0574,0.1294,0.2268),(0.0<r>0.5): (2441,023)\n",
      "lambda 05:   59.948, mean=0.2126, (25,50,75)pctl=(0.0955,0.1876,0.3160),(0.0<r>0.5): (2533,119)\n",
      "lambda 06:  166.810, mean=0.2603, (25,50,75)pctl=(0.1256,0.2412,0.3787),(0.0<r>0.5): (2589,266)\n",
      "lambda 07:  464.159, mean=0.2926, (25,50,75)pctl=(0.1473,0.2784,0.4210),(0.0<r>0.5): (2608,404)\n",
      "lambda 08: 1291.550, mean=0.3067, (25,50,75)pctl=(0.1568,0.2962,0.4397),(0.0<r>0.5): (2608,443)\n",
      "lambda 09: 3593.814, mean=0.2915, (25,50,75)pctl=(0.1524,0.2808,0.4188),(0.0<r>0.5): (2605,362)\n",
      "lambda 10: 10000.000, mean=0.2512, (25,50,75)pctl=(0.1272,0.2319,0.3607),(0.0<r>0.5): (2586,216)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1036, (25,50,75)pctl=(0.0288,0.0828,0.1621),(0.0<r>0.5): (2291,000)\n",
      "lambda 02:    2.783, mean=0.1052, (25,50,75)pctl=(0.0300,0.0840,0.1638),(0.0<r>0.5): (2294,000)\n",
      "lambda 03:    7.743, mean=0.1149, (25,50,75)pctl=(0.0341,0.0918,0.1774),(0.0<r>0.5): (2340,003)\n",
      "lambda 04:   21.544, mean=0.1510, (25,50,75)pctl=(0.0515,0.1185,0.2339),(0.0<r>0.5): (2436,042)\n",
      "lambda 05:   59.948, mean=0.2094, (25,50,75)pctl=(0.0854,0.1760,0.3184),(0.0<r>0.5): (2537,149)\n",
      "lambda 06:  166.810, mean=0.2523, (25,50,75)pctl=(0.1120,0.2221,0.3782),(0.0<r>0.5): (2586,271)\n",
      "lambda 07:  464.159, mean=0.2784, (25,50,75)pctl=(0.1295,0.2561,0.4142),(0.0<r>0.5): (2596,363)\n",
      "lambda 08: 1291.550, mean=0.2894, (25,50,75)pctl=(0.1464,0.2788,0.4201),(0.0<r>0.5): (2604,359)\n",
      "lambda 09: 3593.814, mean=0.2702, (25,50,75)pctl=(0.1429,0.2597,0.3880),(0.0<r>0.5): (2595,250)\n",
      "lambda 10: 10000.000, mean=0.2254, (25,50,75)pctl=(0.1150,0.2103,0.3199),(0.0<r>0.5): (2570,111)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1136, (25,50,75)pctl=(0.0422,0.0954,0.1701),(0.0<r>0.5): (2412,000)\n",
      "lambda 02:    2.783, mean=0.1151, (25,50,75)pctl=(0.0427,0.0965,0.1732),(0.0<r>0.5): (2414,000)\n",
      "lambda 03:    7.743, mean=0.1245, (25,50,75)pctl=(0.0477,0.1047,0.1862),(0.0<r>0.5): (2447,000)\n",
      "lambda 04:   21.544, mean=0.1586, (25,50,75)pctl=(0.0657,0.1369,0.2358),(0.0<r>0.5): (2506,013)\n",
      "lambda 05:   59.948, mean=0.2150, (25,50,75)pctl=(0.1011,0.1926,0.3145),(0.0<r>0.5): (2560,096)\n",
      "lambda 06:  166.810, mean=0.2559, (25,50,75)pctl=(0.1308,0.2343,0.3765),(0.0<r>0.5): (2598,236)\n",
      "lambda 07:  464.159, mean=0.2805, (25,50,75)pctl=(0.1467,0.2593,0.4121),(0.0<r>0.5): (2612,334)\n",
      "lambda 08: 1291.550, mean=0.2891, (25,50,75)pctl=(0.1571,0.2728,0.4189),(0.0<r>0.5): (2617,352)\n",
      "lambda 09: 3593.814, mean=0.2639, (25,50,75)pctl=(0.1444,0.2501,0.3819),(0.0<r>0.5): (2587,206)\n",
      "lambda 10: 10000.000, mean=0.2144, (25,50,75)pctl=(0.1114,0.2009,0.3118),(0.0<r>0.5): (2544,051)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1231, (25,50,75)pctl=(0.0383,0.0990,0.1892),(0.0<r>0.5): (2380,007)\n",
      "lambda 02:    2.783, mean=0.1246, (25,50,75)pctl=(0.0394,0.1003,0.1910),(0.0<r>0.5): (2385,007)\n",
      "lambda 03:    7.743, mean=0.1338, (25,50,75)pctl=(0.0437,0.1078,0.2070),(0.0<r>0.5): (2410,016)\n",
      "lambda 04:   21.544, mean=0.1687, (25,50,75)pctl=(0.0609,0.1377,0.2577),(0.0<r>0.5): (2465,050)\n",
      "lambda 05:   59.948, mean=0.2262, (25,50,75)pctl=(0.0948,0.1977,0.3442),(0.0<r>0.5): (2549,177)\n",
      "lambda 06:  166.810, mean=0.2682, (25,50,75)pctl=(0.1274,0.2446,0.3975),(0.0<r>0.5): (2597,302)\n",
      "lambda 07:  464.159, mean=0.2937, (25,50,75)pctl=(0.1453,0.2793,0.4298),(0.0<r>0.5): (2612,397)\n",
      "lambda 08: 1291.550, mean=0.3052, (25,50,75)pctl=(0.1661,0.2996,0.4401),(0.0<r>0.5): (2610,419)\n",
      "lambda 09: 3593.814, mean=0.2943, (25,50,75)pctl=(0.1626,0.2926,0.4214),(0.0<r>0.5): (2595,318)\n",
      "lambda 10: 10000.000, mean=0.2614, (25,50,75)pctl=(0.1454,0.2588,0.3769),(0.0<r>0.5): (2576,156)\n",
      "pop.cv.best: 1291.550, mean=0.2080, (25,50,75)pctl=(0.1618,0.2885,0.4345),(0.0<r>0.5): (2648,386)\n",
      "10/11: temporal 1/1=1.000, features 10/11=(0.1702, 0.9854)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1046, (25,50,75)pctl=(0.0331,0.0820,0.1589),(0.0<r>0.5): (2336,000)\n",
      "lambda 02:    2.783, mean=0.1050, (25,50,75)pctl=(0.0332,0.0823,0.1596),(0.0<r>0.5): (2336,000)\n",
      "lambda 03:    7.743, mean=0.1081, (25,50,75)pctl=(0.0343,0.0846,0.1630),(0.0<r>0.5): (2351,002)\n",
      "lambda 04:   21.544, mean=0.1251, (25,50,75)pctl=(0.0425,0.0998,0.1871),(0.0<r>0.5): (2395,004)\n",
      "lambda 05:   59.948, mean=0.1726, (25,50,75)pctl=(0.0633,0.1402,0.2610),(0.0<r>0.5): (2498,058)\n",
      "lambda 06:  166.810, mean=0.2305, (25,50,75)pctl=(0.0962,0.1969,0.3489),(0.0<r>0.5): (2572,217)\n",
      "lambda 07:  464.159, mean=0.2680, (25,50,75)pctl=(0.1187,0.2427,0.4014),(0.0<r>0.5): (2595,348)\n",
      "lambda 08: 1291.550, mean=0.2936, (25,50,75)pctl=(0.1413,0.2834,0.4370),(0.0<r>0.5): (2611,435)\n",
      "lambda 09: 3593.814, mean=0.3001, (25,50,75)pctl=(0.1582,0.2928,0.4383),(0.0<r>0.5): (2620,407)\n",
      "lambda 10: 10000.000, mean=0.2768, (25,50,75)pctl=(0.1452,0.2680,0.4004),(0.0<r>0.5): (2610,257)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1045, (25,50,75)pctl=(0.0329,0.0883,0.1593),(0.0<r>0.5): (2339,000)\n",
      "lambda 02:    2.783, mean=0.1050, (25,50,75)pctl=(0.0335,0.0885,0.1597),(0.0<r>0.5): (2342,000)\n",
      "lambda 03:    7.743, mean=0.1082, (25,50,75)pctl=(0.0345,0.0906,0.1648),(0.0<r>0.5): (2347,000)\n",
      "lambda 04:   21.544, mean=0.1256, (25,50,75)pctl=(0.0431,0.1070,0.1904),(0.0<r>0.5): (2389,005)\n",
      "lambda 05:   59.948, mean=0.1741, (25,50,75)pctl=(0.0707,0.1498,0.2603),(0.0<r>0.5): (2479,046)\n",
      "lambda 06:  166.810, mean=0.2336, (25,50,75)pctl=(0.1088,0.2120,0.3438),(0.0<r>0.5): (2563,177)\n",
      "lambda 07:  464.159, mean=0.2734, (25,50,75)pctl=(0.1326,0.2550,0.3944),(0.0<r>0.5): (2597,315)\n",
      "lambda 08: 1291.550, mean=0.3008, (25,50,75)pctl=(0.1509,0.2885,0.4348),(0.0<r>0.5): (2611,434)\n",
      "lambda 09: 3593.814, mean=0.3039, (25,50,75)pctl=(0.1577,0.2942,0.4345),(0.0<r>0.5): (2609,417)\n",
      "lambda 10: 10000.000, mean=0.2780, (25,50,75)pctl=(0.1443,0.2666,0.3991),(0.0<r>0.5): (2595,304)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1029, (25,50,75)pctl=(0.0288,0.0823,0.1608),(0.0<r>0.5): (2288,000)\n",
      "lambda 02:    2.783, mean=0.1033, (25,50,75)pctl=(0.0290,0.0825,0.1610),(0.0<r>0.5): (2289,000)\n",
      "lambda 03:    7.743, mean=0.1066, (25,50,75)pctl=(0.0304,0.0847,0.1655),(0.0<r>0.5): (2302,000)\n",
      "lambda 04:   21.544, mean=0.1241, (25,50,75)pctl=(0.0380,0.0984,0.1909),(0.0<r>0.5): (2380,006)\n",
      "lambda 05:   59.948, mean=0.1722, (25,50,75)pctl=(0.0623,0.1396,0.2634),(0.0<r>0.5): (2480,065)\n",
      "lambda 06:  166.810, mean=0.2280, (25,50,75)pctl=(0.0982,0.1965,0.3414),(0.0<r>0.5): (2559,211)\n",
      "lambda 07:  464.159, mean=0.2629, (25,50,75)pctl=(0.1188,0.2344,0.3947),(0.0<r>0.5): (2590,312)\n",
      "lambda 08: 1291.550, mean=0.2848, (25,50,75)pctl=(0.1352,0.2679,0.4192),(0.0<r>0.5): (2598,375)\n",
      "lambda 09: 3593.814, mean=0.2859, (25,50,75)pctl=(0.1484,0.2771,0.4126),(0.0<r>0.5): (2606,317)\n",
      "lambda 10: 10000.000, mean=0.2543, (25,50,75)pctl=(0.1355,0.2425,0.3647),(0.0<r>0.5): (2591,183)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1129, (25,50,75)pctl=(0.0416,0.0946,0.1686),(0.0<r>0.5): (2412,000)\n",
      "lambda 02:    2.783, mean=0.1134, (25,50,75)pctl=(0.0418,0.0949,0.1695),(0.0<r>0.5): (2412,000)\n",
      "lambda 03:    7.743, mean=0.1165, (25,50,75)pctl=(0.0427,0.0981,0.1754),(0.0<r>0.5): (2419,000)\n",
      "lambda 04:   21.544, mean=0.1333, (25,50,75)pctl=(0.0519,0.1127,0.1994),(0.0<r>0.5): (2472,004)\n",
      "lambda 05:   59.948, mean=0.1788, (25,50,75)pctl=(0.0776,0.1564,0.2632),(0.0<r>0.5): (2532,029)\n",
      "lambda 06:  166.810, mean=0.2328, (25,50,75)pctl=(0.1129,0.2095,0.3436),(0.0<r>0.5): (2576,152)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 07:  464.159, mean=0.2658, (25,50,75)pctl=(0.1368,0.2412,0.3927),(0.0<r>0.5): (2603,275)\n",
      "lambda 08: 1291.550, mean=0.2864, (25,50,75)pctl=(0.1520,0.2686,0.4199),(0.0<r>0.5): (2619,350)\n",
      "lambda 09: 3593.814, mean=0.2836, (25,50,75)pctl=(0.1565,0.2700,0.4094),(0.0<r>0.5): (2605,302)\n",
      "lambda 10: 10000.000, mean=0.2454, (25,50,75)pctl=(0.1325,0.2327,0.3556),(0.0<r>0.5): (2580,137)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1224, (25,50,75)pctl=(0.0377,0.0989,0.1880),(0.0<r>0.5): (2380,007)\n",
      "lambda 02:    2.783, mean=0.1229, (25,50,75)pctl=(0.0380,0.0992,0.1889),(0.0<r>0.5): (2381,007)\n",
      "lambda 03:    7.743, mean=0.1259, (25,50,75)pctl=(0.0398,0.1014,0.1943),(0.0<r>0.5): (2386,007)\n",
      "lambda 04:   21.544, mean=0.1427, (25,50,75)pctl=(0.0473,0.1161,0.2205),(0.0<r>0.5): (2421,023)\n",
      "lambda 05:   59.948, mean=0.1892, (25,50,75)pctl=(0.0733,0.1567,0.2911),(0.0<r>0.5): (2496,093)\n",
      "lambda 06:  166.810, mean=0.2442, (25,50,75)pctl=(0.1078,0.2180,0.3689),(0.0<r>0.5): (2577,235)\n",
      "lambda 07:  464.159, mean=0.2783, (25,50,75)pctl=(0.1344,0.2562,0.4098),(0.0<r>0.5): (2606,340)\n",
      "lambda 08: 1291.550, mean=0.2999, (25,50,75)pctl=(0.1531,0.2892,0.4375),(0.0<r>0.5): (2614,417)\n",
      "lambda 09: 3593.814, mean=0.3037, (25,50,75)pctl=(0.1684,0.3012,0.4362),(0.0<r>0.5): (2607,393)\n",
      "lambda 10: 10000.000, mean=0.2835, (25,50,75)pctl=(0.1577,0.2826,0.4066),(0.0<r>0.5): (2589,259)\n",
      "pop.cv.best: 3593.814, mean=0.2000, (25,50,75)pctl=(0.1622,0.2863,0.4319),(0.0<r>0.5): (2648,387)\n",
      "11/11: temporal 1/1=1.000, features 11/11=(0.0175, 0.9998)\n",
      "train fold  1/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1328, (25,50,75)pctl=(0.0443,0.1073,0.2023),(0.0<r>0.5): (2417,007)\n",
      "lambda 02:    2.783, mean=0.1328, (25,50,75)pctl=(0.0443,0.1073,0.2023),(0.0<r>0.5): (2417,007)\n",
      "lambda 03:    7.743, mean=0.1329, (25,50,75)pctl=(0.0443,0.1074,0.2023),(0.0<r>0.5): (2417,007)\n",
      "lambda 04:   21.544, mean=0.1330, (25,50,75)pctl=(0.0444,0.1074,0.2025),(0.0<r>0.5): (2417,007)\n",
      "lambda 05:   59.948, mean=0.1337, (25,50,75)pctl=(0.0450,0.1080,0.2037),(0.0<r>0.5): (2419,007)\n",
      "lambda 06:  166.810, mean=0.1389, (25,50,75)pctl=(0.0472,0.1131,0.2117),(0.0<r>0.5): (2429,012)\n",
      "lambda 07:  464.159, mean=0.1655, (25,50,75)pctl=(0.0608,0.1357,0.2522),(0.0<r>0.5): (2477,045)\n",
      "lambda 08: 1291.550, mean=0.2193, (25,50,75)pctl=(0.0903,0.1859,0.3328),(0.0<r>0.5): (2564,179)\n",
      "lambda 09: 3593.814, mean=0.2607, (25,50,75)pctl=(0.1146,0.2332,0.3927),(0.0<r>0.5): (2595,318)\n",
      "lambda 10: 10000.000, mean=0.2887, (25,50,75)pctl=(0.1355,0.2748,0.4308),(0.0<r>0.5): (2610,420)\n",
      "train fold  2/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1065, (25,50,75)pctl=(0.0341,0.0892,0.1625),(0.0<r>0.5): (2338,000)\n",
      "lambda 02:    2.783, mean=0.1065, (25,50,75)pctl=(0.0341,0.0892,0.1625),(0.0<r>0.5): (2338,000)\n",
      "lambda 03:    7.743, mean=0.1065, (25,50,75)pctl=(0.0341,0.0892,0.1625),(0.0<r>0.5): (2338,000)\n",
      "lambda 04:   21.544, mean=0.1068, (25,50,75)pctl=(0.0342,0.0895,0.1630),(0.0<r>0.5): (2341,000)\n",
      "lambda 05:   59.948, mean=0.1089, (25,50,75)pctl=(0.0354,0.0910,0.1658),(0.0<r>0.5): (2346,000)\n",
      "lambda 06:  166.810, mean=0.1215, (25,50,75)pctl=(0.0412,0.1018,0.1848),(0.0<r>0.5): (2376,005)\n",
      "lambda 07:  464.159, mean=0.1629, (25,50,75)pctl=(0.0623,0.1390,0.2430),(0.0<r>0.5): (2455,038)\n",
      "lambda 08: 1291.550, mean=0.2234, (25,50,75)pctl=(0.1004,0.1988,0.3298),(0.0<r>0.5): (2545,163)\n",
      "lambda 09: 3593.814, mean=0.2660, (25,50,75)pctl=(0.1278,0.2463,0.3853),(0.0<r>0.5): (2589,289)\n",
      "lambda 10: 10000.000, mean=0.2959, (25,50,75)pctl=(0.1485,0.2830,0.4275),(0.0<r>0.5): (2610,416)\n",
      "train fold  3/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1029, (25,50,75)pctl=(0.0288,0.0819,0.1601),(0.0<r>0.5): (2288,000)\n",
      "lambda 02:    2.783, mean=0.1029, (25,50,75)pctl=(0.0289,0.0820,0.1601),(0.0<r>0.5): (2288,000)\n",
      "lambda 03:    7.743, mean=0.1030, (25,50,75)pctl=(0.0289,0.0820,0.1602),(0.0<r>0.5): (2288,000)\n",
      "lambda 04:   21.544, mean=0.1032, (25,50,75)pctl=(0.0291,0.0823,0.1607),(0.0<r>0.5): (2289,000)\n",
      "lambda 05:   59.948, mean=0.1054, (25,50,75)pctl=(0.0299,0.0841,0.1633),(0.0<r>0.5): (2299,000)\n",
      "lambda 06:  166.810, mean=0.1179, (25,50,75)pctl=(0.0355,0.0936,0.1830),(0.0<r>0.5): (2354,004)\n",
      "lambda 07:  464.159, mean=0.1592, (25,50,75)pctl=(0.0560,0.1282,0.2460),(0.0<r>0.5): (2449,050)\n",
      "lambda 08: 1291.550, mean=0.2172, (25,50,75)pctl=(0.0910,0.1830,0.3266),(0.0<r>0.5): (2543,176)\n",
      "lambda 09: 3593.814, mean=0.2563, (25,50,75)pctl=(0.1150,0.2264,0.3847),(0.0<r>0.5): (2589,293)\n",
      "lambda 10: 10000.000, mean=0.2809, (25,50,75)pctl=(0.1327,0.2605,0.4165),(0.0<r>0.5): (2601,371)\n",
      "train fold  4/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.2347, (25,50,75)pctl=(0.1132,0.2118,0.3455),(0.0<r>0.5): (2597,161)\n",
      "lambda 02:    2.783, mean=0.2347, (25,50,75)pctl=(0.1132,0.2118,0.3455),(0.0<r>0.5): (2597,161)\n",
      "lambda 03:    7.743, mean=0.2347, (25,50,75)pctl=(0.1132,0.2118,0.3455),(0.0<r>0.5): (2597,161)\n",
      "lambda 04:   21.544, mean=0.2347, (25,50,75)pctl=(0.1132,0.2118,0.3455),(0.0<r>0.5): (2597,161)\n",
      "lambda 05:   59.948, mean=0.2347, (25,50,75)pctl=(0.1132,0.2119,0.3455),(0.0<r>0.5): (2597,161)\n",
      "lambda 06:  166.810, mean=0.2347, (25,50,75)pctl=(0.1132,0.2119,0.3456),(0.0<r>0.5): (2597,161)\n",
      "lambda 07:  464.159, mean=0.2353, (25,50,75)pctl=(0.1132,0.2124,0.3464),(0.0<r>0.5): (2597,163)\n",
      "lambda 08: 1291.550, mean=0.2389, (25,50,75)pctl=(0.1156,0.2146,0.3520),(0.0<r>0.5): (2599,177)\n",
      "lambda 09: 3593.814, mean=0.2552, (25,50,75)pctl=(0.1287,0.2285,0.3761),(0.0<r>0.5): (2603,237)\n",
      "lambda 10: 10000.000, mean=0.2805, (25,50,75)pctl=(0.1453,0.2604,0.4110),(0.0<r>0.5): (2612,332)\n",
      "train fold  5/5: ntrain=2880, ntest=720\n",
      "lambda 01:    1.000, mean=0.1221, (25,50,75)pctl=(0.0377,0.0987,0.1880),(0.0<r>0.5): (2382,006)\n",
      "lambda 02:    2.783, mean=0.1221, (25,50,75)pctl=(0.0377,0.0987,0.1880),(0.0<r>0.5): (2382,006)\n",
      "lambda 03:    7.743, mean=0.1221, (25,50,75)pctl=(0.0377,0.0987,0.1880),(0.0<r>0.5): (2382,006)\n",
      "lambda 04:   21.544, mean=0.1224, (25,50,75)pctl=(0.0378,0.0989,0.1883),(0.0<r>0.5): (2383,006)\n",
      "lambda 05:   59.948, mean=0.1243, (25,50,75)pctl=(0.0391,0.0998,0.1917),(0.0<r>0.5): (2385,007)\n",
      "lambda 06:  166.810, mean=0.1362, (25,50,75)pctl=(0.0444,0.1110,0.2091),(0.0<r>0.5): (2418,017)\n",
      "lambda 07:  464.159, mean=0.1756, (25,50,75)pctl=(0.0647,0.1441,0.2688),(0.0<r>0.5): (2474,062)\n",
      "lambda 08: 1291.550, mean=0.2325, (25,50,75)pctl=(0.1007,0.2047,0.3531),(0.0<r>0.5): (2562,200)\n",
      "lambda 09: 3593.814, mean=0.2713, (25,50,75)pctl=(0.1295,0.2476,0.4009),(0.0<r>0.5): (2601,314)\n",
      "lambda 10: 10000.000, mean=0.2960, (25,50,75)pctl=(0.1484,0.2829,0.4327),(0.0<r>0.5): (2617,404)\n",
      "pop.cv.best: 10000.000, mean=0.1807, (25,50,75)pctl=(0.1437,0.2703,0.4224),(0.0<r>0.5): (2644,369)\n",
      "Duration 5.2640[mins]\n",
      "lambda 01:  166.810, mean=0.1757, (25,50,75)pctl=(0.1556,0.2161,0.2161),(0.0<r>0.5): (003,000)\n",
      "3 responses: ridge=  166.810, temporal=1.000, spatial=(0.460, 0.888) perf=0.1757\n",
      "lambda 01: 10000.000, mean=-0.0026, (25,50,75)pctl=(-0.0026,-0.0026,-0.0026),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=10000.000, temporal=1.000, spatial=(0.985, 0.170) perf=-0.0026\n",
      "lambda 01:  464.159, mean=0.4914, (25,50,75)pctl=(0.3575,0.5671,0.6029),(0.0<r>0.5): (028,016)\n",
      "28 responses: ridge=  464.159, temporal=1.000, spatial=(0.888, 0.460) perf=0.4914\n",
      "lambda 01: 1291.550, mean=0.5223, (25,50,75)pctl=(0.4649,0.5628,0.6418),(0.0<r>0.5): (332,226)\n",
      "335 responses: ridge= 1291.550, temporal=1.000, spatial=(1.000, 0.017) perf=0.5223\n",
      "lambda 01: 10000.000, mean=0.0298, (25,50,75)pctl=(0.0298,0.0298,0.0298),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=10000.000, temporal=1.000, spatial=(0.948, 0.319) perf=0.0298\n",
      "lambda 01:  166.810, mean=0.4484, (25,50,75)pctl=(0.3527,0.4484,0.5441),(0.0<r>0.5): (002,001)\n",
      "2 responses: ridge=  166.810, temporal=1.000, spatial=(0.807, 0.591) perf=0.4484\n",
      "lambda 01: 3593.814, mean=0.0560, (25,50,75)pctl=(-0.0368,0.0323,0.1075),(0.0<r>0.5): (005,000)\n",
      "10 responses: ridge= 3593.814, temporal=1.000, spatial=(0.017, 1.000) perf=0.0560\n",
      "lambda 01:  464.159, mean=0.2047, (25,50,75)pctl=(0.0391,0.1694,0.2979),(0.0<r>0.5): (045,007)\n",
      "51 responses: ridge=  464.159, temporal=1.000, spatial=(1.000, 0.017) perf=0.2047\n",
      "lambda 01: 1291.550, mean=0.2424, (25,50,75)pctl=(0.1432,0.2284,0.3144),(0.0<r>0.5): (067,003)\n",
      "67 responses: ridge= 1291.550, temporal=1.000, spatial=(0.985, 0.170) perf=0.2424\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:  166.810, mean=0.0503, (25,50,75)pctl=(-0.0052,0.0362,0.1019),(0.0<r>0.5): (028,000)\n",
      "41 responses: ridge=  166.810, temporal=1.000, spatial=(1.000, 0.017) perf=0.0503\n",
      "lambda 01:    1.000, mean=0.0669, (25,50,75)pctl=(0.0480,0.0669,0.0857),(0.0<r>0.5): (002,000)\n",
      "2 responses: ridge=    1.000, temporal=1.000, spatial=(1.000, 0.017) perf=0.0669\n",
      "lambda 01:   59.948, mean=-0.0106, (25,50,75)pctl=(-0.0106,-0.0106,-0.0106),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.888, 0.460) perf=-0.0106\n",
      "lambda 01:  464.159, mean=0.4939, (25,50,75)pctl=(0.3892,0.5323,0.6365),(0.0<r>0.5): (625,351)\n",
      "629 responses: ridge=  464.159, temporal=1.000, spatial=(0.985, 0.170) perf=0.4939\n",
      "lambda 01: 3593.814, mean=0.2111, (25,50,75)pctl=(0.1799,0.2271,0.2504),(0.0<r>0.5): (003,000)\n",
      "3 responses: ridge= 3593.814, temporal=1.000, spatial=(0.707, 0.707) perf=0.2111\n",
      "lambda 01: 10000.000, mean=-0.1516, (25,50,75)pctl=(-0.1516,-0.1516,-0.1516),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=10000.000, temporal=1.000, spatial=(0.460, 0.888) perf=-0.1516\n",
      "lambda 01:   21.544, mean=0.0312, (25,50,75)pctl=(0.0005,0.0312,0.0620),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=   21.544, temporal=1.000, spatial=(0.460, 0.888) perf=0.0312\n",
      "lambda 01:  166.810, mean=-0.0299, (25,50,75)pctl=(-0.0299,-0.0299,-0.0299),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=  166.810, temporal=1.000, spatial=(0.319, 0.948) perf=-0.0299\n",
      "lambda 01:   59.948, mean=0.0673, (25,50,75)pctl=(0.0207,0.0553,0.0705),(0.0<r>0.5): (007,000)\n",
      "7 responses: ridge=   59.948, temporal=1.000, spatial=(0.948, 0.319) perf=0.0673\n",
      "lambda 01: 3593.814, mean=0.1616, (25,50,75)pctl=(0.0767,0.1547,0.2396),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge= 3593.814, temporal=1.000, spatial=(0.591, 0.807) perf=0.1616\n",
      "lambda 01:   21.544, mean=0.1559, (25,50,75)pctl=(0.1185,0.1410,0.1660),(0.0<r>0.5): (006,000)\n",
      "6 responses: ridge=   21.544, temporal=1.000, spatial=(0.985, 0.170) perf=0.1559\n",
      "lambda 01: 3593.814, mean=0.4036, (25,50,75)pctl=(0.2727,0.4172,0.5270),(0.0<r>0.5): (044,016)\n",
      "44 responses: ridge= 3593.814, temporal=1.000, spatial=(0.170, 0.985) perf=0.4036\n",
      "lambda 01: 1291.550, mean=-0.0230, (25,50,75)pctl=(-0.0384,-0.0230,-0.0077),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge= 1291.550, temporal=1.000, spatial=(0.017, 1.000) perf=-0.0230\n",
      "lambda 01: 3593.814, mean=0.2919, (25,50,75)pctl=(0.2380,0.2855,0.3326),(0.0<r>0.5): (020,001)\n",
      "20 responses: ridge= 3593.814, temporal=1.000, spatial=(0.319, 0.948) perf=0.2919\n",
      "lambda 01:   21.544, mean=-0.1030, (25,50,75)pctl=(-0.1030,-0.1030,-0.1030),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=   21.544, temporal=1.000, spatial=(1.000, 0.017) perf=-0.1030\n",
      "lambda 01:   59.948, mean=0.1310, (25,50,75)pctl=(0.1120,0.1431,0.1620),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge=   59.948, temporal=1.000, spatial=(0.807, 0.591) perf=0.1310\n",
      "lambda 01: 10000.000, mean=0.4110, (25,50,75)pctl=(0.1626,0.4374,0.6888),(0.0<r>0.5): (012,007)\n",
      "14 responses: ridge=10000.000, temporal=1.000, spatial=(0.017, 1.000) perf=0.4110\n",
      "lambda 01:  464.159, mean=0.6106, (25,50,75)pctl=(0.5666,0.6493,0.7166),(0.0<r>0.5): (158,136)\n",
      "159 responses: ridge=  464.159, temporal=1.000, spatial=(0.591, 0.807) perf=0.6106\n",
      "lambda 01:   59.948, mean=-0.0193, (25,50,75)pctl=(-0.0707,-0.0377,0.0275),(0.0<r>0.5): (005,000)\n",
      "14 responses: ridge=   59.948, temporal=1.000, spatial=(1.000, 0.017) perf=-0.0193\n",
      "lambda 01:   59.948, mean=0.1345, (25,50,75)pctl=(0.1345,0.1345,0.1345),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.591, 0.807) perf=0.1345\n",
      "lambda 01: 1291.550, mean=0.3533, (25,50,75)pctl=(0.2607,0.3618,0.4624),(0.0<r>0.5): (058,010)\n",
      "58 responses: ridge= 1291.550, temporal=1.000, spatial=(0.591, 0.807) perf=0.3533\n",
      "lambda 01:    7.743, mean=0.0489, (25,50,75)pctl=(0.0489,0.0489,0.0489),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    7.743, temporal=1.000, spatial=(0.948, 0.319) perf=0.0489\n",
      "lambda 01:   59.948, mean=0.1042, (25,50,75)pctl=(0.0249,0.0662,0.1584),(0.0<r>0.5): (032,000)\n",
      "37 responses: ridge=   59.948, temporal=1.000, spatial=(0.985, 0.170) perf=0.1042\n",
      "lambda 01: 10000.000, mean=0.1014, (25,50,75)pctl=(0.0323,0.1316,0.2008),(0.0<r>0.5): (003,000)\n",
      "4 responses: ridge=10000.000, temporal=1.000, spatial=(0.170, 0.985) perf=0.1014\n",
      "lambda 01:  166.810, mean=0.6285, (25,50,75)pctl=(0.6393,0.6909,0.7177),(0.0<r>0.5): (067,059)\n",
      "67 responses: ridge=  166.810, temporal=1.000, spatial=(0.948, 0.319) perf=0.6285\n",
      "lambda 01:    1.000, mean=0.0768, (25,50,75)pctl=(0.0768,0.0768,0.0768),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(0.170, 0.985) perf=0.0768\n",
      "lambda 01:  166.810, mean=-0.0312, (25,50,75)pctl=(-0.0852,-0.0225,0.0140),(0.0<r>0.5): (003,000)\n",
      "6 responses: ridge=  166.810, temporal=1.000, spatial=(0.170, 0.985) perf=-0.0312\n",
      "lambda 01:  464.159, mean=0.3570, (25,50,75)pctl=(0.0997,0.3881,0.6455),(0.0<r>0.5): (003,002)\n",
      "4 responses: ridge=  464.159, temporal=1.000, spatial=(0.319, 0.948) perf=0.3570\n",
      "lambda 01: 10000.000, mean=0.2345, (25,50,75)pctl=(0.1798,0.2273,0.2953),(0.0<r>0.5): (048,001)\n",
      "50 responses: ridge=10000.000, temporal=1.000, spatial=(1.000, 0.017) perf=0.2345\n",
      "lambda 01:  464.159, mean=0.5467, (25,50,75)pctl=(0.4902,0.5713,0.6285),(0.0<r>0.5): (058,042)\n",
      "58 responses: ridge=  464.159, temporal=1.000, spatial=(0.807, 0.591) perf=0.5467\n",
      "lambda 01:   59.948, mean=0.0703, (25,50,75)pctl=(0.0637,0.0765,0.0800),(0.0<r>0.5): (003,000)\n",
      "3 responses: ridge=   59.948, temporal=1.000, spatial=(0.170, 0.985) perf=0.0703\n",
      "lambda 01: 3593.814, mean=0.1339, (25,50,75)pctl=(0.0449,0.1352,0.1907),(0.0<r>0.5): (007,000)\n",
      "7 responses: ridge= 3593.814, temporal=1.000, spatial=(0.985, 0.170) perf=0.1339\n",
      "lambda 01: 1291.550, mean=0.5598, (25,50,75)pctl=(0.4955,0.6411,0.6792),(0.0<r>0.5): (034,025)\n",
      "34 responses: ridge= 1291.550, temporal=1.000, spatial=(0.170, 0.985) perf=0.5598\n",
      "lambda 01:  464.159, mean=0.5949, (25,50,75)pctl=(0.5576,0.6323,0.6834),(0.0<r>0.5): (117,098)\n",
      "118 responses: ridge=  464.159, temporal=1.000, spatial=(0.707, 0.707) perf=0.5949\n",
      "lambda 01: 1291.550, mean=0.1048, (25,50,75)pctl=(0.1048,0.1048,0.1048),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge= 1291.550, temporal=1.000, spatial=(0.807, 0.591) perf=0.1048\n",
      "lambda 01:  166.810, mean=0.1035, (25,50,75)pctl=(0.0343,0.1035,0.1726),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=  166.810, temporal=1.000, spatial=(0.707, 0.707) perf=0.1035\n",
      "lambda 01: 3593.814, mean=0.1832, (25,50,75)pctl=(0.2124,0.2124,0.2177),(0.0<r>0.5): (005,000)\n",
      "5 responses: ridge= 3593.814, temporal=1.000, spatial=(0.460, 0.888) perf=0.1832\n",
      "lambda 01:  166.810, mean=0.4968, (25,50,75)pctl=(0.2961,0.5905,0.6894),(0.0<r>0.5): (165,104)\n",
      "168 responses: ridge=  166.810, temporal=1.000, spatial=(0.985, 0.170) perf=0.4968\n",
      "lambda 01: 1291.550, mean=0.3778, (25,50,75)pctl=(0.3252,0.4027,0.4802),(0.0<r>0.5): (021,002)\n",
      "22 responses: ridge= 1291.550, temporal=1.000, spatial=(0.707, 0.707) perf=0.3778\n",
      "lambda 01: 1291.550, mean=0.4763, (25,50,75)pctl=(0.3822,0.4925,0.5730),(0.0<r>0.5): (080,038)\n",
      "80 responses: ridge= 1291.550, temporal=1.000, spatial=(0.460, 0.888) perf=0.4763\n",
      "lambda 01:    7.743, mean=-0.0942, (25,50,75)pctl=(-0.0946,-0.0942,-0.0939),(0.0<r>0.5): (000,000)\n",
      "2 responses: ridge=    7.743, temporal=1.000, spatial=(1.000, 0.017) perf=-0.0942\n",
      "lambda 01:  166.810, mean=0.5262, (25,50,75)pctl=(0.5744,0.6157,0.6750),(0.0<r>0.5): (014,013)\n",
      "16 responses: ridge=  166.810, temporal=1.000, spatial=(0.888, 0.460) perf=0.5262\n",
      "lambda 01:  464.159, mean=0.5100, (25,50,75)pctl=(0.4169,0.5392,0.6222),(0.0<r>0.5): (126,079)\n",
      "127 responses: ridge=  464.159, temporal=1.000, spatial=(0.948, 0.319) perf=0.5100\n",
      "lambda 01:  464.159, mean=0.6611, (25,50,75)pctl=(0.6401,0.7040,0.7390),(0.0<r>0.5): (052,047)\n",
      "52 responses: ridge=  464.159, temporal=1.000, spatial=(0.460, 0.888) perf=0.6611\n",
      "lambda 01:   59.948, mean=0.0528, (25,50,75)pctl=(-0.0076,0.0475,0.1080),(0.0<r>0.5): (002,000)\n",
      "4 responses: ridge=   59.948, temporal=1.000, spatial=(0.319, 0.948) perf=0.0528\n",
      "lambda 01: 10000.000, mean=0.0080, (25,50,75)pctl=(0.0015,0.0080,0.0144),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=10000.000, temporal=1.000, spatial=(0.319, 0.948) perf=0.0080\n",
      "lambda 01:    1.000, mean=0.0109, (25,50,75)pctl=(0.0109,0.0109,0.0109),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    1.000, temporal=1.000, spatial=(0.591, 0.807) perf=0.0109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda 01:    7.743, mean=0.0701, (25,50,75)pctl=(0.0701,0.0701,0.0701),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    7.743, temporal=1.000, spatial=(0.985, 0.170) perf=0.0701\n",
      "lambda 01: 1291.550, mean=0.4132, (25,50,75)pctl=(0.2693,0.4610,0.5374),(0.0<r>0.5): (059,027)\n",
      "60 responses: ridge= 1291.550, temporal=1.000, spatial=(0.319, 0.948) perf=0.4132\n",
      "lambda 01: 3593.814, mean=0.3764, (25,50,75)pctl=(0.3081,0.3993,0.4753),(0.0<r>0.5): (183,036)\n",
      "185 responses: ridge= 3593.814, temporal=1.000, spatial=(1.000, 0.017) perf=0.3764\n",
      "lambda 01:  166.810, mean=0.0968, (25,50,75)pctl=(0.0717,0.1170,0.1321),(0.0<r>0.5): (003,000)\n",
      "3 responses: ridge=  166.810, temporal=1.000, spatial=(0.591, 0.807) perf=0.0968\n",
      "lambda 01:   59.948, mean=-0.0612, (25,50,75)pctl=(-0.0612,-0.0612,-0.0612),(0.0<r>0.5): (000,000)\n",
      "1 responses: ridge=   59.948, temporal=1.000, spatial=(0.707, 0.707) perf=-0.0612\n",
      "lambda 01: 1291.550, mean=0.1112, (25,50,75)pctl=(0.1112,0.1112,0.1112),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge= 1291.550, temporal=1.000, spatial=(0.948, 0.319) perf=0.1112\n",
      "lambda 01:    7.743, mean=0.1177, (25,50,75)pctl=(0.1177,0.1177,0.1177),(0.0<r>0.5): (001,000)\n",
      "1 responses: ridge=    7.743, temporal=1.000, spatial=(0.807, 0.591) perf=0.1177\n",
      "lambda 01:   21.544, mean=-0.0132, (25,50,75)pctl=(-0.0481,-0.0132,0.0216),(0.0<r>0.5): (001,000)\n",
      "2 responses: ridge=   21.544, temporal=1.000, spatial=(0.888, 0.460) perf=-0.0132\n",
      "lambda 01: 1291.550, mean=0.3059, (25,50,75)pctl=(0.2842,0.3776,0.3993),(0.0<r>0.5): (004,000)\n",
      "4 responses: ridge= 1291.550, temporal=1.000, spatial=(0.888, 0.460) perf=0.3059\n",
      "lambda 01:   21.544, mean=0.0262, (25,50,75)pctl=(0.0199,0.0295,0.0342),(0.0<r>0.5): (003,000)\n",
      "3 responses: ridge=   21.544, temporal=1.000, spatial=(0.948, 0.319) perf=0.0262\n",
      "lambda 01:  464.159, mean=0.1069, (25,50,75)pctl=(0.1150,0.1320,0.1399),(0.0<r>0.5): (005,000)\n",
      "5 responses: ridge=  464.159, temporal=1.000, spatial=(0.170, 0.985) perf=0.1069\n",
      "Total duration 110.3670[mins]\n"
     ]
    }
   ],
   "source": [
    "fit_bandedhrf_polar = models.estimate_stem_wmvnp([Mtrain, Otrain], Ytrain, \n",
    "                                               [Mtest, Otest],Ytest,\n",
    "                                               feature_priors=[moten_prior, obcat_prior],\n",
    "                                               temporal_prior=temporal_prior,\n",
    "                                               ridges=np.logspace(0,4,10),\n",
    "                                               normalize_hyparams=True,\n",
    "                                               folds=(1,5),\n",
    "                                               performance=True,\n",
    "                                               verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: divide by zero encountered in log\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe3ace0c4d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = plot_model_2dhist_comparison(np.nan_to_num(fit_banded_polar['performance'].squeeze()),\n",
    "                                           np.nan_to_num(fit_bandedhrf_polar['performance'].squeeze()),\n",
    "                                           'banded ridge\\n(spherical temporal prior)', \n",
    "                                           'banded ridge\\n(hrf temporal prior)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
